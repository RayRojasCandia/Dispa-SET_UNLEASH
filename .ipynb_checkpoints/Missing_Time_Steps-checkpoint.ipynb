{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0082e63c-84b4-4fc5-bd56-7ee2b7f6d3ed",
   "metadata": {},
   "source": [
    "<div style=\"background-color: black;\">\n",
    "<hr style=\"border: 3px solid skyblue;\">\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman; color: skyblue; text-transform: uppercase;\">\n",
    "    TIME SERIES DATA PROCESSING\n",
    "    <br>\n",
    "    AVAILABILITY FACTOR | BOUNDARY SECTOR DEMAND | SCALLED INFLOWS | RESERVOIR LEVELS | LOAD REAL TIME | LOAD DAY AHEAD\n",
    "</div>\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Fulfilling 15 minutes Not Available Time Steps Notebook\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "        Each part of the following script was used for the pacially processed time series i.e. Availability Factors, Scalled Inflows, Current Load and Load Dayahead data, to cumpliment all those data which is missing of 15 and 30 minute time steps.\n",
    "<br>\n",
    "Read explanation text cells to follow and understand all the process until final results were got stept by step.\n",
    "</div>\n",
    "    <hr style=\"border: 2px solid skyblue;\">\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    1. Notebook Set Up\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    Importing needed libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed12d7a-a85e-40b5-9d89-821a18fb14ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone, utc\n",
    "import shutil\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa79d2-d4dc-41f2-8a9e-0f9ec636b8cd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: black;\">\n",
    "<hr style=\"border: 2px solid skyblue;\">\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    2. Dispa-SET_Unleash Folder Path\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    Determinning dynamically the zone_folder_path based on the location of the \"Dispa-SET_Unleash\" folder relative to the current working directory.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "- If the \"Dispa-SET_Unleash\" folder is copied to a different machine or location, the dispaSET_unleash_folder_path variable will automatically adjust accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df2c7883-8574-451f-ab36-62760613a9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: home\n",
      "dispaSET_unleash_folder_path: /home\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory of \"Dispa-SET_Unleash\"\n",
    "dispaSET_unleash_parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Get the path to the \"Dispa-SET_Unleash\" folder\n",
    "dispaSET_unleash_folder_path = os.path.dirname(dispaSET_unleash_parent_directory)\n",
    "\n",
    "# Construct the dispaSET_unleash_folder_name variable\n",
    "dispaSET_unleash_folder_name = os.path.basename(dispaSET_unleash_folder_path)\n",
    "\n",
    "print(\"dispaSET_unleash_folder_name:\", dispaSET_unleash_folder_name)\n",
    "print(\"dispaSET_unleash_folder_path:\", dispaSET_unleash_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259de816-2560-4362-b5e3-47b9ba6ed7c8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: black;\">\n",
    "<hr style=\"border: 2px solid skyblue;\">\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    3. Usefull Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Entering a value to all the variables which content are going to be used in some of the next stages of this script. \n",
    "</div>\n",
    "<div style=\"text-align: Justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Indicate the year of all data is referring to in the variable data_year.\n",
    "<br>\n",
    "The universal_standar_time variable is going to be used to download all the time series data in this horary zone. Additionally as each european country belongs a particular time sector the corresponding time series data related to its time sector are going to be downloaded as well but in a different file.\n",
    "<br>Additionally there are some default parameters that has to be defined to the correct working and calling to the ENTSO-E downloading functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ea33407-f6d7-4d90-b443-dbe383f47228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year to which data refers to:\n",
    "data_years_list = [2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43a3b076-b0dd-47ef-9c41-f611e5462c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_list = ['1h', '30min', '15min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58655f06-c3b2-4974-ac90-06f804035584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries to which data refers to:\n",
    "countries_list = [\n",
    "    'AL', 'BA', 'BG', 'CY', 'DE', 'EE', 'ES', 'FR', 'HR', 'IE', 'LT', 'LV', 'ME', 'MT', 'NO', 'PT', 'RS', 'SI', 'UK',\n",
    "    'AT', 'BE', 'CH', 'CZ', 'DK', 'EL', 'FI', 'GR', 'HU', 'IT', 'LU', 'MD', 'MK', 'NL', 'PL', 'RO', 'SE', 'SK'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3de0331f-e4ae-4c5b-b6b2-72036d470477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "availability_factors_folder_path: /home/Database/AvailabilityFactors\n",
      "load_dayahead_folder_path: /home/Database/Load_DayAhead\n",
      "load_realtime_folder_path: /home/Database/Load_RealTime\n",
      "scalled_inflows_folder_path: /home/Database/HydroData/ScaledInflows\n",
      "reservoir_level_folder_path: /home/Database/HydroData/ReservoirLevel\n",
      "boundary_sector_demand_folder_path: /home/Database/BoudarySector/BoudarySectorDemand\n"
     ]
    }
   ],
   "source": [
    "# Additional string to be appended\n",
    "availability_factors_folder_path = os.path.join(dispaSET_unleash_folder_path, \"Database\", \"AvailabilityFactors\")\n",
    "load_dayahead_folder_path = os.path.join(dispaSET_unleash_folder_path, \"Database\", \"Load_DayAhead\")\n",
    "load_realtime_folder_path = os.path.join(dispaSET_unleash_folder_path, \"Database\", \"Load_RealTime\")\n",
    "scalled_inflows_folder_path = os.path.join(dispaSET_unleash_folder_path, \"Database\", \"HydroData\", \"ScaledInflows\")\n",
    "reservoir_level_folder_path = os.path.join(dispaSET_unleash_folder_path, \"Database\", \"HydroData\", \"ReservoirLevel\")\n",
    "boundary_sector_demand_folder_path = os.path.join(dispaSET_unleash_folder_path, \"Database\", \"BoudarySector\", \"BoudarySectorDemand\")\n",
    "\n",
    "print(\"availability_factors_folder_path:\", availability_factors_folder_path)\n",
    "print(\"load_dayahead_folder_path:\", load_dayahead_folder_path)\n",
    "print(\"load_realtime_folder_path:\", load_realtime_folder_path)\n",
    "print(\"scalled_inflows_folder_path:\", scalled_inflows_folder_path)\n",
    "print(\"reservoir_level_folder_path:\", reservoir_level_folder_path)\n",
    "print(\"boundary_sector_demand_folder_path:\", boundary_sector_demand_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b69f0b-f0b9-42c8-8d61-0b1eca366e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4d06274-404f-4ba8-8f27-a76cf2d6aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country folder not found: /home/Database/AvailabilityFactors/AL\n",
      "Country folder not found: /home/Database/AvailabilityFactors/BA\n",
      "Country folder not found: /home/Database/AvailabilityFactors/BG\n",
      "Country folder not found: /home/Database/AvailabilityFactors/CY\n",
      "Country folder not found: /home/Database/AvailabilityFactors/DE\n",
      "Country folder not found: /home/Database/AvailabilityFactors/EE\n",
      "Country folder not found: /home/Database/AvailabilityFactors/ES\n",
      "Country folder not found: /home/Database/AvailabilityFactors/FR\n",
      "Country folder not found: /home/Database/AvailabilityFactors/HR\n",
      "Country folder not found: /home/Database/AvailabilityFactors/IE\n",
      "Country folder not found: /home/Database/AvailabilityFactors/LT\n",
      "Country folder not found: /home/Database/AvailabilityFactors/LV\n",
      "Country folder not found: /home/Database/AvailabilityFactors/ME\n",
      "Country folder not found: /home/Database/AvailabilityFactors/MT\n",
      "Country folder not found: /home/Database/AvailabilityFactors/NO\n",
      "Country folder not found: /home/Database/AvailabilityFactors/PT\n",
      "Country folder not found: /home/Database/AvailabilityFactors/RS\n",
      "Country folder not found: /home/Database/AvailabilityFactors/SI\n",
      "Country folder not found: /home/Database/AvailabilityFactors/UK\n",
      "Country folder not found: /home/Database/AvailabilityFactors/AT\n",
      "Country folder not found: /home/Database/AvailabilityFactors/BE\n",
      "Country folder not found: /home/Database/AvailabilityFactors/CH\n",
      "Country folder not found: /home/Database/AvailabilityFactors/CZ\n",
      "Country folder not found: /home/Database/AvailabilityFactors/DK\n",
      "Country folder not found: /home/Database/AvailabilityFactors/EL\n",
      "Country folder not found: /home/Database/AvailabilityFactors/FI\n",
      "Country folder not found: /home/Database/AvailabilityFactors/GR\n",
      "Country folder not found: /home/Database/AvailabilityFactors/HU\n",
      "Country folder not found: /home/Database/AvailabilityFactors/IT\n",
      "Country folder not found: /home/Database/AvailabilityFactors/LU\n",
      "Country folder not found: /home/Database/AvailabilityFactors/MD\n",
      "Country folder not found: /home/Database/AvailabilityFactors/MK\n",
      "Country folder not found: /home/Database/AvailabilityFactors/NL\n",
      "Country folder not found: /home/Database/AvailabilityFactors/PL\n",
      "Country folder not found: /home/Database/AvailabilityFactors/RO\n",
      "Country folder not found: /home/Database/AvailabilityFactors/SE\n",
      "Country folder not found: /home/Database/AvailabilityFactors/SK\n"
     ]
    }
   ],
   "source": [
    "def interpolate_time_series(df, original_step, target_steps):\n",
    "    \"\"\"\n",
    "    Interpolate time series data from one time step to multiple target time steps\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Original DataFrame\n",
    "    - original_step: Original time step (e.g., '1h', '30min')\n",
    "    - target_steps: List of target time steps to interpolate to\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of DataFrames for each target time step\n",
    "    \"\"\"\n",
    "    # Convert time index to datetime if not already\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    # Create interpolation results dictionary\n",
    "    interpolated_dfs = {}\n",
    "    \n",
    "    for target_step in target_steps:\n",
    "        # Create new index with target time step\n",
    "        start = df.index[0]\n",
    "        end = df.index[-1]\n",
    "        \n",
    "        # Convert time steps to minutes\n",
    "        step_to_minutes = {\n",
    "            '15min': 15,\n",
    "            '30min': 30,\n",
    "            '1h': 60\n",
    "        }\n",
    "        \n",
    "        new_index = pd.date_range(\n",
    "            start=start, \n",
    "            end=end, \n",
    "            freq=target_step\n",
    "        )\n",
    "        \n",
    "        # Create new DataFrame with new index\n",
    "        new_df = pd.DataFrame(index=new_index)\n",
    "        \n",
    "        # Interpolate each column\n",
    "        for col in df.columns:\n",
    "            new_df[col] = np.interp(\n",
    "                new_index.astype(int) / 10**9,  # Convert to timestamps\n",
    "                df.index.astype(int) / 10**9,   # Convert to timestamps\n",
    "                df[col]\n",
    "            )\n",
    "        \n",
    "        interpolated_dfs[target_step] = new_df\n",
    "    \n",
    "    return interpolated_dfs\n",
    "\n",
    "def process_directories(\n",
    "    base_folder_path, \n",
    "    countries_list, \n",
    "    time_steps_list, \n",
    "    data_years_list\n",
    "):\n",
    "    \"\"\"\n",
    "    Process directories and interpolate time series data\n",
    "    \"\"\"\n",
    "    # Desired time steps for interpolation\n",
    "    all_time_steps = ['15min', '30min', '1h']\n",
    "    \n",
    "    # Iterate through countries\n",
    "    for country in countries_list:\n",
    "        country_path = os.path.join(base_folder_path, country)\n",
    "        \n",
    "        # Skip if country folder doesn't exist\n",
    "        if not os.path.exists(country_path):\n",
    "            print(f\"Country folder not found: {country_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Check time step folders\n",
    "        for main_time_step in time_steps_list:\n",
    "            main_time_step_path = os.path.join(country_path, main_time_step)\n",
    "            \n",
    "            # Skip if main time step folder doesn't exist\n",
    "            if not os.path.exists(main_time_step_path):\n",
    "                continue\n",
    "            \n",
    "            # Find missing time steps\n",
    "            missing_steps = [step for step in all_time_steps if step not in time_steps_list]\n",
    "            \n",
    "            # Process each year's CSV\n",
    "            for year in data_years_list:\n",
    "                csv_path = os.path.join(main_time_step_path, f\"{year}.csv\")\n",
    "                \n",
    "                # Skip if CSV doesn't exist\n",
    "                if not os.path.exists(csv_path):\n",
    "                    print(f\"CSV not found: {csv_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Read CSV\n",
    "                df = pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
    "                \n",
    "                # Interpolate for missing time steps\n",
    "                for missing_step in missing_steps:\n",
    "                    # Create output folder for missing time step\n",
    "                    missing_step_folder = os.path.join(country_path, missing_step)\n",
    "                    os.makedirs(missing_step_folder, exist_ok=True)\n",
    "                    \n",
    "                    # Interpolate\n",
    "                    interpolated_dfs = interpolate_time_series(\n",
    "                        df, \n",
    "                        main_time_step, \n",
    "                        [missing_step]\n",
    "                    )\n",
    "                    \n",
    "                    # Save interpolated CSV\n",
    "                    output_csv_path = os.path.join(missing_step_folder, f\"{year}.csv\")\n",
    "                    interpolated_dfs[missing_step].to_csv(output_csv_path)\n",
    "                    \n",
    "                    print(f\"Created interpolated file: {output_csv_path}\")\n",
    "\n",
    "# Example usage (you would replace these with your actual paths and lists)\n",
    "base_folder_path = availability_factors_folder_path\n",
    "\n",
    "# Run the processing\n",
    "process_directories(\n",
    "    base_folder_path, \n",
    "    countries_list, \n",
    "    time_steps_list, \n",
    "    data_years_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d734a-adcc-4985-b62f-712096f99625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
