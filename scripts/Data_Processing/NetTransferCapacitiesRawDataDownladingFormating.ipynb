{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4af4c44-8f75-4f4e-9cee-8a3db92d0eda",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman;\">\n",
    "    TIME SERIES DATA PROCESSING | NET TRANSFER CAPACITIES\n",
    "</div>\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman;\">\n",
    "    Downloading / Formatting Notebook\n",
    "<div style=\"text-align: left; margin-left: 0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Each part of the following script is used to donwload and formating the raw data for the Net Transfer Capacities Time Series Raw Data for all the european countries of the Dispa-SET_Unleash project.\n",
    "<br>\n",
    "Read explanation text cells to follow and understand all the process until final results were got stept by step.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    1. Notebook Set Up\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Importing needed libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a590ae9c-56f6-4dbf-a4f4-9f39b4fa21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from entsoe import EntsoePandasClient\n",
    "from entsoe.exceptions import NoMatchingDataError  # Ensure this line is included\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone, utc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de47724-25dd-456b-90ed-5b0f0f73cbb5",
   "metadata": {},
   "source": [
    " <div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    2. ENTSO-E RESTful API.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Connecting with the ENTSO-E API Tool.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- To donwload the neeeded data using the API tool, is mandatory to use a token autentication to connect and make the future request of data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6899aaf-90da-463a-b5e6-6092997b2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = EntsoePandasClient(api_key='61e5bbbb-7e80-4540-a471-bd993873aa74')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2e76f-2442-4fdd-b2db-b3b71f3f544b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    3. Dispa-SET_Unleash Folder Path\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Determinning dynamically the zone_folder_path based on the location of the \"Dispa-SET_Unleash\" folder relative to the current working directory.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- If the \"Dispa-SET_Unleash\" folder is copied to a different machine or location, the dispaSET_unleash_folder_path variable will automatically adjust accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8926675d-6398-4db7-80cd-42c829b7b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory of \"Dispa-SET_Unleash\"\n",
    "dispaSET_unleash_parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Get the path to the \"Dispa-SET_Unleash\" folder\n",
    "dispaSET_unleash_folder_path = os.path.dirname(dispaSET_unleash_parent_directory)\n",
    "\n",
    "# Construct the dispaSET_unleash_folder_name variable\n",
    "dispaSET_unleash_folder_name = os.path.basename(dispaSET_unleash_folder_path)\n",
    "\n",
    "print(\"dispaSET_unleash_folder_name:\", dispaSET_unleash_folder_name)\n",
    "print(\"dispaSET_unleash_folder_path:\", dispaSET_unleash_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38b8cd-b666-4738-9813-f1fbeb216ab7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    4. Usefull Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Entering a value to all the variables which content are going to be used in some of the next stages of this script. \n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- Indicate the year of all data is referring to in the variable data_year.\n",
    "<br>\n",
    "- The universal_standar_time variable is going to be used to download all the time series data in this horary zone. Additionally as each european country belongs a particular time sector the corresponding time series data related to its time sector are going to be downloaded as well but in a different file.\n",
    "<br>\n",
    "- Additionally there are some default parameters that has to be defined to the correct working and calling to the ENTSO-E downloading functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17cff013-7efc-43b0-a572-8f32f1f9a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year to which data refers to:\n",
    "data_year = 2023\n",
    "data_year_1 = data_year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fe94548-6223-4ace-9793-b857b53e2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntc_folder_path: /home/ray/Dispa-SET_Unleash/RawData/NTC/\n",
      "ntc_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/\n"
     ]
    }
   ],
   "source": [
    "# Additional string to be appended\n",
    "additional_path = \"/RawData/NTC/\"\n",
    "additional_path_1 = \"/RawData/NTC/Raw_Data_Sources/\"\n",
    "\n",
    "# Construct the Outage_Factors_folder_path variable\n",
    "ntc_folder_path = dispaSET_unleash_folder_path + additional_path\n",
    "\n",
    "# Construct the Outage_Factors_Raw_Data_folder_path variable\n",
    "ntc_raw_data_folder_path = dispaSET_unleash_folder_path + additional_path_1\n",
    "\n",
    "print(\"ntc_folder_path:\", ntc_folder_path)\n",
    "print(\"ntc_raw_data_folder_path:\", ntc_raw_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab19470a-140b-4a86-a0c9-32f845c3497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "type_marketagreement_type = 'A01'\n",
    "contract_marketagreement_type = \"A01\"\n",
    "process_type = 'A51'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610c8df-68ce-40aa-af2e-d67c3b2df7a0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    5. Country List Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Defining the list of countries according to the available data. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9485328c-cd02-43c9-a78d-ad5c22cff2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of country codes\n",
    "ntc_per_unit_country_list = [\"AT\", \"BE\", \"BG\", \"CH\", \"CY\", \"CZ\", \"DE\", \"DK\", \"EE\", \"GR\", \"ES\", \"FI\", \"FR\", \"HR\", \"HU\", \n",
    "                             \"IE\", \"IT\", \"LT\", \"LU\", \"LV\", \"MT\", \"NL\", \"NO\", \"PL\", \"PT\", \"RO\", \"SE\", \"SI\", \"SK\", \"UK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f959e45-e5f2-4856-831d-454684b52816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/country_list.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define the directory and file path\n",
    "file_name = 'country_list.csv'\n",
    "file_path = os.path.join(ntc_raw_data_folder_path, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(ntc_raw_data_folder_path, exist_ok=True)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(ntc_per_unit_country_list, columns=['Country_From'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to '{file_path}'\")\n",
    "ntc_country_list_file = file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e96e3b-c1f7-4dd1-b455-107f8f37d533",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2d3ed6b-4f59-4092-a51c-e1e71c833e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name:                              Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path:                              /home/ray/Dispa-SET_Unleash\n",
      "data_year:                                                 2023\n",
      "ntc_folder_path:                                           /home/ray/Dispa-SET_Unleash/RawData/NTC/\n",
      "ntc_raw_data_folder_path:                                  /home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/\n",
      "ntc_country_list_file:                                     /home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/country_list.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"dispaSET_unleash_folder_name:                              {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path:                              {dispaSET_unleash_folder_path}\")\n",
    "print (f\"data_year:                                                 {data_year}\")\n",
    "print (f\"ntc_folder_path:                                           {ntc_folder_path}\")   \n",
    "print (f\"ntc_raw_data_folder_path:                                  {ntc_raw_data_folder_path}\")\n",
    "print (f\"ntc_country_list_file:                                     {ntc_country_list_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88b8d7-8432-4155-a9c7-c1731043ee24",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Defining the sub-folders where all the cross border flows raw data is saved. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14f38fa4-bfc9-44fd-862e-18fecb61998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/country_list.csv' with new subfolders created.\n"
     ]
    }
   ],
   "source": [
    "# Convert data_year to string if it's not already\n",
    "data_year = str(data_year)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(ntc_country_list_file)\n",
    "\n",
    "# Ensure the column 'Country_From' exists\n",
    "if 'Country_From' not in df.columns:\n",
    "    raise ValueError(\"Column 'Country_From' does not exist in the CSV file\")\n",
    "\n",
    "# Define the base directory where subfolders will be created\n",
    "base_directory = os.path.join(ntc_raw_data_folder_path, data_year)\n",
    "\n",
    "# Create a list to hold the paths of the created subfolders\n",
    "country_folder_paths = []\n",
    "\n",
    "# Create subfolders and save their paths\n",
    "for country in df['Country_From']:\n",
    "    # Create the subfolder path\n",
    "    subfolder_path = os.path.join(base_directory, country)\n",
    "    \n",
    "    # Create the subfolder if it doesn't exist\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    \n",
    "    # Append the subfolder path to the list\n",
    "    country_folder_paths.append(subfolder_path)\n",
    "\n",
    "# Add the new column 'Country_Folder' to the DataFrame\n",
    "df['Country_Folder'] = country_folder_paths\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(ntc_country_list_file, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved to '{ntc_country_list_file}' with new subfolders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c69f72-aa17-4308-8525-710e0c91ae83",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Defining the neighbor countries. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c06590d-9da6-40fa-b17c-745801e846b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/country_list.csv' as new columns\n"
     ]
    }
   ],
   "source": [
    "# Raw data as a multiline string\n",
    "data = \"\"\"\n",
    "CZ, DE, HU, IT, SI, CH\n",
    "FR, DE, LU, NL, UK\n",
    "GR, RO, \n",
    "AT, FR, DE, IT\n",
    "\n",
    "AT, DE, PL, SK\n",
    "AT, BE, CZ, DK, FR, LU, NL, NO, PL, SE, CH\n",
    "DE, NL, NO, SE, UK\n",
    "FI, LV\n",
    "BG, IT\n",
    "FR, PT\n",
    "EE, NO, SE\n",
    "BE, DE, IT, ES, CH, UK\n",
    "HU, SI\n",
    "AT, HR, RO, SK, SI\n",
    "UK\n",
    "AT, FR, GR, MT, SI, CH\n",
    "LV, PL, SE\n",
    "BE, DE\n",
    "EE, LT\n",
    "IT\n",
    "BE, DK, DE, NO, UK\n",
    "DK, FI, DE, NL, SE, UK\n",
    "CZ, DE, LT, SK, SE\n",
    "ES\n",
    "BG, HU\n",
    "DK, FI, DE, LT, NO, PL\n",
    "AT, HR, HU, IT\n",
    "CZ, HU, PL\n",
    "BE, DK, FR, IE, NL, NO\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into lines\n",
    "lines = data.strip().split(\"\\n\")\n",
    "\n",
    "# Initialize a list to hold the data\n",
    "data_list = []\n",
    "\n",
    "# Process each line\n",
    "for line in lines:\n",
    "    if line.strip() == \"\":\n",
    "        # If the line is empty, add 11 empty strings\n",
    "        data_list.append([\"\"] * 11)\n",
    "    else:\n",
    "        neighbors = line.split(\", \")\n",
    "        data_list.append(neighbors)\n",
    "\n",
    "# Create the DataFrame for new data\n",
    "new_df = pd.DataFrame(data_list, columns=[f\"Neighbor_{i}\" for i in range(1, 12)])\n",
    "\n",
    "# Path to the existing CSV file\n",
    "existing_csv_file_path = ntc_country_list_file\n",
    "\n",
    "# Read the existing CSV file into a DataFrame\n",
    "existing_df = pd.read_csv(existing_csv_file_path)\n",
    "\n",
    "# Concatenate the existing DataFrame with the new DataFrame horizontally\n",
    "combined_df = pd.concat([existing_df, new_df], axis=1)\n",
    "\n",
    "# Save the combined DataFrame back to the CSV file\n",
    "combined_df.to_csv(existing_csv_file_path, index=False)\n",
    "print(f\"Data appended to '{existing_csv_file_path}' as new columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51abf96-5094-435e-948f-5e1ea885518f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    6. Raw Data Download\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Donwloading the cross border flows raw data. \n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- The crows border flow data is downloaded in separate files for each country.\n",
    "<br>\n",
    "- Since the Acronym of Grece in the downloaded data is 'GR' and the Dispa-SET format for the country is 'EL'. All the needed changes in the used variables are done.\n",
    "<br>\n",
    "- The downloaded files will be joined into a single csv file under the name of the country which the flow comes from.\n",
    "<br>\n",
    "- The headers of these joined csv files are changed accordign the Dispa-SET cross border flow data format e.g. BE -> DE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7a37faa-8f4e-4cf6-98af-f10f1bab227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for AT to CZ saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/AT/CZ.csv'\n",
      "Data for AT to DE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/AT/DE.csv'\n",
      "Data for AT to HU saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/AT/HU.csv'\n",
      "Data for AT to IT saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/AT/IT.csv'\n",
      "Data for AT to SI saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/AT/SI.csv'\n",
      "Data for AT to CH saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/AT/CH.csv'\n",
      "Data for BE to FR saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/BE/FR.csv'\n",
      "Data for BE to DE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/BE/DE.csv'\n",
      "Data for BE to LU saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/BE/LU.csv'\n",
      "Data for BE to NL saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/BE/NL.csv'\n",
      "Data for BE to UK saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/BE/UK.csv'\n",
      "Data for BG to GR saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/BG/GR.csv'\n",
      "Data for BG to RO saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/BG/RO.csv'\n",
      "Data for CH to AT saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/CH/AT.csv'\n",
      "Data for CH to FR saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/CH/FR.csv'\n",
      "Data for CH to DE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/CH/DE.csv'\n",
      "Data for CH to IT saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/CH/IT.csv'\n",
      "Data for CZ to AT saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/CZ/AT.csv'\n",
      "Data for CZ to DE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/CZ/DE.csv'\n",
      "Data for CZ to PL saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/CZ/PL.csv'\n",
      "Data for CZ to SK saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/CZ/SK.csv'\n",
      "Data for DE to AT saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/AT.csv'\n",
      "Data for DE to BE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/BE.csv'\n",
      "Data for DE to CZ saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/CZ.csv'\n",
      "Data for DE to DK saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/DK.csv'\n",
      "Data for DE to FR saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/FR.csv'\n",
      "Data for DE to LU saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/LU.csv'\n",
      "Data for DE to NL saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/NL.csv'\n",
      "Data for DE to NO saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/NO.csv'\n",
      "Data for DE to PL saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/PL.csv'\n",
      "Data for DE to SE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/SE.csv'\n",
      "Data for DE to CH saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DE/CH.csv'\n",
      "Data for DK to DE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DK/DE.csv'\n",
      "Data for DK to NL saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DK/NL.csv'\n",
      "Data for DK to NO saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DK/NO.csv'\n",
      "Data for DK to SE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DK/SE.csv'\n",
      "Data for DK to UK saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/DK/UK.csv'\n",
      "Data for EE to FI saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/EE/FI.csv'\n",
      "Data for EE to LV saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/EE/LV.csv'\n",
      "Data for GR to BG saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/GR/BG.csv'\n",
      "Data for GR to IT saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/GR/IT.csv'\n",
      "Data for ES to FR saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/ES/FR.csv'\n",
      "Data for ES to PT saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/ES/PT.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connection Error, retrying in 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for FI to EE saved to '/home/ray/Dispa-SET_Unleash/RawData/NTC/Raw_Data_Sources/2023/FI/EE.csv'\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:791\u001b[0m in \u001b[1;35murlopen\u001b[0m\n    response = self._make_request(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:537\u001b[0m in \u001b[1;35m_make_request\u001b[0m\n    response = conn.getresponse()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m in \u001b[1;35mgetresponse\u001b[0m\n    httplib_response = super().getresponse()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/http/client.py:1386\u001b[0m in \u001b[1;35mgetresponse\u001b[0m\n    response.begin()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/http/client.py:325\u001b[0m in \u001b[1;35mbegin\u001b[0m\n    version, status, reason = self._read_status()\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:294\u001b[0;36m in \u001b[0;35m_read_status\u001b[0;36m\n\u001b[0;31m    raise RemoteDisconnected(\"Remote end closed connection without\"\u001b[0;36m\n",
      "\u001b[0;31mRemoteDisconnected\u001b[0m\u001b[0;31m:\u001b[0m Remote end closed connection without response\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m in \u001b[1;35msend\u001b[0m\n    resp = conn.urlopen(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:845\u001b[0m in \u001b[1;35murlopen\u001b[0m\n    retries = retries.increment(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py:470\u001b[0m in \u001b[1;35mincrement\u001b[0m\n    raise reraise(type(error), error, _stacktrace)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/util.py:38\u001b[0m in \u001b[1;35mreraise\u001b[0m\n    raise value.with_traceback(tb)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:791\u001b[0m in \u001b[1;35murlopen\u001b[0m\n    response = self._make_request(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:537\u001b[0m in \u001b[1;35m_make_request\u001b[0m\n    response = conn.getresponse()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m in \u001b[1;35mgetresponse\u001b[0m\n    httplib_response = super().getresponse()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/http/client.py:1386\u001b[0m in \u001b[1;35mgetresponse\u001b[0m\n    response.begin()\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/http/client.py:325\u001b[0m in \u001b[1;35mbegin\u001b[0m\n    version, status, reason = self._read_status()\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:294\u001b[0;36m in \u001b[0;35m_read_status\u001b[0;36m\n\u001b[0;31m    raise RemoteDisconnected(\"Remote end closed connection without\"\u001b[0;36m\n",
      "\u001b[0;31mProtocolError\u001b[0m\u001b[0;31m:\u001b[0m ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  Cell \u001b[1;32mIn[28], line 32\u001b[0m\n    net_transfer = client.query_crossborder_flows(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/entsoe/decorators.py:124\u001b[0m in \u001b[1;35myear_wrapper\u001b[0m\n    frame = func(*args, start=_start, end=_end, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/entsoe/entsoe.py:1447\u001b[0m in \u001b[1;35mquery_crossborder_flows\u001b[0m\n    text = super(EntsoePandasClient, self).query_crossborder_flows(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/entsoe/entsoe.py:493\u001b[0m in \u001b[1;35mquery_crossborder_flows\u001b[0m\n    return self._query_crossborder(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/entsoe/entsoe.py:710\u001b[0m in \u001b[1;35m_query_crossborder\u001b[0m\n    response = self._base_request(params=params, start=start, end=end)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/entsoe/decorators.py:36\u001b[0m in \u001b[1;35mretry_wrapper\u001b[0m\n    raise error\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/entsoe/decorators.py:24\u001b[0m in \u001b[1;35mretry_wrapper\u001b[0m\n    result = func(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/entsoe/entsoe.py:96\u001b[0m in \u001b[1;35m_base_request\u001b[0m\n    response = self.session.get(url=URL, params=params,\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:602\u001b[0m in \u001b[1;35mget\u001b[0m\n    return self.request(\"GET\", url, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m in \u001b[1;35mrequest\u001b[0m\n    resp = self.send(prep, **send_kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m in \u001b[1;35msend\u001b[0m\n    r = adapter.send(request, **kwargs)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:501\u001b[0;36m in \u001b[0;35msend\u001b[0;36m\n\u001b[0;31m    raise ConnectionError(err, request=request)\u001b[0;36m\n",
      "\u001b[0;31mConnectionError\u001b[0m\u001b[0;31m:\u001b[0m ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    }
   ],
   "source": [
    "# Define the start and end timestamps using the data_year variable\n",
    "start = pd.Timestamp(f'{data_year}0101', tz='Europe/Brussels')\n",
    "end = pd.Timestamp(f'{data_year_1}0101', tz='Europe/Brussels')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(ntc_country_list_file)\n",
    "\n",
    "# Ensure the necessary columns exist\n",
    "required_columns = ['Country_From', 'Country_Folder'] + [f'Neighbor_{i}' for i in range(1, 12)]\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' does not exist in the CSV file\")\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    country_code_from = row['Country_From']\n",
    "    country_folder = row['Country_Folder']\n",
    "    \n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(country_folder, exist_ok=True)\n",
    "    \n",
    "    # Iterate through each neighbor column\n",
    "    for neighbor_col in [f'Neighbor_{i}' for i in range(1, 12)]:\n",
    "        country_code_to = row[neighbor_col]\n",
    "        \n",
    "        # Skip if the neighbor field is empty\n",
    "        if pd.isna(country_code_to):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Query crossborder flows\n",
    "            net_transfer = client.query_crossborder_flows(\n",
    "                country_code_from, country_code_to, start=start, end=end\n",
    "            )\n",
    "            \n",
    "            # Convert the index to a column\n",
    "            net_transfer = net_transfer.reset_index()\n",
    "            \n",
    "            # Define the output file path\n",
    "            output_file = os.path.join(country_folder, f'{country_code_to}.csv')\n",
    "            \n",
    "            # Save the DataFrame to a CSV file, including the index as a column\n",
    "            net_transfer.to_csv(output_file, index=False)\n",
    "\n",
    "            print(f\"Data for {country_code_from} to {country_code_to} saved to '{output_file}'\")\n",
    "        \n",
    "        except NoMatchingDataError:\n",
    "            print(f\"No matching data for {country_code_from} to {country_code_to} for the period {start} to {end}. Skipping.\")\n",
    "\n",
    "print(\"All data has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7d655-1f47-4729-ac12-9550c08e5a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(ntc_country_list_file)\n",
    "\n",
    "# Replace 'GR' with 'EL' in the entire DataFrame\n",
    "df = df.applymap(lambda x: x.replace('GR', 'EL') if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(ntc_country_list_file, index=False)\n",
    "\n",
    "print(f\"Replacements made and file saved: {ntc_country_list_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c431d-6d5f-464e-914b-5a4116440fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walk through the directory tree\n",
    "for root, dirs, files in os.walk(ntc_raw_data_folder_path, topdown=False):\n",
    "    # Rename files\n",
    "    for name in files:\n",
    "        if 'GR' in name:\n",
    "            new_name = name.replace('GR', 'EL')\n",
    "            old_file_path = os.path.join(root, name)\n",
    "            new_file_path = os.path.join(root, new_name)\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed file: {old_file_path} to {new_file_path}\")\n",
    "\n",
    "    # Rename directories\n",
    "    for name in dirs:\n",
    "        if 'GR' in name:\n",
    "            new_name = name.replace('GR', 'EL')\n",
    "            old_dir_path = os.path.join(root, name)\n",
    "            new_dir_path = os.path.join(root, new_name)\n",
    "            os.rename(old_dir_path, new_dir_path)\n",
    "            print(f\"Renamed directory: {old_dir_path} to {new_dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3884da9-281b-4848-ae8c-1b1755b1ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace 'GR' with 'EL'\n",
    "def replace_gr_with_el(lst):\n",
    "    return ['EL' if x == 'GR' else x for x in lst]\n",
    "\n",
    "# Applying the function to both lists\n",
    "data = replace_gr_with_el(data)\n",
    "ntc_per_unit_country_list = replace_gr_with_el(ntc_per_unit_country_list)\n",
    "\n",
    "# Print the updated lists\n",
    "print(\"Updated data list:\", data)\n",
    "print(\"Updated ntc_per_unit_country_list:\", ntc_per_unit_country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835c832-e4be-4216-a857-c826dfce40d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(ntc_country_list_file)\n",
    "\n",
    "# Ensure the column 'Country_Folder' exists\n",
    "if 'Country_Folder' not in df.columns:\n",
    "    raise ValueError(\"Column 'Country_Folder' does not exist in the CSV file\")\n",
    "\n",
    "# Function to join CSV files in a directory\n",
    "def join_csv_files_in_directory(directory_path):\n",
    "    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    # Read all CSV files into DataFrames\n",
    "    dataframes = {csv_file: pd.read_csv(os.path.join(directory_path, csv_file)) for csv_file in csv_files}\n",
    "    \n",
    "    # Find the CSV file with the largest number of rows\n",
    "    largest_file = max(dataframes, key=lambda x: len(dataframes[x]))\n",
    "    base_df = dataframes[largest_file].iloc[:, :2].copy()\n",
    "    base_df.columns = [base_df.columns[0], largest_file.replace('.csv', '')]\n",
    "    \n",
    "    # Merge the other CSV files based on the first column\n",
    "    for csv_file, df in dataframes.items():\n",
    "        if csv_file == largest_file:\n",
    "            continue\n",
    "        temp_df = df.iloc[:, [0, 1]]\n",
    "        temp_df.columns = [temp_df.columns[0], csv_file.replace('.csv', '')]\n",
    "        base_df = pd.merge(base_df, temp_df, on=base_df.columns[0], how='left')\n",
    "    \n",
    "    return base_df\n",
    "\n",
    "# Create a new column for the paths of the new CSV files\n",
    "df['Country_File_Path'] = ''\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    country_folder = row['Country_Folder']\n",
    "    \n",
    "    # Join CSV files in the directory\n",
    "    joined_df = join_csv_files_in_directory(country_folder)\n",
    "    \n",
    "    if joined_df is not None:\n",
    "        # Define the output file path\n",
    "        output_file = os.path.join(country_folder, f\"{os.path.basename(country_folder)}.csv\")\n",
    "        \n",
    "        # Save the joined DataFrame to a new CSV file\n",
    "        joined_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Update the DataFrame with the path of the new CSV file\n",
    "        df.at[index, 'Country_File_Path'] = output_file\n",
    "\n",
    "        print(f\"Joined CSV file saved to '{output_file}'\")\n",
    "\n",
    "# Save the updated DataFrame back to the main CSV file\n",
    "df.to_csv(ntc_country_list_file, index=False)\n",
    "\n",
    "print(\"All data has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b0461-7364-4b74-93f3-0a396a13b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the main CSV file into a DataFrame\n",
    "df = pd.read_csv(ntc_country_list_file)\n",
    "\n",
    "# Ensure the required columns exist\n",
    "if 'Country_From' not in df.columns or 'Country_File_Path' not in df.columns:\n",
    "    raise ValueError(\"The CSV file must contain 'Country_From' and 'Country_File_Path' columns.\")\n",
    "\n",
    "# Function to update the headers of a CSV file\n",
    "def update_csv_headers(file_path, new_header_prefix):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    csv_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get the current headers\n",
    "    current_headers = csv_df.columns.tolist()\n",
    "    \n",
    "    # Create new headers for columns from the second column onward\n",
    "    new_headers = [current_headers[0]] + [f\"{new_header_prefix} -> {col}\" for col in current_headers[1:]]\n",
    "    \n",
    "    # Update the DataFrame with the new headers\n",
    "    csv_df.columns = new_headers\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    csv_df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated headers in '{file_path}'\")\n",
    "\n",
    "# Iterate through each row in the main DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    country_from = row['Country_From']\n",
    "    country_file_path = row['Country_File_Path']\n",
    "    \n",
    "    # Check if the file path is not empty and exists\n",
    "    if pd.notna(country_file_path) and os.path.exists(country_file_path):\n",
    "        update_csv_headers(country_file_path, country_from)\n",
    "    else:\n",
    "        print(f\"File path '{country_file_path}' does not exist or is empty. Skipping...\")\n",
    "\n",
    "print(\"All CSV files have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80667de7-f250-40b5-a1ab-ed3f2d57628a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    7. Raw Data Format\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Addapting the time step data to the UTC for all the countries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7963855-5bd9-4488-b5f5-40de4a3b4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(ntc_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Define the function to convert time to UTC\n",
    "def convert_to_utc(time_str):\n",
    "    local_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "    utc_time = local_time.astimezone(pytz.utc)\n",
    "    return utc_time.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# Process each CSV file\n",
    "for file_path in country_list_df['Country_File_Path'].dropna():\n",
    "    # Ensure the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the 'index' column exists\n",
    "    if 'index' not in df.columns:\n",
    "        print(f\"'index' column not found in file: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert the 'index' column to UTC\n",
    "    df['index'] = df['index'].apply(convert_to_utc)\n",
    "    \n",
    "    # Save the updated CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d670d7e-6222-4fb1-baf0-eb038853da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(ntc_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Function to update the year in the 'index' column\n",
    "def update_index_year(df, data_year):\n",
    "    # Ensure the 'index' column exists\n",
    "    if 'index' not in df.columns:\n",
    "        raise ValueError(\"'index' column not found in DataFrame\")\n",
    "    \n",
    "    # Update the year in the 'index' column\n",
    "    df['index'] = df['index'].apply(lambda x: f\"{data_year}{x[4:]}\" if str(x)[:4] != str(data_year) else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each CSV file specified in the 'Country_File_Path' column\n",
    "for file_path in country_list_df['Country_File_Path'].dropna():\n",
    "    # Ensure the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure there are enough rows to move the first four rows to the last\n",
    "    if len(df) < 4:\n",
    "        print(f\"Not enough rows to process in file: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract the first four rows (excluding headers)\n",
    "    first_four_rows = df.iloc[:4].copy()\n",
    "    \n",
    "    # Drop the first four rows from the DataFrame\n",
    "    df = df.iloc[4:].reset_index(drop=True)\n",
    "    \n",
    "    # Append the first_four_rows to the end of the DataFrame\n",
    "    df = pd.concat([df, first_four_rows]).reset_index(drop=True)\n",
    "    \n",
    "    # Update the 'index' column year\n",
    "    df = update_index_year(df, data_year)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12470b-5500-4767-860b-76dff3017dc1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    7. Net Transfer Capacities Clean File\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Joining all the cros border flows data to a single csv file with named as the analized year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fd6f2-ef2c-4b13-9031-7a147b12f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(ntc_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Process each CSV file specified in the 'Country_File_Path' column\n",
    "file_paths = country_list_df['Country_File_Path'].dropna().tolist()\n",
    "\n",
    "# Identify the CSV file with the largest number of rows\n",
    "max_rows = 0\n",
    "base_df = None\n",
    "for file_path in file_paths:\n",
    "    # Ensure the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > max_rows:\n",
    "            max_rows = len(df)\n",
    "            base_df = df.copy()\n",
    "\n",
    "# If no base_df was found, raise an error\n",
    "if base_df is None:\n",
    "    raise ValueError(\"No valid CSV files found.\")\n",
    "\n",
    "# Initialize the combined DataFrame with the first column from the base DataFrame\n",
    "combined_df = pd.DataFrame(base_df.iloc[:, 0])\n",
    "combined_df.columns = [base_df.columns[0]]  # Keep the original name of the first column\n",
    "\n",
    "# Add data from each CSV file to the combined DataFrame\n",
    "for file_path in file_paths:\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Merge the data based on the first column\n",
    "        combined_df = pd.merge(combined_df, df, on=base_df.columns[0], how='left')\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file named after the data_year variable\n",
    "output_file_path = os.path.join(ntc_folder_path, f\"{data_year}.csv\")\n",
    "combined_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Combined CSV file saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6caaf-602e-40a1-b46a-ae1620b6d9fc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Dividing the clean data in time stepts of 15 minutes, 30 minutes, and 1 hour.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2aa826-0a03-4dd0-a746-53771e73cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = os.path.join(ntc_folder_path, f'{data_year}.csv')\n",
    "\n",
    "# Create the new directories\n",
    "intervals = ['1h', '30min', '15min']\n",
    "for interval in intervals:\n",
    "    os.makedirs(os.path.join(ntc_folder_path, interval), exist_ok=True)\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Convert the 'index' column to datetime\n",
    "df['index'] = pd.to_datetime(df['index'], format='%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# Function to extract rows at a specific time step and save to a new CSV file\n",
    "def extract_and_save(df, interval, folder_name):\n",
    "    # Resample the DataFrame\n",
    "    resampled_df = df.set_index('index').resample(interval).first().reset_index()\n",
    "    \n",
    "    # Define the new file path\n",
    "    new_file_path = os.path.join(ntc_folder_path, folder_name, f'{data_year}.csv')\n",
    "    \n",
    "    # Save the resampled DataFrame to the new CSV file\n",
    "    resampled_df.to_csv(new_file_path, index=False)\n",
    "    print(f\"File saved: {new_file_path}\")\n",
    "\n",
    "# Extract and save rows at different time steps\n",
    "extract_and_save(df, '1H', '1h')\n",
    "extract_and_save(df, '30T', '30min')\n",
    "extract_and_save(df, '15T', '15min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635513e-ee86-4c66-8df3-65c1e37a7797",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Copying the time already formated Net Transfer Capacities data to the main Dispa-SET data base dirtectory\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d9e2f-718d-4bbd-ae71-6acf9243de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_path_2 = \"/Database/NTC/\"\n",
    "\n",
    "# Construct the power_plants_raw_data_folder_path variable\n",
    "ntc_data_base_folder_path = dispaSET_unleash_folder_path + additional_path_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425eda04-4885-4185-845c-c8ce803aa211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subfolder names\n",
    "subfolders = ['1h', '30min', '15min']\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(data_year, source_base_path, dest_base_path, subfolders):\n",
    "    for subfolder in subfolders:\n",
    "        source_path = os.path.join(source_base_path, subfolder, f\"{data_year}.csv\")\n",
    "        dest_folder_path = os.path.join(dest_base_path, subfolder)\n",
    "\n",
    "        # Create the destination subfolder if it does not exist\n",
    "        os.makedirs(dest_folder_path, exist_ok=True)\n",
    "\n",
    "        dest_path = os.path.join(dest_folder_path, f\"{data_year}.csv\")\n",
    "        \n",
    "        # Copy the file\n",
    "        if os.path.isfile(source_path):\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            print(f\"Copied {source_path} to {dest_path}\")\n",
    "        else:\n",
    "            print(f\"File {source_path} does not exist\")\n",
    "\n",
    "# Call the function\n",
    "copy_files(data_year, ntc_folder_path, ntc_data_base_folder_path, subfolders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cedab8-1c68-47a4-ac14-75887c997814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
