{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9673ddcf-cd4c-4b95-aab5-ab49b5c49514",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman;\">\n",
    "    POWER PLANTS DATA PROCESSING - Main Notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0755a09a-9ba6-4b6b-b0ee-b17760f50078",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Each part of the following script was used to proccess the raw data for power plants units of the Dispa-SET_Unleash project. Read explanation text cells to follow and understand all the process until final results were got stept by step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6a6e6f-51e4-4464-b909-2a18cb89271e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    1. Notebook Set Up\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bb156f-0187-4679-acb0-090bdc623675",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Importing needed libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "124e1ebd-cfd1-40d0-8571-38d1a5997f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from shutil import move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef78e5-6ee4-4015-acc5-f9843445c668",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    2. Dispa-SET_Unleash Folder Path\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4307bb-12f2-4d63-9131-9f3f7b349e4c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Determinning dynamically the zone_folder_path based on the location of the \"Dispa-SET_Unleash\" folder relative to the current working directory. If the \"Dispa-SET_Unleash\" folder is copied to a different machine or location, the dispaSET_unleash_folder_path variable will automatically adjust accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "01352aec-0922-4df8-8203-9aa91494e2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory of \"Dispa-SET_Unleash\"\n",
    "dispaSET_unleash_parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Get the path to the \"Dispa-SET_Unleash\" folder\n",
    "dispaSET_unleash_folder_path = os.path.dirname(dispaSET_unleash_parent_directory)\n",
    "\n",
    "# Construct the dispaSET_unleash_folder_name variable\n",
    "dispaSET_unleash_folder_name = os.path.basename(dispaSET_unleash_folder_path)\n",
    "\n",
    "print(\"dispaSET_unleash_folder_name:\", dispaSET_unleash_folder_name)\n",
    "print(\"dispaSET_unleash_folder_path:\", dispaSET_unleash_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cd9cd-255a-4670-ab92-d0f810f53670",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    3. Zone(s) Creation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee6216-cd5b-4646-954d-5dfe0bd77c1c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Entering the zone name or names (in case of more than one zone wanted to be modelled) to create the folder where all data related to the corresponding zone are going to be storage.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2cf6aa-3601-4b94-98a1-e332c5c411d7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    For European country names use the ISO 3166-1 standars i.e. AT, BE, BG, CH.... etc. to give the zone_name.\n",
    "    <br>\n",
    "    For non European countries it would rather to call the zone_name with the same word of how it is defined in the data to be downloaded and processed. e.g. \n",
    "    <br>\n",
    "    <div style=\"text-align: left; margin-left: 1.50em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "        If it is downloading a csv file with all power plants of Spain but just the units fo Pamplona city are wanted, and in the corresponding donwloaded file, Pamplona is refering with the acronim \"PMPLN\"; set the zone_name variable with the word \"PMPLN\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7e6805cf-9138-445d-abfd-17af3532a341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of folder names to create\n",
    "zone_names = [\"DE\", \n",
    "              \"DK\", \n",
    "              \"CH\",\n",
    "              \"BE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "42dc50eb-dbbe-46e2-878c-94249e2e2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/\n",
      "Created zone: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "Created zone: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DK\n",
      "Created zone: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/CH\n",
      "Created zone: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/BE\n",
      "Created zones:\n",
      "DE: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "DK: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DK\n",
      "CH: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/CH\n",
      "BE: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/BE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DE': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE',\n",
       " 'DK': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DK',\n",
       " 'CH': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/CH',\n",
       " 'BE': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/BE'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original value of dispaSET_unleash_folder_path\n",
    "#dispaSET_unleash_folder_path = \"/home/ray/Dispa-SET_Unleash\"\n",
    "\n",
    "# Additional string to be appended\n",
    "additional_path = \"/RawData/PowerPlants/\"\n",
    "\n",
    "# Construct the power_plants_raw_data_folder_path variable\n",
    "power_plants_raw_data_folder_path = dispaSET_unleash_folder_path + additional_path\n",
    "print(\"power_plants_raw_data_folder_path:\", power_plants_raw_data_folder_path)\n",
    "\n",
    "# Dictionary to store created zone paths\n",
    "created_zones = {}\n",
    "\n",
    "# Create the zone\n",
    "for zone_name in zone_names:\n",
    "    zone_path = os.path.join(power_plants_raw_data_folder_path, zone_name)\n",
    "    os.makedirs(zone_path, exist_ok=True)\n",
    "    created_zones[zone_name] = zone_path\n",
    "    print(f\"Created zone: {zone_path}\")\n",
    "\n",
    "# Print the created zone paths\n",
    "print(\"Created zones:\")\n",
    "for zone_name, zone_path in created_zones.items():\n",
    "    print(f\"{zone_name}: {zone_path}\")\n",
    "    \n",
    "created_zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c72ac-b9ce-4476-90af-83967ca3bfe7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8cc6e9cc-0d2a-4c82-91c8-745bc5a9aafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/\n",
      "zone_names: ['DE', 'DK', 'CH', 'BE']\n",
      "created_zones: {'DE': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE', 'DK': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DK', 'CH': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/CH', 'BE': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/BE'}\n"
     ]
    }
   ],
   "source": [
    "print (f\"dispaSET_unleash_folder_name: {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path: {dispaSET_unleash_folder_path}\")\n",
    "print (f\"power_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\")\n",
    "print (f\"zone_names: {zone_names}\")\n",
    "print (f\"created_zones: {created_zones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d5364-f18b-420e-9634-b2ee4d109781",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    4. Download Link Sources\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c323d8-ad26-477e-b9a6-a623dac42cca",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Entering the all the download links of where the raw data is content.\n",
    "    <br>\n",
    "        That list is going to be saved to be used as input for next stages.\n",
    "      <br>\n",
    "    <div style=\"text-align: left; margin-left: 1.50em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "        Notice that to process the data all the links has to download .csv files.\n",
    "    <br>\n",
    "    <div style=\"text-align: left; margin-left: -1.5em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    In the other hand, it is important to define which zone is refering the download link sources.\n",
    "    <br>\n",
    "    <div style=\"text-align: left; margin-left: 1.5em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    If all the downloaded file contents data that belongs to only one zone, epecify it in the variable download_links_zone_related applying the same order than the variable download_links.\n",
    "    <br>\n",
    "    If the downloaded file contents data that refers different zones at the same time, specify it with the word \"General\" in the variable download_links_zone_related using the same order than the variable download_links.\n",
    "    <br>\n",
    "    <div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    Remember that the next filtering stages depend on the correct setting of this step.\n",
    "    <br>\n",
    "    <div style=\"text-align: left; margin-left: -1.5em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Additionally indicate the year of all data is referring to.\n",
    "    <br>\n",
    "    <div style=\"text-align: left; margin-left: 1.5em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    This is going to be used as the name root under which all next files are going to be created.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "26f91f27-5f40-472e-b099-e61c224daa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of the download links:\n",
    "download_links = [\n",
    "    'https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_EU.csv',\n",
    "    'https://data.open-power-system-data.org/renewable_power_plants/2020-08-25/renewable_power_plants_DE.csv',\n",
    "    'https://data.open-power-system-data.org/renewable_power_plants/2020-08-25/renewable_power_plants_DK.csv',\n",
    "    'https://data.open-power-system-data.org/renewable_power_plants/2020-08-25/renewable_power_plants_CH.csv',\n",
    "    'https://opendata.elia.be/api/explore/v2.1/catalog/datasets/ods036/exports/csv?lang=en&timezone=Europe%2FBrussels&use_labels=true&delimiter=%3B'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c431ed5d-3b8a-4914-a379-ad28a0406bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of zones related to the download links:\n",
    "download_links_zone_related = [\n",
    "    'General',\n",
    "    'DE',\n",
    "    'DK',\n",
    "    'CH',\n",
    "    'BE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f9dad09d-7a34-46ca-afd8-3d5392ccdcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year to which data refers to:\n",
    "data_year = '2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0cd4f487-2c59-4c46-bc9c-5122f296e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download links saved to: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/2020_power_plants_raw_data_sources_20240405_083658/2020_power_plants_raw_data_sources_20240405_083658.csv\n",
      "File path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/2020_power_plants_raw_data_sources_20240405_083658/2020_power_plants_raw_data_sources_20240405_083658.csv\n",
      "File name: 2020_power_plants_raw_data_sources_20240405_083658.csv\n"
     ]
    }
   ],
   "source": [
    "def save_download_links_to_csv(links, zones, folder_path, data_year):\n",
    "    # Create the filename using the data year, current date, and time\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    file_name = f\"{data_year}_power_plants_raw_data_sources_{timestamp}.csv\"\n",
    "    \n",
    "    # Create a folder with the same name as the file (without extension)\n",
    "    folder_name = os.path.splitext(file_name)[0]\n",
    "    folder_path = os.path.join(folder_path, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Combine the folder path and filename\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Write links to CSV file\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow(['Download_Link_Sources', 'Zone', 'File_Name'])\n",
    "        \n",
    "        # Write the links, zones, and file names\n",
    "        for i, (link, zone) in enumerate(zip(links, zones), start=1):\n",
    "            writer.writerow([link, zone, i])\n",
    "    \n",
    "    print(f\"Download links saved to: {file_path}\")\n",
    "    \n",
    "    return file_path, file_name\n",
    "\n",
    "# Save the download links to a CSV file and get the file path and name\n",
    "power_plants_raw_data_sources_file_path, power_plants_raw_data_sources_file_name = save_download_links_to_csv(download_links, download_links_zone_related, power_plants_raw_data_folder_path, data_year)\n",
    "\n",
    "print(\"File path:\", power_plants_raw_data_sources_file_path)\n",
    "print(\"File name:\", power_plants_raw_data_sources_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d8108-9537-49f5-8475-0928b96d36b2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "33b7ddb5-df65-46de-aad3-042b6b0b1cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/\n",
      "zone_names: ['DE', 'DK', 'CH', 'BE']\n",
      "created_zones: {'DE': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE', 'DK': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DK', 'CH': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/CH', 'BE': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/BE'}\n",
      "power_plants_raw_data_sources_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/2020_power_plants_raw_data_sources_20240405_083658/2020_power_plants_raw_data_sources_20240405_083658.csv\n",
      "power_plants_raw_data_sources_file_name: 2020_power_plants_raw_data_sources_20240405_083658.csv\n",
      "data_year: 2020\n",
      "download_links_zone_related: ['General', 'DE', 'DK', 'CH', 'BE']\n"
     ]
    }
   ],
   "source": [
    "print (f\"dispaSET_unleash_folder_name: {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path: {dispaSET_unleash_folder_path}\")\n",
    "print (f\"power_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\")\n",
    "print (f\"zone_names: {zone_names}\")\n",
    "print (f\"created_zones: {created_zones}\")\n",
    "print (f\"power_plants_raw_data_sources_file_path: {power_plants_raw_data_sources_file_path}\")\n",
    "print (f\"power_plants_raw_data_sources_file_name: {power_plants_raw_data_sources_file_name}\")\n",
    "print (f\"data_year: {data_year}\")\n",
    "print (f\"download_links_zone_related: {download_links_zone_related}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801c12b-f6f9-4fb4-920a-53541fb3bafd",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    5. Power Plants Raw Data Download Files\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b01bd-b83d-4089-a4c9-a01f9033201d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Using the download list given previously to download and save all the power units raw data files inside a folder called as is it specified in the variable power_plants raw_data_sources _file_path.\n",
    "    <br>\n",
    "    All the downloaded files are named under the ordering of the download_links list.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "81cf289f-8b9d-4f31-b8d8-eaec45f58f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '1' downloaded and saved successfully.\n",
      "File '2' downloaded and saved successfully.\n",
      "File '3' downloaded and saved successfully.\n",
      "File '4' downloaded and saved successfully.\n",
      "File '5' downloaded and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "def download_files_from_csv(csv_file_path):\n",
    "    # Create a folder to save downloaded files\n",
    "    download_folder = os.path.dirname(csv_file_path)\n",
    "    \n",
    "    # Open and read the CSV file\n",
    "    with open(csv_file_path, 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Iterate over each row\n",
    "        for row in reader:\n",
    "            download_link = row['Download_Link_Sources']\n",
    "            file_name = row['File_Name']\n",
    "            \n",
    "            # Download the file from the URL\n",
    "            response = requests.get(download_link)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                # Save the downloaded file\n",
    "                file_path = os.path.join(download_folder, file_name)\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"File '{file_name}' downloaded and saved successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to download file from '{download_link}'.\")\n",
    "\n",
    "# Path to the recently created CSV file\n",
    "recently_created_csv_file_path = power_plants_raw_data_sources_file_path\n",
    "\n",
    "# Call the function to download files from the CSV\n",
    "download_files_from_csv(recently_created_csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1628f4f-2a1d-4a15-9711-af79cfcb0712",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    6. Zone Classification\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b79d84-437d-44a6-a899-dd7ecbd0c9e4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Filtering the data contained in each downloaded file accordingly the zone previously specified. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033fab2-fa04-4214-8e54-cabc154238b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.5em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;\">\n",
    "    6.1. Zone Definition\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c451a-82cc-4b5c-8de7-9cf8dff9fe41",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.5em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Adding a new column under the name \"Country\" to each downloaded file which have been relationated with a key Zone in the list download_links_zone_related fulfilled previously\n",
    "    <br>\n",
    "    For all those files that have been set with the key \"General\", it is assumed that so file contains data from various zones, so It will be filtered in a different way.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "260e6277-71d9-4725-932c-72c55f228ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No action needed for 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88653/1710078625.py:20: DtypeWarning: Columns (14,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_csv = pd.read_csv(csv_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'Country' column to 2 with value 'DE'\n",
      "Added 'Country' column to 3 with value 'DK'\n",
      "Added 'Country' column to 4 with value 'CH'\n",
      "Added 'Country' column to 5 with value 'BE'\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file specified in power_plants_raw_data_sources_file_path\n",
    "df_sources = pd.read_csv(power_plants_raw_data_sources_file_path)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_sources.iterrows():\n",
    "    file_name = str(row['File_Name'])  # Convert to string\n",
    "    zone = row['Zone']\n",
    "    \n",
    "    # Check if the zone is not \"General\"\n",
    "    if zone != \"General\":\n",
    "        # Construct the path to the corresponding CSV file\n",
    "        csv_file_path = os.path.join(os.path.dirname(power_plants_raw_data_sources_file_path), file_name)\n",
    "        \n",
    "        # Check if the CSV file exists\n",
    "        if os.path.exists(csv_file_path):\n",
    "            # Read the CSV file\n",
    "            df_csv = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            # Add a new column \"Country\" with the value from the \"Zone\" column\n",
    "            df_csv['Country'] = zone\n",
    "            \n",
    "            # Write the updated DataFrame back to the CSV file\n",
    "            df_csv.to_csv(csv_file_path, index=False)\n",
    "            \n",
    "            print(f\"Added 'Country' column to {file_name} with value '{zone}'\")\n",
    "        else:\n",
    "            print(f\"CSV file {file_name} does not exist.\")\n",
    "    else:\n",
    "        print(f\"No action needed for {file_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac2bbe1-e14b-440a-91b8-46f8113406a7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "bf073512-a22b-444f-a888-b83e46e3da37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/\n",
      "zone_names: ['DE', 'DK', 'CH', 'BE']\n",
      "created_zones: {'DE': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE', 'DK': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DK', 'CH': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/CH', 'BE': '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/BE'}\n",
      "power_plants_raw_data_sources_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/2020_power_plants_raw_data_sources_20240405_083658/2020_power_plants_raw_data_sources_20240405_083658.csv\n",
      "power_plants_raw_data_sources_file_name: 2020_power_plants_raw_data_sources_20240405_083658.csv\n",
      "data_year: 2020\n",
      "download_links_zone_related: ['General', 'DE', 'DK', 'CH', 'BE']\n"
     ]
    }
   ],
   "source": [
    "print (f\"dispaSET_unleash_folder_name: {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path: {dispaSET_unleash_folder_path}\")\n",
    "print (f\"power_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\")\n",
    "print (f\"zone_names: {zone_names}\")\n",
    "print (f\"created_zones: {created_zones}\")\n",
    "print (f\"power_plants_raw_data_sources_file_path: {power_plants_raw_data_sources_file_path}\")\n",
    "print (f\"power_plants_raw_data_sources_file_name: {power_plants_raw_data_sources_file_name}\")\n",
    "print (f\"data_year: {data_year}\")\n",
    "print (f\"download_links_zone_related: {download_links_zone_related}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17f7261-6553-44a9-bec6-8e6e0d1c5857",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.5em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;\">\n",
    "    6.2. Raw Data File Zone Classification\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef588fdb-0222-49e2-b6e9-ae53b4dea77b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.5em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Moving each downloade file to its corresponding folder zone accordinlgy the ownload_links_zone_related list.\n",
    "    <br>\n",
    "    The files relationed to the key \"General\" just will keep their current location.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f79b72-1a42-4370-b385-0c8a81b829c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file specified in power_plants_raw_data_sources_file_path\n",
    "df_sources = pd.read_csv(power_plants_raw_data_sources_file_path)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_sources.iterrows():\n",
    "    file_name = str(row['File_Name'])  # Convert to string\n",
    "    zone = row['Zone']\n",
    "    file_path = os.path.join(os.path.dirname(power_plants_raw_data_sources_file_path), file_name)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Open and read the file\n",
    "        df_csv = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the file has the header 'Country'\n",
    "        if 'Country' in df_csv.columns:\n",
    "            # Get the corresponding value of 'Zone'\n",
    "            zone_value = row['Zone']\n",
    "            \n",
    "            # Check if the zone folder exists\n",
    "            if zone_value in created_zones:\n",
    "                # Construct the destination folder path\n",
    "                destination_folder = created_zones[zone_value]\n",
    "                \n",
    "                # Move the file to the destination folder\n",
    "                destination_file_path = os.path.join(destination_folder, file_name)\n",
    "                move(file_path, destination_file_path)\n",
    "                print(f\"Moved file '{file_name}' to '{destination_folder}'\")\n",
    "            else:\n",
    "                print(f\"Destination folder for zone '{zone_value}' does not exist.\")\n",
    "        else:\n",
    "            print(f\"No 'Country' header found in file '{file_name}'. No action needed.\")\n",
    "    else:\n",
    "        print(f\"File '{file_name}' does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c450b-fe13-4ece-bde0-e6bbe0555e10",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    7. Data Formating\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78730c7a-354d-4431-b90a-5c1d5691da49",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.5em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;\">\n",
    "    7.1. Clean Data File Creation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f95902-be45-4c99-b76b-166fa14c69f4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.5em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Creating for each zone an empty csv file with all the technical features needed for Dispa-SET simulations as headers.\n",
    "    <br>\n",
    "    This file will be named under the value of the variable data year previously specified\n",
    "    <br>\n",
    "    On this csv file all the filtered data in the following steps will be written.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "20b0b19b-d93b-488b-bfa3-5d6f0fe4a957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created CSV file at: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "Created CSV file at: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DK/2020.csv\n",
      "Created CSV file at: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/CH/2020.csv\n",
      "Created CSV file at: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/BE/2020.csv\n",
      "Created CSV file at: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/2020_power_plants_raw_data_sources_20240405_083658/2020.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/2020_power_plants_raw_data_sources_20240405_083658/2020.csv'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the headers for the new CSV files\n",
    "headers = [\n",
    "    \"\", \"Unit\", \"PowerCapacity\", \"Nunits\", \"Zone\", \"Zone_th\", \"Zone_h2\", \"Technology\", \"Fuel\", \"Efficiency\",\n",
    "    \"MinUpTime\", \"MinDownTime\", \"RampUpRate\", \"RampDownRate\", \"StartUpCost\", \"NoLoadCost_pu\", \"RampingCost\",\n",
    "    \"PartLoadMin\", \"MinEfficiency\", \"StartUpTime\", \"CO2Intensity\", \"CHPType\", \"CHPPowerToHeat\",\n",
    "    \"CHPPowerLossFactor\", \"CHPMaxHeat\", \"COP\", \"Tnominal\", \"coef_COP_a\", \"coef_COP_b\", \"STOCapacity\",\n",
    "    \"STOSelfDischarge\", \"STOMaxChargingPower\", \"STOChargingEfficiency\", \"WaterWithdrawal\", \"WaterConsumption\", \"Status\", \"Source\",\n",
    "    \"Company\"\n",
    "]\n",
    "\n",
    "# Function to create CSV file in a given folder\n",
    "def create_csv_file(folder_path, data_year):\n",
    "    # Construct the file path for the new CSV file\n",
    "    csv_file_path = os.path.join(folder_path, f\"{data_year}.csv\")\n",
    "    \n",
    "    # Write the headers to the new CSV file\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(headers)\n",
    "    \n",
    "    print(f\"Created CSV file at: {csv_file_path}\")\n",
    "    return csv_file_path\n",
    "\n",
    "# Iterate over each zone name and its corresponding folder path\n",
    "for zone_name, folder_path in created_zones.items():\n",
    "    create_csv_file(folder_path, data_year)\n",
    "\n",
    "# Create a CSV file in the folder of power_plants_raw_data_sources_file_path\n",
    "create_csv_file(os.path.dirname(power_plants_raw_data_sources_file_path), data_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7db60a-0dfe-48ea-af65-c336631a04ba",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.5em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;\">\n",
    "    7.2. General Raw Data Files Zone Classification\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f267fb0-abb8-4a9f-b025-ac136201adbc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.5em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Coping all the data from all the downloaded files set with the key \"General\" to the cleand clean data file in their corresponding folde zone.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f43b57-9782-4aca-89c9-517943141237",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 1.5em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman;\">\n",
    "    6.5. Clean Data File Creation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d480d94-e558-4c80-b688-b6c5c517a7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f39925-beac-41eb-934a-23655f3cc5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf48d85-dde7-42fc-856b-3657050fe430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d1252-9428-4236-b0f7-3e635e00404c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa96616-36dc-4d81-881e-328c288664bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107dfc6f-7821-4c9d-95b5-2727650c71de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c9525-b824-4381-a6ad-3e4b215994e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47512e9-c156-4017-8ed6-346c4993a1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210ee92-384d-4180-8ac9-fd9299bde13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac048b7-468a-4796-bfab-00dd5ce67bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bae99a-7442-433e-aa10-7c827c3d5fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3d2c7-7ddd-4b0d-990e-b683eb07e8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c83423-c558-4511-a58b-9ccafd7e6a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333b54f-4784-4d6c-9764-e5a7e5f4d40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86029cf6-07a3-4b16-b4ed-c0899e3e7ab5",
   "metadata": {},
   "source": [
    "            3. POWER PLANTS DATA FORMATING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f8b3d-9000-46aa-8423-6b03c144f026",
   "metadata": {},
   "source": [
    "3.1. Defining variable as the file name of all clean data and the directory where it will be stored. \n",
    "    \n",
    "    It is recommended to give the year which the data is refering to as the name of the power plants clean data file. \n",
    "    Aditionally, take account to give the appropiate file extension to the downloaded file name, .csv in this case:\n",
    "\n",
    "    power_plants_clean_data_file_name = \"20##.csv\"\n",
    "        e.g.\n",
    "    power_plants_clean_data_file_name = \"2020.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0481c62b-e1b7-45ee-9476-547c84429fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the power plants data file name\n",
    "#power_plants_raw_data_folder_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/0\"\n",
    "power_plants_clean_data_file_name = \"2020.csv\"  # Replace with the desired file name\n",
    "# Call the external Python script with defined variables\n",
    "#%run -i Power_Plants_Data_Formating.py {power_plants_raw_data_folder_path} {power_plants_data_file_name} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4acc6-3c5e-4128-9344-7c8c713b6969",
   "metadata": {},
   "source": [
    "3.2. Creating .csv clean data file with its respective headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef94f32-320b-4197-8357-38af94fdc530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file '2020.csv' created with headers: , Unit, PowerCapacity, Nunits, Zone, Zone_th, Zone_h2, Technology, Fuel, Efficiency, MinUpTime, MinDownTime, RampUpRate, RampDownRate, StartUpCost, NoLoadCost_pu, RampingCost, PartLoadMin, MinEfficiency, StartUpTime, CO2Intensity, CHPType, CHPPowerToHeat, CHPPowerLossFactor, CHPMaxHeat, COP, Tnominal, coef_COP_a, coef_COP_b, STOCapacity, STOSelfDischarge, STOMaxChargingPower, STOChargingEfficiency, WaterWithdrawal, WaterConsumption, Status, Source, Company\n",
      "CSV file '2020.csv' created with headers: , Unit, PowerCapacity, Nunits, Zone, Zone_th, Zone_h2, Technology, Fuel, Efficiency, MinUpTime, MinDownTime, RampUpRate, RampDownRate, StartUpCost, NoLoadCost_pu, RampingCost, PartLoadMin, MinEfficiency, StartUpTime, CO2Intensity, CHPType, CHPPowerToHeat, CHPPowerLossFactor, CHPMaxHeat, COP, Tnominal, coef_COP_a, coef_COP_b, STOCapacity, STOSelfDischarge, STOMaxChargingPower, STOChargingEfficiency, WaterWithdrawal, WaterConsumption, Status, Source, Company\n",
      "power_plants_clean_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_csv_file(folder_path, file_name, data=None):\n",
    "  \"\"\"\n",
    "  Creates a CSV file with predefined headers using pandas.\n",
    "\n",
    "  Args:\n",
    "      folder_path (str): The path of the folder where the CSV file will be created.\n",
    "      file_name (str): The name of the CSV file.\n",
    "      data (list of lists or pandas.DataFrame, optional): Data to write to the CSV (default: None).\n",
    "\n",
    "  Returns:\n",
    "      None\n",
    "  \"\"\"\n",
    "\n",
    "  headers = [\n",
    "      \"\", \"Unit\", \"PowerCapacity\", \"Nunits\", \"Zone\", \"Zone_th\", \"Zone_h2\", \"Technology\", \"Fuel\", \"Efficiency\",\n",
    "      \"MinUpTime\", \"MinDownTime\", \"RampUpRate\", \"RampDownRate\", \"StartUpCost\", \"NoLoadCost_pu\", \"RampingCost\",\n",
    "      \"PartLoadMin\", \"MinEfficiency\", \"StartUpTime\", \"CO2Intensity\", \"CHPType\", \"CHPPowerToHeat\",\n",
    "      \"CHPPowerLossFactor\", \"CHPMaxHeat\", \"COP\", \"Tnominal\", \"coef_COP_a\", \"coef_COP_b\", \"STOCapacity\",\n",
    "      \"STOSelfDischarge\", \"STOMaxChargingPower\", \"STOChargingEfficiency\", \"WaterWithdrawal\", \"WaterConsumption\", \"Status\", \"Source\",\n",
    "      \"Company\"\n",
    "  ]\n",
    "\n",
    "  # Create empty DataFrame if no data provided\n",
    "  if data is None:\n",
    "      data = []\n",
    "  elif not isinstance(data, (list, pd.DataFrame)):\n",
    "      raise TypeError(\"Data must be a list of lists or a pandas.DataFrame\")\n",
    "\n",
    "  df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "  # Handle potential directory error and combine path with file name\n",
    "  try:\n",
    "      file_path = os.path.join(folder_path, file_name)\n",
    "  except IsADirectoryError:\n",
    "      print(f\"Error: '{folder_path}' is a directory. Please provide a file name within the directory.\")\n",
    "      return None\n",
    "\n",
    "  # Save DataFrame to CSV file\n",
    "  df.to_csv(file_path, index=False)  # Don't include row index in CSV\n",
    "\n",
    "  print(f\"CSV file '{file_name}' created with headers: {', '.join(headers)}\")\n",
    "  return file_path\n",
    "\n",
    "#power_plants_raw_data_folder_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/0\"\n",
    "#power_plants_clean_data_file_name = \"2020.csv\"\n",
    "\n",
    "# Input variables from command line arguments\n",
    "#power_plants_raw_data_folder_path = sys.argv[1]\n",
    "#power_plants_data_file_name = sys.argv[2]\n",
    "\n",
    "\n",
    "create_csv_file(power_plants_raw_data_folder_path, power_plants_clean_data_file_name, data=None)\n",
    "file_path = create_csv_file(power_plants_raw_data_folder_path, power_plants_clean_data_file_name, data=None)\n",
    "\n",
    "power_plants_clean_data_folder_path = power_plants_raw_data_folder_path  # Copy the path to a new variable\n",
    "power_plants_clean_data_file_path = file_path\n",
    "\n",
    "print(\"power_plants_clean_data_folder_path:\", power_plants_clean_data_folder_path)\n",
    "print(\"power_plants_clean_data_file_path:\", power_plants_clean_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de7765-7af1-4d42-927e-cae4ca7da58a",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44feed5d-2a8b-444c-9134-362e32dc1664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c7b56-7d79-42f1-bb1f-9b89bb85c6af",
   "metadata": {},
   "source": [
    "3.3. Copying the corresponding data to the power_plants_clean_data_file \n",
    "\n",
    "    This part compares headers of both files power_plants_raw_data_file and power_plants_clean_data_file to select the common ones according to the equivalencies of the power_plants_clean_data_equivalent_headers_file to finally copy all the data of the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6784de2-fb88-43d4-bcab-6164e565b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_plants_clean_data_equivalent_headers_file_name = 'power_plants_clean_data_equivalent_headers.csv'\n",
    "power_plants_clean_data_equivalent_headers_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a368016-f5b7-4bb6-ab77-65b0f499110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "power_plants_raw_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\"\n",
    "power_plants_clean_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\"\n",
    "power_plants_clean_data_equivalent_headers_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\"\n",
    "\n",
    "# Read the headers from the raw data file\n",
    "raw_data_headers = pd.read_csv(power_plants_raw_data_file_path, nrows=0).columns.tolist()\n",
    "\n",
    "# Read the equivalent headers file\n",
    "equivalent_headers_df = pd.read_csv(power_plants_clean_data_equivalent_headers_file_path)\n",
    "\n",
    "# Initialize an empty dictionary to store mapping of equivalent headers to dispaset headers\n",
    "equivalent_to_dispaset_mapping = {}\n",
    "\n",
    "# Iterate over the rows in the equivalent headers DataFrame\n",
    "for index, row in equivalent_headers_df.iterrows():\n",
    "    # Get the equivalent headers from all columns\n",
    "    equivalent_headers = []\n",
    "    for i in range(1, 9):  # Assuming there are 8 columns for equivalent headers\n",
    "        col_name = f'Equivalent_Headers_{i}'\n",
    "        if col_name in row and isinstance(row[col_name], str):\n",
    "            equivalent_headers.extend(row[col_name].split(','))\n",
    "    \n",
    "    # Get the dispaset headers\n",
    "    dispaset_headers = row['Dispaset_Headers'].split(',')\n",
    "    \n",
    "    # Map each equivalent header to its corresponding dispaset header\n",
    "    for eq_header, disp_header in zip(equivalent_headers, dispaset_headers):\n",
    "        equivalent_to_dispaset_mapping[eq_header] = disp_header\n",
    "\n",
    "# Read the clean data file to get all existing headers\n",
    "existing_headers_df = pd.read_csv(power_plants_clean_data_file_path, nrows=0)\n",
    "existing_headers = existing_headers_df.columns.tolist()\n",
    "\n",
    "# Read the raw data file\n",
    "raw_data_df = pd.read_csv(power_plants_raw_data_file_path)\n",
    "\n",
    "# Initialize a new DataFrame to store the copied columns along with existing headers\n",
    "clean_data_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the existing headers in the clean data file\n",
    "for existing_header in existing_headers:\n",
    "    # Check if the existing header exists in the equivalent headers mapping\n",
    "    if existing_header in equivalent_to_dispaset_mapping.values():\n",
    "        # Find the corresponding equivalent headers\n",
    "        equivalent_headers = [key for key, value in equivalent_to_dispaset_mapping.items() if value == existing_header]\n",
    "        for equivalent_header in equivalent_headers:\n",
    "            if equivalent_header in raw_data_df.columns:\n",
    "                # Copy the corresponding column from the raw data file to the clean data DataFrame\n",
    "                clean_data_df[existing_header] = raw_data_df[equivalent_header]\n",
    "                break  # Copy only the first matching column\n",
    "    else:\n",
    "        # If the existing header doesn't have a corresponding equivalent, copy it as is\n",
    "        clean_data_df[existing_header] = None\n",
    "\n",
    "# Write the clean data DataFrame to the clean data file\n",
    "clean_data_df.to_csv(power_plants_clean_data_file_path, index=False)\n",
    "\n",
    "print(\"Columns copied successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6702759-b1fa-48b3-90f7-98a31b8265d7",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9f011d3-bc96-4677-9159-3d4352d2e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab755144-287e-4e3d-b866-c1a76791a926",
   "metadata": {},
   "source": [
    "3.4. Zone Field Filter\n",
    "\n",
    "    This part erase all data that does not belongs to the zone/country/region/state/etc. to be modeled.\n",
    "    This stept can be jumped if it would rather to use an a single file where all units of all the zones are placed in however it is recommended to keep the dispa-SET directories structure i.e. storage all the power plants data related to a zone in a separated folder.\n",
    "    Additioanlly, if the name of the zone/country/region to be filtered is not the same of the zone_folder_name variable uncommen the next line and comment and enter the correponding name of the zone wanted to be filtered next comment the subsecuent line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7895544d-e5d6-452f-9a1b-ba612fcf6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zone_name= \"Set the name of zone to be filtered\"\n",
    "zone_name = zone_folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d27d4a56-d970-4b30-891e-3e79e2928c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows filtered successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Variables\n",
    "#power_plants_clean_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\"\n",
    "#zone_name = zone_folder_name\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "\n",
    "# Filter rows where the \"Zone\" column matches zone_folder_name\n",
    "filtered_df = df[df[\"Zone\"] == zone_name]\n",
    "\n",
    "# Overwrite the CSV file with the filtered DataFrame\n",
    "filtered_df.to_csv(power_plants_clean_data_file_path, index=False)\n",
    "\n",
    "print(\"Rows filtered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e50ccd7-5d22-4c4b-bc9b-c8ed6bf3241a",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a18e8080-bde2-4475-b07c-5cf1c658b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769016b-54f1-4c63-b10d-64688e15e73e",
   "metadata": {},
   "source": [
    "3.5.  Nunits Field Filter\n",
    "\n",
    "    This part assign a number of 1 to the column called Units of the respective cell that does not have any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "798c08c7-2c5f-4965-882b-b7d6a9d1b742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty cells in the 'Nunits' column have been filled with 1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Variables\n",
    "#power_plants_clean_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\"\n",
    "Header_3 = \"Nunits\"\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "\n",
    "# Fill empty cells in the \"Nunits\" column with 1\n",
    "df[Header_3] = df[Header_3].fillna(1)\n",
    "\n",
    "# Write the updated DataFrame back to the CSV file\n",
    "df.to_csv(power_plants_clean_data_file_path, index=False)\n",
    "\n",
    "print(\"Empty cells in the 'Nunits' column have been filled with 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157f6f7-7989-4fd9-aa7b-60b3f724ec7d",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d60f4228-f9de-4ecf-b860-0fa4de7d7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1975ca3c-e928-4213-bdc6-ee69a5ec39e0",
   "metadata": {},
   "source": [
    "3.6.  Unit Field Filter\n",
    "\n",
    "    This part joint all the units with the same name, Technology and Fuel adding their respective capacity and number of units and CHPMaxHeat values.\n",
    "    Additionally takes the average of the Efficiency field as resulting value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "487b78f1-91de-412e-a1c4-f7528aa1da45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicated cells in the 'Unit' column processed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82108/363426553.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unit_rows[Header_9] = pd.to_numeric(unit_rows[Header_9], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Variables\n",
    "Header_1 = \"Unit\"\n",
    "Header_2 = \"PowerCapacity\"\n",
    "Header_3 = \"Nunits\"\n",
    "Header_4 = \"Technology\"\n",
    "Header_5 = \"Fuel\"\n",
    "Header_6 = \"Efficiency\"\n",
    "Header_7 = \"PowerCapacity\"\n",
    "Header_8 = \"CHPType\"\n",
    "Header_9 = \"CHPMaxHeat\"\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "\n",
    "# Find duplicated cells in the Header_1 column\n",
    "duplicated_units = df[df.duplicated(subset=[Header_1], keep=False)]\n",
    "\n",
    "# Process duplicated units\n",
    "for unit in duplicated_units[Header_1].unique():\n",
    "    # Filter rows with the current duplicated unit\n",
    "    unit_rows = df[df[Header_1] == unit]\n",
    "    \n",
    "    # Check if there are matching values in Header_4 and Header_5\n",
    "    if unit_rows[Header_4].nunique() == 1 and unit_rows[Header_5].nunique() == 1:\n",
    "        # Calculate the average of Header_6\n",
    "        average_efficiency = unit_rows[Header_6].mean()\n",
    "        \n",
    "        # Convert Header_9 to numeric and then sum its values\n",
    "        unit_rows[Header_9] = pd.to_numeric(unit_rows[Header_9], errors='coerce')\n",
    "        total_chp_max_heat = unit_rows[Header_9].sum()\n",
    "        \n",
    "        # Sum the values of Header_3, Header_7, and Header_9\n",
    "        total_nunits = unit_rows[Header_3].sum()\n",
    "        total_power_capacity = unit_rows[Header_7].sum()\n",
    "        \n",
    "        # Put the sum and average results in the first duplicated row\n",
    "        first_row_index = unit_rows.index[0]\n",
    "        df.at[first_row_index, Header_3] = total_nunits\n",
    "        df.at[first_row_index, Header_6] = average_efficiency\n",
    "        df.at[first_row_index, Header_7] = total_power_capacity\n",
    "        df.at[first_row_index, Header_9] = total_chp_max_heat\n",
    "        \n",
    "        # Update Header_8 with next non-empty value if necessary\n",
    "        if pd.isnull(df.at[first_row_index, Header_8]):\n",
    "            for index, row in unit_rows.iterrows():\n",
    "                if not pd.isnull(row[Header_8]):\n",
    "                    df.at[first_row_index, Header_8] = row[Header_8]\n",
    "                    break\n",
    "        \n",
    "        # Drop other duplicated rows\n",
    "        df.drop(unit_rows.index[1:], inplace=True)\n",
    "\n",
    "# Write the updated DataFrame back to the CSV file\n",
    "df.to_csv(power_plants_clean_data_file_path, index=False)\n",
    "\n",
    "print(\"Duplicated cells in the 'Unit' column processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397975a-fffa-4c95-844f-de17672c3294",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8600f39d-c2a1-46ee-b024-1837ad5b604c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416765e-f202-44ae-9780-f286d29bd8a9",
   "metadata": {},
   "source": [
    "3.8 Clustering Power Units Data\n",
    "\n",
    "    This part join all the units of a same \"Company\" creating a new separated csv file as a clustered verions of the data. The same conditions of the last code was used to this stept too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4d8cc4c-1c2e-40a4-a77b-5acd77e8e716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-Clustered.csv\n",
      "Processed data saved under the name: 2020-Clustered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82108/1332994827.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unit_rows[Header_9] = pd.to_numeric(unit_rows[Header_9], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Variables\n",
    "Header_1 = \"Company\"  # Change to the desired column name\n",
    "Header_2 = \"PowerCapacity\"\n",
    "Header_3 = \"Nunits\"\n",
    "Header_4 = \"Technology\"\n",
    "Header_5 = \"Fuel\"\n",
    "Header_6 = \"Efficiency\"\n",
    "Header_7 = \"PowerCapacity\"\n",
    "Header_8 = \"CHPType\"\n",
    "Header_9 = \"CHPMaxHeat\"\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "\n",
    "# Find duplicated cells in the Header_1 column\n",
    "duplicated_units = df[df.duplicated(subset=[Header_1], keep=False)]\n",
    "\n",
    "# Process duplicated units\n",
    "for unit in duplicated_units[Header_1].unique():\n",
    "    # Filter rows with the current duplicated unit\n",
    "    unit_rows = df[df[Header_1] == unit]\n",
    "    \n",
    "    # Check if there are matching values in Header_4 and Header_5\n",
    "    if unit_rows[Header_4].nunique() == 1 and unit_rows[Header_5].nunique() == 1:\n",
    "        # Calculate the average of Header_6\n",
    "        average_efficiency = unit_rows[Header_6].mean()\n",
    "        \n",
    "        # Convert Header_9 to numeric and then sum its values\n",
    "        unit_rows[Header_9] = pd.to_numeric(unit_rows[Header_9], errors='coerce')\n",
    "        total_chp_max_heat = unit_rows[Header_9].sum()\n",
    "        \n",
    "        # Sum the values of Header_3, Header_7, and Header_9\n",
    "        total_nunits = unit_rows[Header_3].sum()\n",
    "        total_power_capacity = unit_rows[Header_7].sum()\n",
    "        \n",
    "        # Put the sum and average results in the first duplicated row\n",
    "        first_row_index = unit_rows.index[0]\n",
    "        df.at[first_row_index, Header_3] = total_nunits\n",
    "        df.at[first_row_index, Header_6] = average_efficiency\n",
    "        df.at[first_row_index, Header_7] = total_power_capacity\n",
    "        df.at[first_row_index, Header_9] = total_chp_max_heat\n",
    "        \n",
    "        # Drop other duplicated rows\n",
    "        df.drop(unit_rows.index[1:], inplace=True)\n",
    "\n",
    "# Add a new column \"Clustered\" and set its value to \"yes\" for rows where Nunits is more than 1\n",
    "df['Clustered'] = 'no'\n",
    "df.loc[df[Header_3] > 1, 'Clustered'] = 'yes'\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "new_file_name = os.path.basename(power_plants_clean_data_file_path).replace(\".csv\", \"-Clustered.csv\")\n",
    "new_csv_file_path = os.path.join(power_plants_clean_data_folder_path, new_file_name)\n",
    "df.to_csv(new_csv_file_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Processed data saved to: {new_csv_file_path}\")\n",
    "print(f\"Processed data saved under the name: {new_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0008dd23-5964-46c2-893d-7e2713415ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_plants_clean_clustered_data_file_name = new_file_name\n",
    "power_plants_clean_clustered_data_file_path = new_csv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f89d47c5-7388-4d33-bfb1-48a5f3b59487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_clustered_data_file_name: 2020-Clustered.csv\n",
      "power_plants_clean_clustered_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-Clustered.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\\npower_plants_clean_clustered_data_file_name: {power_plants_clean_clustered_data_file_name}\\npower_plants_clean_clustered_data_file_path: {power_plants_clean_clustered_data_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd56336-60f3-4b23-9f4e-0c3422d3c574",
   "metadata": {},
   "source": [
    "3.7 Technology Field Filter\n",
    "\n",
    "    This part read the current value of the column called \"technology\" of power_plants_clean_data_file and the power_plants_clean_clustered_data_file and makes a compariton with the contend of the power_plants_all_data_equivalent_Technologies_file and replace all the values of the column by their respective equivalent format needed to dispa-SET.\n",
    "    Additionally it copies to the power_plants_all_data_not_defined_units_file all the rows of the units with not defined technology or other kind of information in that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "588da451-f235-4ad3-866e-4c5565f15f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_plants_all_data_equivalent_technologies_file_name = \"power_plants_all_data_equivalent_technologies.csv\"\n",
    "power_plants_all_data_equivalent_technologies_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_technologies.csv\"\n",
    "power_plants_all_data_not_defined_units_file_name = \"power_plants_all_data_not_defined_units.csv\"\n",
    "power_plants_all_data_not_defined_units_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d22aab1-6c20-4657-89de-7aa412fbf09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_plants_all_data_equivalent_technologies_file_name: power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_equivalent_technologies_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_not_defined_units_file_name: power_plants_all_data_not_defined_units.csv\n",
      "power_plants_all_data_not_defined_units_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"power_plants_all_data_equivalent_technologies_file_name: {power_plants_all_data_equivalent_technologies_file_name}\")\n",
    "print(f\"power_plants_all_data_equivalent_technologies_file_path: {power_plants_all_data_equivalent_technologies_file_path}\")\n",
    "print(f\"power_plants_all_data_not_defined_units_file_name: {power_plants_all_data_not_defined_units_file_name}\")\n",
    "print(f\"power_plants_all_data_not_defined_units_file_path: {power_plants_all_data_not_defined_units_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "149c7240-8273-4030-a955-c2d5178126a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read data from CSV files\n",
    "clean_data_df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "equivalent_technologies_df = pd.read_csv(power_plants_all_data_equivalent_technologies_file_path)\n",
    "\n",
    "# Initialize list to store not defined units\n",
    "not_defined_units = []\n",
    "\n",
    "# Iterate over each row in the clean data file\n",
    "for index, row in clean_data_df.iterrows():\n",
    "    technology = row[\"Technology\"]\n",
    "    found = False\n",
    "    # Look for the technology in equivalent technologies\n",
    "    for i in range(1, 9):\n",
    "        equivalent_tech_col = f\"Equivalent_Technology_{i}\"\n",
    "        dispaset_tech_col = \"Dispaset_Technology\"\n",
    "        if technology in equivalent_technologies_df[equivalent_tech_col].values:\n",
    "            found = True\n",
    "            # Get the corresponding Dispaset technology\n",
    "            dispaset_technology = equivalent_technologies_df.loc[equivalent_technologies_df[equivalent_tech_col] == technology, dispaset_tech_col].iloc[0]\n",
    "            # Update the Technology column in clean data\n",
    "            clean_data_df.at[index, \"Technology\"] = dispaset_technology\n",
    "            break\n",
    "    # If not found, add the row to the list of not defined units and drop from clean_data_df\n",
    "    if not found:\n",
    "        not_defined_units.append(index)\n",
    "\n",
    "# Create DataFrame for not defined units\n",
    "not_defined_df = clean_data_df.iloc[not_defined_units]\n",
    "\n",
    "# Drop rows with unmatched fields from clean data DataFrame\n",
    "clean_data_df.drop(not_defined_units, inplace=True)\n",
    "\n",
    "# Write not defined units to a separate CSV file (append mode)\n",
    "not_defined_df.to_csv(power_plants_all_data_not_defined_units_file_path, mode='a', index=False, header=not os.path.exists(power_plants_all_data_not_defined_units_file_path))\n",
    "\n",
    "# Overwrite the clean data CSV file with updated data\n",
    "clean_data_df.to_csv(power_plants_clean_data_file_path, index=False)\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3bea292-b7e6-4fee-81d1-fde7df42cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read data from CSV files\n",
    "clean_data_df = pd.read_csv(power_plants_clean_clustered_data_file_path)\n",
    "equivalent_technologies_df = pd.read_csv(power_plants_all_data_equivalent_technologies_file_path)\n",
    "\n",
    "# Initialize list to store not defined units\n",
    "not_defined_units = []\n",
    "\n",
    "# Iterate over each row in the clean data file\n",
    "for index, row in clean_data_df.iterrows():\n",
    "    technology = row[\"Technology\"]\n",
    "    found = False\n",
    "    # Look for the technology in equivalent technologies\n",
    "    for i in range(1, 9):\n",
    "        equivalent_tech_col = f\"Equivalent_Technology_{i}\"\n",
    "        dispaset_tech_col = \"Dispaset_Technology\"\n",
    "        if technology in equivalent_technologies_df[equivalent_tech_col].values:\n",
    "            found = True\n",
    "            # Get the corresponding Dispaset technology\n",
    "            dispaset_technology = equivalent_technologies_df.loc[equivalent_technologies_df[equivalent_tech_col] == technology, dispaset_tech_col].iloc[0]\n",
    "            # Update the Technology column in clean data\n",
    "            clean_data_df.at[index, \"Technology\"] = dispaset_technology\n",
    "            break\n",
    "    # If not found, add the row to the list of not defined units and drop from clean_data_df\n",
    "    if not found:\n",
    "        not_defined_units.append(index)\n",
    "\n",
    "# Create DataFrame for not defined units\n",
    "not_defined_df = clean_data_df.iloc[not_defined_units]\n",
    "\n",
    "# Drop rows with unmatched fields from clean data DataFrame\n",
    "clean_data_df.drop(not_defined_units, inplace=True)\n",
    "\n",
    "# Write not defined units to a separate CSV file (append mode)\n",
    "not_defined_df.to_csv(power_plants_all_data_not_defined_units_file_path, mode='a', index=False, header=not os.path.exists(power_plants_all_data_not_defined_units_file_path))\n",
    "\n",
    "# Overwrite the clean data CSV file with updated data\n",
    "clean_data_df.to_csv(power_plants_clean_clustered_data_file_path, index=False)\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b16d2-1368-4161-9c24-6dda682a88f2",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36bc925a-6987-4ae2-9446-806bafc76d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_clustered_data_file_name: 2020-Clustered.csv\n",
      "power_plants_clean_clustered_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-Clustered.csv\n",
      "power_plants_all_data_equivalent_technologies_file_name: power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_equivalent_technologies_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_not_defined_units_file_name: power_plants_all_data_not_defined_units.csv\\mpower_plants_all_data_not_defined_units_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\\npower_plants_clean_clustered_data_file_name: {power_plants_clean_clustered_data_file_name}\\npower_plants_clean_clustered_data_file_path: {power_plants_clean_clustered_data_file_path}\\npower_plants_all_data_equivalent_technologies_file_name: {power_plants_all_data_equivalent_technologies_file_name}\\npower_plants_all_data_equivalent_technologies_file_path: {power_plants_all_data_equivalent_technologies_file_path}\\npower_plants_all_data_not_defined_units_file_name: {power_plants_all_data_not_defined_units_file_name}\\mpower_plants_all_data_not_defined_units_file_path: {power_plants_all_data_not_defined_units_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8995e-4c79-48d6-9ed6-8baa682e0486",
   "metadata": {},
   "source": [
    "3.8. Fuel Field Filter\n",
    "\n",
    "    This part read the current value of the column called \"fuel\" of power_plants_clean_data_file and power_plants_clean_clustered_data_file and makes a compariton with the contend of the power_plants_all_data_equivalent_fuels_file and replace all the values of the column by their respective equivalent format needed to dispa-SET.\n",
    "    Additionally it copies to the power_plants_all_data_not_defined_units_file all the rows of the units with not defined fuel or other kind of information in that field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b561dcb-eb9b-4774-a49a-90603c365cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_plants_all_data_equivalent_fuels_file_name = \"power_plants_all_data_equivalent_fuels.csv\"\n",
    "power_plants_all_data_equivalent_fuels_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_fuels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c343d1f2-a3d3-44b6-92df-9aa67fae0769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read data from CSV files\n",
    "clean_data_df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "equivalent_fuels_df = pd.read_csv(power_plants_all_data_equivalent_fuels_file_path)\n",
    "\n",
    "# Initialize list to store not defined units\n",
    "not_defined_units = []\n",
    "\n",
    "# Iterate over each row in the clean data file\n",
    "for index, row in clean_data_df.iterrows():\n",
    "    fuel = row[\"Fuel\"]\n",
    "    found = False\n",
    "    # Look for the fuel in equivalent fuels\n",
    "    for i in range(1, 9):\n",
    "        equivalent_fuel_col = f\"Equivalent_Fuel_{i}\"\n",
    "        dispaset_fuel_col = \"Dispaset_Fuel\"\n",
    "        if fuel in equivalent_fuels_df[equivalent_fuel_col].values:\n",
    "            found = True\n",
    "            # Get the corresponding Dispaset fuel\n",
    "            dispaset_fuel = equivalent_fuels_df.loc[equivalent_fuels_df[equivalent_fuel_col] == fuel, dispaset_fuel_col].iloc[0]\n",
    "            # Update the Fuel column in clean data\n",
    "            clean_data_df.at[index, \"Fuel\"] = dispaset_fuel\n",
    "            break\n",
    "    # If not found, add the row to the list of not defined units and drop from clean_data_df\n",
    "    if not found:\n",
    "        not_defined_units.append(index)\n",
    "\n",
    "# Create DataFrame for not defined units\n",
    "not_defined_df = clean_data_df.iloc[not_defined_units]\n",
    "\n",
    "# Drop rows with unmatched fields from clean data DataFrame\n",
    "clean_data_df.drop(not_defined_units, inplace=True)\n",
    "\n",
    "# Write not defined units to a separate CSV file (append mode)\n",
    "not_defined_df.to_csv(power_plants_all_data_not_defined_units_file_path, mode='a', index=False, header=not os.path.exists(power_plants_all_data_not_defined_units_file_path))\n",
    "\n",
    "# Overwrite the clean data CSV file with updated data\n",
    "clean_data_df.to_csv(power_plants_clean_data_file_path, index=False)\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a88e3c83-63ac-45f3-9711-076675ddf54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read data from CSV files\n",
    "clean_data_df = pd.read_csv(power_plants_clean_clustered_data_file_path)\n",
    "equivalent_fuels_df = pd.read_csv(power_plants_all_data_equivalent_fuels_file_path)\n",
    "\n",
    "# Initialize list to store not defined units\n",
    "not_defined_units = []\n",
    "\n",
    "# Iterate over each row in the clean data file\n",
    "for index, row in clean_data_df.iterrows():\n",
    "    fuel = row[\"Fuel\"]\n",
    "    found = False\n",
    "    # Look for the fuel in equivalent fuels\n",
    "    for i in range(1, 9):\n",
    "        equivalent_fuel_col = f\"Equivalent_Fuel_{i}\"\n",
    "        dispaset_fuel_col = \"Dispaset_Fuel\"\n",
    "        if fuel in equivalent_fuels_df[equivalent_fuel_col].values:\n",
    "            found = True\n",
    "            # Get the corresponding Dispaset fuel\n",
    "            dispaset_fuel = equivalent_fuels_df.loc[equivalent_fuels_df[equivalent_fuel_col] == fuel, dispaset_fuel_col].iloc[0]\n",
    "            # Update the Fuel column in clean data\n",
    "            clean_data_df.at[index, \"Fuel\"] = dispaset_fuel\n",
    "            break\n",
    "    # If not found, add the row to the list of not defined units and drop from clean_data_df\n",
    "    if not found:\n",
    "        not_defined_units.append(index)\n",
    "\n",
    "# Create DataFrame for not defined units\n",
    "not_defined_df = clean_data_df.iloc[not_defined_units]\n",
    "\n",
    "# Drop rows with unmatched fields from clean data DataFrame\n",
    "clean_data_df.drop(not_defined_units, inplace=True)\n",
    "\n",
    "# Write not defined units to a separate CSV file (append mode)\n",
    "not_defined_df.to_csv(power_plants_all_data_not_defined_units_file_path, mode='a', index=False, header=not os.path.exists(power_plants_all_data_not_defined_units_file_path))\n",
    "\n",
    "# Overwrite the clean data CSV file with updated data\n",
    "clean_data_df.to_csv(power_plants_clean_clustered_data_file_path, index=False)\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec7a04-fb8e-4c18-9f7d-1c9b76a51b20",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c003621-75d4-43ba-8fe0-db18dbb7404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_clustered_data_file_name: 2020-Clustered.csv\n",
      "power_plants_clean_clustered_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-Clustered.csv\n",
      "power_plants_all_data_equivalent_technologies_file_name: power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_equivalent_technologies_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_not_defined_units_file_name: power_plants_all_data_not_defined_units.csv\n",
      "power_plants_all_data_not_defined_units_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv\n",
      "power_plants_all_data_equivalent_fuels_file_name power_plants_all_data_equivalent_fuels.csv\n",
      "power_plants_all_data_equivalent_fuels_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_fuels.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\\npower_plants_clean_clustered_data_file_name: {power_plants_clean_clustered_data_file_name}\\npower_plants_clean_clustered_data_file_path: {power_plants_clean_clustered_data_file_path}\\npower_plants_all_data_equivalent_technologies_file_name: {power_plants_all_data_equivalent_technologies_file_name}\\npower_plants_all_data_equivalent_technologies_file_path: {power_plants_all_data_equivalent_technologies_file_path}\\npower_plants_all_data_not_defined_units_file_name: {power_plants_all_data_not_defined_units_file_name}\\npower_plants_all_data_not_defined_units_file_path: {power_plants_all_data_not_defined_units_file_path}\\npower_plants_all_data_equivalent_fuels_file_name {power_plants_all_data_equivalent_fuels_file_name}\\npower_plants_all_data_equivalent_fuels_file_path {power_plants_all_data_equivalent_fuels_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d707a-4d39-411c-88ca-069ce0ba5302",
   "metadata": {},
   "source": [
    "3.9. CHPType, CHPPowerToHeat, CHPPowerLossFactor and CHPMaxHeat Fields Filter\n",
    "\n",
    "    This part read for each unit the current value of the column called \"CHPMaxHeat\" of power_plants_clean_data_file and power_plants_clean_clustered_data_file, if the unit has any value on this field,  checks out its respective value in the column CHPType, and compares wiht the table power_plants_all_data_equivalent_CHPTypes_file, if there is no coincydense, copies the row to the power_plants_all_data_not_defined_units_file leaving this field empty, but keeping the information of the other related fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56136348-4302-4e79-99ba-a8726d5c7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_plants_all_data_equivalent_CHPTypes_file_name = \"power_plants_all_data_equivalent_CHPTypes.csv\"\n",
    "power_plants_all_data_equivalent_CHPTypes_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_CHPTypes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af6228a3-21af-4bf2-8ef1-b05dbbab0e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV files\n",
    "clean_data_df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "equivalent_chp_types_df = pd.read_csv(power_plants_all_data_equivalent_CHPTypes_file_path)\n",
    "\n",
    "# Initialize list to store not defined units\n",
    "not_defined_units = []\n",
    "\n",
    "# Iterate over each row in the clean data file\n",
    "for index, row in clean_data_df.iterrows():\n",
    "    chtype = row[\"CHPType\"]\n",
    "    found = False\n",
    "    # Look for the CHPType in equivalent CHP Types\n",
    "    for i in range(1, 9):\n",
    "        equivalent_chp_type_col = f\"Equivalent_CHPType_{i}\"\n",
    "        dispaset_chp_type_col = \"Dispaset_CHPType\"\n",
    "        if chtype in equivalent_chp_types_df[equivalent_chp_type_col].values:\n",
    "            found = True\n",
    "            # Get the corresponding Dispaset CHP Type\n",
    "            dispaset_chp_type = equivalent_chp_types_df.loc[equivalent_chp_types_df[equivalent_chp_type_col] == chtype, dispaset_chp_type_col].iloc[0]\n",
    "            # Update the CHPType column in clean data\n",
    "            clean_data_df.at[index, \"CHPType\"] = dispaset_chp_type\n",
    "            break\n",
    "    if not found:\n",
    "        clean_data_df.at[index, \"CHPType\"] = \"\"\n",
    "        not_defined_units.append(row)\n",
    "\n",
    "# Create DataFrame for not defined units\n",
    "not_defined_df = pd.DataFrame(not_defined_units)\n",
    "\n",
    "# Write not defined units to a separate CSV file\n",
    "not_defined_df.to_csv(power_plants_all_data_not_defined_units_file_path, mode='a', index=False, header=not os.path.exists(power_plants_all_data_not_defined_units_file_path))\n",
    "\n",
    "# Overwrite the clean data CSV file with updated data\n",
    "clean_data_df.to_csv(power_plants_clean_data_file_path, index=False)\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d7437cd-dd92-43c8-85d4-fcf11393aebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar values to 'n.b.' and empty fields removed from specified columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read CSV files\n",
    "df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "equiv_chp_types = pd.read_csv(power_plants_all_data_equivalent_CHPTypes_file_path)\n",
    "\n",
    "# Columns to clean\n",
    "columns_to_clean = ['CHPType', 'CHPMaxHeat', 'CHPPowerLossFactor', 'CHPPowerToHeat']\n",
    "\n",
    "# Define regex pattern to match 'n.b.' and similar values\n",
    "pattern = r'^n\\.b\\.|^\\s*$'\n",
    "\n",
    "# Replace matching values with empty strings\n",
    "df[columns_to_clean] = df[columns_to_clean].replace(to_replace=pattern, value='', regex=True)\n",
    "\n",
    "# Convert columns to numeric, ignoring errors\n",
    "df[columns_to_clean[1:]] = df[columns_to_clean[1:]].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Write the updated DataFrame back to the CSV file\n",
    "df.to_csv(power_plants_clean_data_file_path, index=False)\n",
    "\n",
    "print(\"Similar values to 'n.b.' and empty fields removed from specified columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c84024c6-3596-48f2-9801-7c7652852a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV files\n",
    "clean_data_df = pd.read_csv(power_plants_clean_clustered_data_file_path)\n",
    "equivalent_chp_types_df = pd.read_csv(power_plants_all_data_equivalent_CHPTypes_file_path)\n",
    "\n",
    "# Initialize list to store not defined units\n",
    "not_defined_units = []\n",
    "\n",
    "# Iterate over each row in the clean data file\n",
    "for index, row in clean_data_df.iterrows():\n",
    "    chtype = row[\"CHPType\"]\n",
    "    found = False\n",
    "    # Look for the CHPType in equivalent CHP Types\n",
    "    for i in range(1, 9):\n",
    "        equivalent_chp_type_col = f\"Equivalent_CHPType_{i}\"\n",
    "        dispaset_chp_type_col = \"Dispaset_CHPType\"\n",
    "        if chtype in equivalent_chp_types_df[equivalent_chp_type_col].values:\n",
    "            found = True\n",
    "            # Get the corresponding Dispaset CHP Type\n",
    "            dispaset_chp_type = equivalent_chp_types_df.loc[equivalent_chp_types_df[equivalent_chp_type_col] == chtype, dispaset_chp_type_col].iloc[0]\n",
    "            # Update the CHPType column in clean data\n",
    "            clean_data_df.at[index, \"CHPType\"] = dispaset_chp_type\n",
    "            break\n",
    "    if not found:\n",
    "        clean_data_df.at[index, \"CHPType\"] = \"\"\n",
    "        not_defined_units.append(row)\n",
    "\n",
    "# Create DataFrame for not defined units\n",
    "not_defined_df = pd.DataFrame(not_defined_units)\n",
    "\n",
    "# Write not defined units to a separate CSV file\n",
    "not_defined_df.to_csv(power_plants_all_data_not_defined_units_file_path, mode='a', index=False, header=not os.path.exists(power_plants_all_data_not_defined_units_file_path))\n",
    "\n",
    "# Overwrite the clean data CSV file with updated data\n",
    "clean_data_df.to_csv(power_plants_clean_clustered_data_file_path, index=False)\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be871049-311d-492a-bde7-1692ac7d786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar values to 'n.b.' and empty fields removed from specified columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read CSV files\n",
    "df = pd.read_csv(power_plants_clean_clustered_data_file_path)\n",
    "equiv_chp_types = pd.read_csv(power_plants_all_data_equivalent_CHPTypes_file_path)\n",
    "\n",
    "# Columns to clean\n",
    "columns_to_clean = ['CHPType', 'CHPMaxHeat', 'CHPPowerLossFactor', 'CHPPowerToHeat']\n",
    "\n",
    "# Define regex pattern to match 'n.b.' and similar values\n",
    "pattern = r'^n\\.b\\.|^\\s*$'\n",
    "\n",
    "# Replace matching values with empty strings\n",
    "df[columns_to_clean] = df[columns_to_clean].replace(to_replace=pattern, value='', regex=True)\n",
    "\n",
    "# Convert columns to numeric, ignoring errors\n",
    "df[columns_to_clean[1:]] = df[columns_to_clean[1:]].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Write the updated DataFrame back to the CSV file\n",
    "df.to_csv(power_plants_clean_clustered_data_file_path, index=False)\n",
    "\n",
    "print(\"Similar values to 'n.b.' and empty fields removed from specified columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b2622-1431-4fa7-b47a-2df2d13b5c79",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea8add0a-6af2-4b16-9774-eaa7b92d7963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_clustered_data_file_name: 2020-Clustered.csv\n",
      "power_plants_clean_clustered_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-Clustered.csv\n",
      "power_plants_all_data_equivalent_technologies_file_name: power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_equivalent_technologies_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_not_defined_units_file_name: power_plants_all_data_not_defined_units.csv\n",
      "power_plants_all_data_not_defined_units_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv\n",
      "power_plants_all_data_equivalent_fuels_file_name power_plants_all_data_equivalent_fuels.csv\n",
      "power_plants_all_data_equivalent_fuels_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_fuels.csv\n",
      "power_plants_all_data_equivalent_CHPTypes_file_name power_plants_all_data_equivalent_CHPTypes.csv\n",
      "power_plants_all_data_equivalent_CHPTypes_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_CHPTypes.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\\npower_plants_clean_clustered_data_file_name: {power_plants_clean_clustered_data_file_name}\\npower_plants_clean_clustered_data_file_path: {power_plants_clean_clustered_data_file_path}\\npower_plants_all_data_equivalent_technologies_file_name: {power_plants_all_data_equivalent_technologies_file_name}\\npower_plants_all_data_equivalent_technologies_file_path: {power_plants_all_data_equivalent_technologies_file_path}\\npower_plants_all_data_not_defined_units_file_name: {power_plants_all_data_not_defined_units_file_name}\\npower_plants_all_data_not_defined_units_file_path: {power_plants_all_data_not_defined_units_file_path}\\npower_plants_all_data_equivalent_fuels_file_name {power_plants_all_data_equivalent_fuels_file_name}\\npower_plants_all_data_equivalent_fuels_file_path {power_plants_all_data_equivalent_fuels_file_path}\\npower_plants_all_data_equivalent_CHPTypes_file_name {power_plants_all_data_equivalent_CHPTypes_file_name}\\npower_plants_all_data_equivalent_CHPTypes_file_path {power_plants_all_data_equivalent_CHPTypes_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0e341-8e6a-4d3a-93ff-9b1d8d287433",
   "metadata": {},
   "source": [
    "3.10. Power Capacity field filter\n",
    "\n",
    "    This part erase all the units with the Power Capacity field empty or zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73e18c6c-91da-4ce5-ad8c-1527fe8e8cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rows with empty 'PowerCapacity' column found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(power_plants_clean_data_file_path)\n",
    "\n",
    "# Filter rows with empty 'PowerCapacity' column\n",
    "empty_power_capacity_rows = df[df['PowerCapacity'].isna()]\n",
    "\n",
    "if empty_power_capacity_rows.empty:\n",
    "    print(\"No rows with empty 'PowerCapacity' column found.\")\n",
    "else:\n",
    "    # Save the filtered rows to a new CSV file\n",
    "    empty_power_capacity_rows.to_csv(power_plants_all_data_not_defined_units_file_path, index=False)\n",
    "    print(f\"Rows with empty 'PowerCapacity' column saved to: {power_plants_all_data_not_defined_units_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d66cdea-f265-4f2c-b4b5-7228df837403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rows with empty 'PowerCapacity' column found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(power_plants_clean_clustered_data_file_path)\n",
    "\n",
    "# Filter rows with empty 'PowerCapacity' column\n",
    "empty_power_capacity_rows = df[df['PowerCapacity'].isna()]\n",
    "\n",
    "if empty_power_capacity_rows.empty:\n",
    "    print(\"No rows with empty 'PowerCapacity' column found.\")\n",
    "else:\n",
    "    # Save the filtered rows to a new CSV file\n",
    "    empty_power_capacity_rows.to_csv(power_plants_all_data_not_defined_units_file_path, index=False)\n",
    "    print(f\"Rows with empty 'PowerCapacity' column saved to: {power_plants_all_data_not_defined_units_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a416bbfb-0584-47dd-92aa-021bba4586c2",
   "metadata": {},
   "source": [
    "3.11. Efficiency Field Fullfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a508ed96-f5cb-4289-91d1-20292f518d96",
   "metadata": {},
   "source": [
    "    This part copy the closer value of the Efficiency technical feauture from the already agregated data base EU_Power_Units_Technical_Features.csv using the notebook: EU_Power_Plant_Technical_Data_Base_Gathering.ipynb sellecting in base on the Technology, the Fuel and the PowerCapacity features. This is done just to the empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e00d10c1-973b-4940-9f40-b3000eadcb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "EU_Power_Units_Technical_Features_File_name = 'EU_Power_Units_Technical_Features.csv'\n",
    "EU_Power_Units_Technical_Features_File_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/EU_Power_Units_Technical_Features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f400808-eac9-443f-8dc0-3d123716ed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying efficiency values...\n",
      "Efficiency values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def copy_efficiency_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying efficiency values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Iterate over rows in the second file\n",
    "            for index, second_row in second_fuel_filtered.iterrows():\n",
    "                # Check if \"Efficiency\" value is empty in the second file\n",
    "                if pd.isnull(second_row['Efficiency']):\n",
    "                    # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                    closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                    # Copy \"Efficiency\" value from the first file to the second file\n",
    "                    if not closest_row.empty:\n",
    "                        second_df.at[index, 'Efficiency'] = closest_row['Efficiency'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"Efficiency values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'Efficiency']\n",
    "\n",
    "copy_efficiency_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2dd5a63c-cd47-4202-a1a6-ac944dc61c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying efficiency values...\n",
      "Efficiency values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def copy_efficiency_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying efficiency values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Iterate over rows in the second file\n",
    "            for index, second_row in second_fuel_filtered.iterrows():\n",
    "                # Check if \"Efficiency\" value is empty in the second file\n",
    "                if pd.isnull(second_row['Efficiency']):\n",
    "                    # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                    closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                    # Copy \"Efficiency\" value from the first file to the second file\n",
    "                    if not closest_row.empty:\n",
    "                        second_df.at[index, 'Efficiency'] = closest_row['Efficiency'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"Efficiency values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'Efficiency']\n",
    "\n",
    "copy_efficiency_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8630a-9e9d-4133-a773-2e89527276f8",
   "metadata": {},
   "source": [
    "        Setting with the value of 1 to all those units which no Efficiecy data was found and adding them to the do not defined units file. This is done for both files power_plants_clean_data_file and power_plants_clean_clustered_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9b5c7c1-6822-4c53-9387-f08f66bb1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty 'Efficiency' cells filled with 1 in the original file.\n",
      "Rows with empty 'Efficiency' values appended to '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def fill_empty_efficiency_with_one(input_file_path, output_file_path):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(input_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{input_file_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Check if there are empty cells in the \"Efficiency\" column\n",
    "    empty_efficiency_rows = df[df['Efficiency'].isnull()]\n",
    "\n",
    "    if not empty_efficiency_rows.empty:\n",
    "        # Replace empty cells in the \"Efficiency\" column with 1\n",
    "        df['Efficiency'].fillna(1, inplace=True)\n",
    "\n",
    "        # Write the updated DataFrame back to the original CSV file\n",
    "        df.to_csv(input_file_path, index=False)\n",
    "\n",
    "        print(\"Empty 'Efficiency' cells filled with 1 in the original file.\")\n",
    "\n",
    "        # Check if the output file exists\n",
    "        if os.path.exists(output_file_path):\n",
    "            # Append rows with empty \"Efficiency\" values to the specified output file without headers\n",
    "            empty_efficiency_rows.to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            # Write the rows with empty \"Efficiency\" values to the specified output file with headers\n",
    "            empty_efficiency_rows.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Rows with empty 'Efficiency' values appended to '{output_file_path}'.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No empty 'Efficiency' cells found.\")\n",
    "\n",
    "# Example usage:\n",
    "fill_empty_efficiency_with_one(power_plants_clean_data_file_path, power_plants_all_data_not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f1c23a5-6951-413b-951f-e016a97d3ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty 'Efficiency' cells filled with 1 in the original file.\n",
      "Rows with empty 'Efficiency' values appended to '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def fill_empty_efficiency_with_one(input_file_path, output_file_path):\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(input_file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{input_file_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Check if there are empty cells in the \"Efficiency\" column\n",
    "    empty_efficiency_rows = df[df['Efficiency'].isnull()]\n",
    "\n",
    "    if not empty_efficiency_rows.empty:\n",
    "        # Replace empty cells in the \"Efficiency\" column with 1\n",
    "        df['Efficiency'].fillna(1, inplace=True)\n",
    "\n",
    "        # Write the updated DataFrame back to the original CSV file\n",
    "        df.to_csv(input_file_path, index=False)\n",
    "\n",
    "        print(\"Empty 'Efficiency' cells filled with 1 in the original file.\")\n",
    "\n",
    "        # Check if the output file exists\n",
    "        if os.path.exists(output_file_path):\n",
    "            # Append rows with empty \"Efficiency\" values to the specified output file without headers\n",
    "            empty_efficiency_rows.to_csv(output_file_path, mode='a', index=False, header=False)\n",
    "        else:\n",
    "            # Write the rows with empty \"Efficiency\" values to the specified output file with headers\n",
    "            empty_efficiency_rows.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f\"Rows with empty 'Efficiency' values appended to '{output_file_path}'.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No empty 'Efficiency' cells found.\")\n",
    "\n",
    "# Example usage:\n",
    "fill_empty_efficiency_with_one(power_plants_clean_clustered_data_file_path, power_plants_all_data_not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc041b6a-fc7f-4d98-ac54-234459ddc203",
   "metadata": {},
   "source": [
    "- verifiyng variables\n",
    "\n",
    "  This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c62c324-2a83-4485-8d30-f0998e2dda19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zone_folder_name: DE\n",
      "zone_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_raw_data_file_name: 2020-01.csv\n",
      "power_plants_raw_data_download_link: https://data.open-power-system-data.org/conventional_power_plants/2020-10-01/conventional_power_plants_DE.csv\n",
      "power_plants_raw_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-01.csv\n",
      "power_plants_raw_data_download_link_sources_file_name: power_plants_raw_data_download_link_sources.csv\n",
      "power_plants_raw_data_download_link_sources_folder_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants\n",
      "power_plants_clean_data_file_name: 2020.csv\n",
      "power_plants_clean_data_folder_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE\n",
      "power_plants_clean_data_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020.csv\n",
      "power_plants_clean_data_equivalent_headers_file_name: power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_data_equivalent_headers_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_clean_data_equivalent_headers.csv\n",
      "power_plants_clean_clustered_data_file_name: 2020-Clustered.csv\n",
      "power_plants_clean_clustered_data_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/DE/2020-Clustered.csv\n",
      "power_plants_all_data_equivalent_technologies_file_name: power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_equivalent_technologies_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_technologies.csv\n",
      "power_plants_all_data_not_defined_units_file_name: power_plants_all_data_not_defined_units.csv\n",
      "power_plants_all_data_not_defined_units_file_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv\n",
      "power_plants_all_data_equivalent_fuels_file_name power_plants_all_data_equivalent_fuels.csv\n",
      "power_plants_all_data_equivalent_fuels_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_fuels.csv\n",
      "power_plants_all_data_equivalent_CHPTypes_file_name power_plants_all_data_equivalent_CHPTypes.csv\n",
      "power_plants_all_data_equivalent_CHPTypes_file_path /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_equivalent_CHPTypes.csv\n",
      "EU_Power_Units_Technical_Features_File_name: EU_Power_Units_Technical_Features.csv\n",
      "EU_Power_Units_Technical_Features_File_path: /home/ray/Dispa-SET_Unleash/RawData/PowerPlants/EU_Power_Units_Technical_Features.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"zone_folder_name: {zone_folder_name}\\nzone_folder_path: {zone_folder_path}\\npower_plants_raw_data_folder_path: {power_plants_raw_data_folder_path}\\npower_plants_raw_data_file_name: {power_plants_raw_data_file_name}\\npower_plants_raw_data_download_link: {power_plants_raw_data_download_link}\\npower_plants_raw_data_file_path: {power_plants_raw_data_file_path}\\npower_plants_raw_data_download_link_sources_file_name: {power_plants_raw_data_download_link_sources_file_name}\\npower_plants_raw_data_download_link_sources_folder_path: {power_plants_raw_data_download_link_sources_folder_path}\\npower_plants_clean_data_file_name: {power_plants_clean_data_file_name}\\npower_plants_clean_data_folder_path {power_plants_clean_data_folder_path}\\npower_plants_clean_data_file_path {power_plants_clean_data_file_path}\\npower_plants_clean_data_equivalent_headers_file_name: {power_plants_clean_data_equivalent_headers_file_name}\\npower_plants_clean_data_equivalent_headers_file_path: {power_plants_clean_data_equivalent_headers_file_path}\\npower_plants_clean_clustered_data_file_name: {power_plants_clean_clustered_data_file_name}\\npower_plants_clean_clustered_data_file_path: {power_plants_clean_clustered_data_file_path}\\npower_plants_all_data_equivalent_technologies_file_name: {power_plants_all_data_equivalent_technologies_file_name}\\npower_plants_all_data_equivalent_technologies_file_path: {power_plants_all_data_equivalent_technologies_file_path}\\npower_plants_all_data_not_defined_units_file_name: {power_plants_all_data_not_defined_units_file_name}\\npower_plants_all_data_not_defined_units_file_path: {power_plants_all_data_not_defined_units_file_path}\\npower_plants_all_data_equivalent_fuels_file_name {power_plants_all_data_equivalent_fuels_file_name}\\npower_plants_all_data_equivalent_fuels_file_path {power_plants_all_data_equivalent_fuels_file_path}\\npower_plants_all_data_equivalent_CHPTypes_file_name {power_plants_all_data_equivalent_CHPTypes_file_name}\\npower_plants_all_data_equivalent_CHPTypes_file_path {power_plants_all_data_equivalent_CHPTypes_file_path}\\nEU_Power_Units_Technical_Features_File_name: {EU_Power_Units_Technical_Features_File_name}\\nEU_Power_Units_Technical_Features_File_path: {EU_Power_Units_Technical_Features_File_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac5dd9c-37cf-4e50-a223-e60b413d60b3",
   "metadata": {},
   "source": [
    "3.12. MinUpTime Field Fulfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3deaf2f-d887-4cef-8b9e-d6607a83b193",
   "metadata": {},
   "source": [
    "    This part copy the closer value of the MinUpTime technical feauture from the already agregated data base EU_Power_Units_Technical_Features.csv using the notebook: EU_Power_Plant_Technical_Data_Base_Gathering.ipynb sellecting in base on the Technology, the Fuel and the PowerCapacity features. This is done just to the empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ccc14839-6a8e-421e-a2b3-f8a9afdec6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinUpTime values...\n",
      "MinUpTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_up_time_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinUpTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"MinUpTime\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['MinUpTime']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"MinUpTime\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'MinUpTime'] = closest_row['MinUpTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinUpTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'MinUpTime']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_min_up_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d54cf9b9-ed0a-4f57-b043-edb284ecb065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinUpTime values...\n",
      "MinUpTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_up_time_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinUpTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"MinUpTime\" value is empty in the second file\n",
    "            if pd.isnull(second_row['MinUpTime']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"MinUpTime\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'MinUpTime'] = closest_row['MinUpTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinUpTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'MinUpTime']\n",
    "\n",
    "copy_min_up_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3adeecce-0c0f-43a4-a11f-d0029c536ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinUpTime values...\n",
      "MinUpTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_up_time_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinUpTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"MinUpTime\" value is empty in the second file\n",
    "            if pd.isnull(second_row['MinUpTime']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"MinUpTime\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'MinUpTime'] = closest_row['MinUpTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinUpTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'MinUpTime']\n",
    "\n",
    "copy_min_up_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d88a0fd-f7e5-411a-af32-f55fd58ade9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinUpTime values...\n",
      "MinUpTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_up_time_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinUpTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"MinUpTime\" value is empty in the second file\n",
    "            if pd.isnull(second_row['MinUpTime']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"MinUpTime\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'MinUpTime'] = closest_row['MinUpTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinUpTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'MinUpTime']\n",
    "\n",
    "copy_min_up_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab157ed-1160-4dbb-911d-aa9a076219f6",
   "metadata": {},
   "source": [
    "3.13. MinDownTime RampUpRate Fulfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9dfb1d-77b8-42e3-9b16-fc3ac437f475",
   "metadata": {},
   "source": [
    "    This part copy the closer value of the MinDownTime technical feauture from the already agregated data base EU_Power_Units_Technical_Features.csv using the notebook: EU_Power_Plant_Technical_Data_Base_Gathering.ipynb sellecting in base on the Technology, the Fuel and the PowerCapacity features. This is done just to the empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e3fccb0-e072-4d9c-9b41-f304816d74f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinDownTime values...\n",
      "MinDownTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_down_time_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinDownTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"MinDownTime\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['MinDownTime']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"MinDownTime\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'MinDownTime'] = closest_row['MinDownTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinDownTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'MinDownTime']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_min_down_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d900f34-5bac-4afa-8633-01d3a81dd81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinDownTime values...\n",
      "MinDownTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_down_time_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinDownTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"MinDownTime\" value is empty in the second file\n",
    "            if pd.isnull(second_row['MinDownTime']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"MinDownTime\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'MinDownTime'] = closest_row['MinDownTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinDownTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'MinDownTime']\n",
    "\n",
    "copy_min_down_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed52ae08-3429-44a7-a5a3-30e1a60c05cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinDownTime values...\n",
      "MinDownTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_down_time_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinDownTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"MinDownTime\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['MinDownTime']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"MinDownTime\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'MinDownTime'] = closest_row['MinDownTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinDownTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'MinDownTime']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_min_down_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2114844-9926-49f1-adf6-60f01272ae6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinDownTime values...\n",
      "MinDownTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_down_time_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinDownTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"MinDownTime\" value is empty in the second file\n",
    "            if pd.isnull(second_row['MinDownTime']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"MinDownTime\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'MinDownTime'] = closest_row['MinDownTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinDownTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'MinDownTime']\n",
    "\n",
    "copy_min_down_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9949cfb-2e42-4179-bc6c-03f34af564f8",
   "metadata": {},
   "source": [
    "3.14. RampUpRate Field Fulfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275799a-135f-4cf4-9319-3fccc3badcc5",
   "metadata": {},
   "source": [
    "    This part copy the closer value of the RampUpRate technical feauture from the already agregated data base EU_Power_Units_Technical_Features.csv using the notebook: EU_Power_Plant_Technical_Data_Base_Gathering.ipynb sellecting in base on the Technology, the Fuel and the PowerCapacity features. This is done just to the empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53940962-8933-408c-a18e-d9b4b2224f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampUpRate values...\n",
      "RampUpRate values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramp_up_rate_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampUpRate values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"RampUpRate\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['RampUpRate']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"RampUpRate\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'RampUpRate'] = closest_row['RampUpRate'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampUpRate values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'RampUpRate']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_ramp_up_rate_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e83f32f8-43f5-4010-b03a-bca40d991651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampDownRate values...\n",
      "RampDownRate values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramp_down_rate_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampDownRate values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"RampDownRate\" value is empty in the second file\n",
    "            if pd.isnull(second_row['RampDownRate']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"RampDownRate\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'RampDownRate'] = closest_row['RampDownRate'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampDownRate values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'RampDownRate']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_ramp_down_rate_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "261a706a-504c-499e-b197-d074f3a586f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampUpRate values...\n",
      "RampUpRate values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramp_up_rate_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampUpRate values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"RampUpRate\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['RampUpRate']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"RampUpRate\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'RampUpRate'] = closest_row['RampUpRate'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampUpRate values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'RampUpRate']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_ramp_up_rate_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "902bb2c7-3baa-4d42-9ec7-84ab98462da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying MinDownTime values...\n",
      "MinDownTime values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_min_down_time_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying MinDownTime values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"MinDownTime\" value is empty in the second file\n",
    "            if pd.isnull(second_row['MinDownTime']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"MinDownTime\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'MinDownTime'] = closest_row['MinDownTime'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"MinDownTime values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'MinDownTime']\n",
    "\n",
    "copy_min_down_time_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f45cd-a678-4cc2-89fc-aee544831f0b",
   "metadata": {},
   "source": [
    "3.15. RampDownRate Fulfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719036f4-a41b-4aba-841a-e0e24692280b",
   "metadata": {},
   "source": [
    "    This part copy the closer value of the RampDownRate technical feauture from the already agregated data base EU_Power_Units_Technical_Features.csv using the notebook: EU_Power_Plant_Technical_Data_Base_Gathering.ipynb sellecting in base on the Technology, the Fuel and the PowerCapacity features. This is done just to the empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42b4e7d7-04ba-40f0-acb2-7f5dec1c7267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampDownRate values...\n",
      "RampDownRate values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramp_down_rate_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampDownRate values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"RampDownRate\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['RampDownRate']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"RampDownRate\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'RampDownRate'] = closest_row['RampDownRate'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampDownRate values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'RampDownRate']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_ramp_down_rate_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50386ef7-aec5-4084-ac12-024a1db81e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampDownRate values...\n",
      "RampDownRate values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramp_down_rate_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampDownRate values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"RampDownRate\" value is empty in the second file\n",
    "            if pd.isnull(second_row['RampDownRate']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"RampDownRate\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'RampDownRate'] = closest_row['RampDownRate'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampDownRate values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'RampDownRate']\n",
    "\n",
    "copy_ramp_down_rate_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c9a7ce5-b639-4572-85d9-58c1fc3cd305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampDownRate values...\n",
      "RampDownRate values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramp_down_rate_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampDownRate values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"RampDownRate\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['RampDownRate']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"RampDownRate\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'RampDownRate'] = closest_row['RampDownRate'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampDownRate values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'RampDownRate']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_ramp_down_rate_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66aa825c-3ce9-4b3f-a259-d9e05e624635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampDownRate values...\n",
      "RampDownRate values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramp_down_rate_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampDownRate values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"RampDownRate\" value is empty in the second file\n",
    "            if pd.isnull(second_row['RampDownRate']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"RampDownRate\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'RampDownRate'] = closest_row['RampDownRate'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampDownRate values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'RampDownRate']\n",
    "\n",
    "copy_ramp_down_rate_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411dba73-cec0-478f-80c4-ef3313c0a3b9",
   "metadata": {},
   "source": [
    "3.16. StartUpCost Field Fulfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1a52a-f54f-4f1d-8346-19fbbb37a495",
   "metadata": {},
   "source": [
    "    This part copy the closer value of the StartUpCost technical feauture from the already agregated data base EU_Power_Units_Technical_Features.csv using the notebook: EU_Power_Plant_Technical_Data_Base_Gathering.ipynb sellecting in base on the Technology, the Fuel and the PowerCapacity features. This is done just to the empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20dfc561-8c67-421b-bd17-2c0f6a3fda6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying StartUpCost values...\n",
      "StartUpCost values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_start_up_cost_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying StartUpCost values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"StartUpCost\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['StartUpCost']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"StartUpCost\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'StartUpCost'] = closest_row['StartUpCost'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"StartUpCost values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'StartUpCost']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_start_up_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86d60c08-aefd-48b7-aa26-c171ad88ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying StartUpCost values...\n",
      "StartUpCost values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_start_up_cost_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying StartUpCost values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"StartUpCost\" value is empty in the second file\n",
    "            if pd.isnull(second_row['StartUpCost']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"StartUpCost\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'StartUpCost'] = closest_row['StartUpCost'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"StartUpCost values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'StartUpCost']\n",
    "\n",
    "copy_start_up_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a784050-e15f-4cfd-9142-10ed696bca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying StartUpCost values...\n",
      "StartUpCost values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_start_up_cost_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying StartUpCost values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"StartUpCost\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['StartUpCost']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"StartUpCost\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'StartUpCost'] = closest_row['StartUpCost'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"StartUpCost values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'StartUpCost']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_start_up_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "946fef8a-1767-43f6-b485-2272c5299c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying StartUpCost values...\n",
      "StartUpCost values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_start_up_cost_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying StartUpCost values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"StartUpCost\" value is empty in the second file\n",
    "            if pd.isnull(second_row['StartUpCost']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"StartUpCost\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'StartUpCost'] = closest_row['StartUpCost'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"StartUpCost values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'StartUpCost']\n",
    "\n",
    "copy_start_up_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9c1aa-f924-453b-8be2-69846dcbb2fe",
   "metadata": {},
   "source": [
    "3.17. NoLoadCost_pu Field Fulfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63064a41-b1a8-4183-aa96-22841cd4069e",
   "metadata": {},
   "source": [
    "    This part copy the closer value of the NoLoadCost_pu technical feauture from the already agregated data base EU_Power_Units_Technical_Features.csv using the notebook: EU_Power_Plant_Technical_Data_Base_Gathering.ipynb sellecting in base on the Technology, the Fuel and the PowerCapacity features. This is done just to the empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "84490407-51d1-40ec-a253-cc88ba2cb50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying NoLoadCost_pu values...\n",
      "NoLoadCost_pu values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_no_load_cost_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying NoLoadCost_pu values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"NoLoadCost_pu\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['NoLoadCost_pu']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"NoLoadCost_pu\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'NoLoadCost_pu'] = closest_row['NoLoadCost_pu'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"NoLoadCost_pu values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'NoLoadCost_pu']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_no_load_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b7ad723-efd4-4e0c-a0f2-f2bcfbc054de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying NoLoadCost_pu values...\n",
      "NoLoadCost_pu values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_no_load_cost_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying NoLoadCost_pu values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"NoLoadCost_pu\" value is empty in the second file\n",
    "            if pd.isnull(second_row['NoLoadCost_pu']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"NoLoadCost_pu\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'NoLoadCost_pu'] = closest_row['NoLoadCost_pu'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"NoLoadCost_pu values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'NoLoadCost_pu']\n",
    "\n",
    "copy_no_load_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fbfc1b6e-8fe0-4fb8-898d-c2e537605a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying NoLoadCost_pu values...\n",
      "NoLoadCost_pu values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_no_load_cost_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying NoLoadCost_pu values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"NoLoadCost_pu\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['NoLoadCost_pu']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"NoLoadCost_pu\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'NoLoadCost_pu'] = closest_row['NoLoadCost_pu'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"NoLoadCost_pu values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'NoLoadCost_pu']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_no_load_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e087bdc-f1b8-4acd-8ebe-fd0e1757de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying NoLoadCost_pu values...\n",
      "NoLoadCost_pu values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_no_load_cost_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying NoLoadCost_pu values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"NoLoadCost_pu\" value is empty in the second file\n",
    "            if pd.isnull(second_row['NoLoadCost_pu']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"NoLoadCost_pu\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'NoLoadCost_pu'] = closest_row['NoLoadCost_pu'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"NoLoadCost_pu values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'NoLoadCost_pu']\n",
    "\n",
    "copy_no_load_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0fc198-05e7-4be7-aa22-2f8b1538ffac",
   "metadata": {},
   "source": [
    "3.18. RampingCost Field Fulfilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef88064-182c-4f84-90fd-14dcea7f8885",
   "metadata": {},
   "source": [
    "    This part copy the closer value of the RampingCost technical feauture from the already agregated data base EU_Power_Units_Technical_Features.csv using the notebook: EU_Power_Plant_Technical_Data_Base_Gathering.ipynb sellecting in base on the Technology, the Fuel and the PowerCapacity features. This is done just to the empty fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "12107d2b-e0cc-4244-92ac-e66c0d1d10df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampingCost values...\n",
      "RampingCost values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramping_cost_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampingCost values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"RampingCost\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['RampingCost']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"RampingCost\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'RampingCost'] = closest_row['RampingCost'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampingCost values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'RampingCost']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_ramping_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "002f74ad-3c2c-4f60-a501-87d46cbb0a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampingCost values...\n",
      "RampingCost values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramping_cost_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampingCost values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"RampingCost\" value is empty in the second file\n",
    "            if pd.isnull(second_row['RampingCost']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"RampingCost\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'RampingCost'] = closest_row['RampingCost'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampingCost values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'RampingCost']\n",
    "\n",
    "copy_ramping_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4274d6d3-fc8e-4d76-9d5b-1a39e841de1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampingCost values...\n",
      "RampingCost values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramping_cost_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampingCost values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"RampingCost\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['RampingCost']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"RampingCost\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'RampingCost'] = closest_row['RampingCost'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampingCost values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'RampingCost']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_ramping_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dd267fbd-4d1f-4319-9c91-347598555bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying RampingCost values...\n",
      "RampingCost values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_ramping_cost_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying RampingCost values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"RampingCost\" value is empty in the second file\n",
    "            if pd.isnull(second_row['RampingCost']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"RampingCost\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'RampingCost'] = closest_row['RampingCost'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"RampingCost values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'RampingCost']\n",
    "\n",
    "copy_ramping_cost_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_clustered_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987a9bc-e93c-4e08-8086-f3750f5781c3",
   "metadata": {},
   "source": [
    "3.19. PartLoadMin Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c91160-6989-432b-aaae-5d9e200c5af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75eb5d07-302d-40e4-ba49-59776474eb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying PartLoadMin values...\n",
      "PartLoadMin values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_part_load_min_values(first_file_path, second_file_path, common_columns, not_defined_units_file_path):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying PartLoadMin values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over unique values of \"Fuel\" in the second file\n",
    "        for fuel in second_filtered['Fuel'].unique():\n",
    "            # Filter rows with matching \"Fuel\" in both files\n",
    "            first_fuel_filtered = first_filtered[first_filtered['Fuel'] == fuel]\n",
    "            second_fuel_filtered = second_filtered[second_filtered['Fuel'] == fuel]\n",
    "\n",
    "            # Check if any rows exist in the first file for this combination of \"Technology\" and \"Fuel\"\n",
    "            if first_fuel_filtered.empty:\n",
    "                # No matching rows found, so copy the entire rows from the second file to the not defined units file\n",
    "                with open(not_defined_units_file_path, 'a') as f:\n",
    "                    for index, second_row in second_fuel_filtered.iterrows():\n",
    "                        f.write(','.join(map(str, second_row.values)) + '\\n')\n",
    "            else:\n",
    "                # Iterate over rows in the second file\n",
    "                for index, second_row in second_fuel_filtered.iterrows():\n",
    "                    # Check if \"PartLoadMin\" value is empty in the second file\n",
    "                    if pd.isnull(second_row['PartLoadMin']):\n",
    "                        # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                        closest_row = first_fuel_filtered.iloc[(first_fuel_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                        # Copy \"PartLoadMin\" value from the first file to the second file\n",
    "                        if not closest_row.empty:\n",
    "                            second_df.at[index, 'PartLoadMin'] = closest_row['PartLoadMin'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"PartLoadMin values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'Fuel', 'PartLoadMin']\n",
    "not_defined_units_file_path = '/home/ray/Dispa-SET_Unleash/RawData/PowerPlants/power_plants_all_data_not_defined_units.csv'\n",
    "\n",
    "copy_part_load_min_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns, not_defined_units_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "180e23f1-ab29-457a-9c81-fec8175314c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying PartLoadMin values...\n",
      "PartLoadMin values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def copy_part_load_min_values(first_file_path, second_file_path, common_columns):\n",
    "    # Read CSV files into DataFrames\n",
    "    first_df = pd.read_csv(first_file_path)\n",
    "    second_df = pd.read_csv(second_file_path)\n",
    "\n",
    "    print(\"Copying PartLoadMin values...\")\n",
    "\n",
    "    # Iterate over unique values of \"Technology\" in the second file\n",
    "    for technology in second_df['Technology'].unique():\n",
    "        # Filter rows in both files where \"Technology\" matches\n",
    "        first_filtered = first_df[first_df['Technology'] == technology]\n",
    "        second_filtered = second_df[second_df['Technology'] == technology]\n",
    "\n",
    "        # Iterate over rows in the second file\n",
    "        for index, second_row in second_filtered.iterrows():\n",
    "            # Check if \"PartLoadMin\" value is empty in the second file\n",
    "            if pd.isnull(second_row['PartLoadMin']):\n",
    "                # Find the row in the first file with the closest \"PowerCapacity\" value\n",
    "                closest_row = first_filtered.iloc[(first_filtered['PowerCapacity'] - second_row['PowerCapacity']).abs().argsort()[:1]]\n",
    "\n",
    "                # Copy \"PartLoadMin\" value from the first file to the second file\n",
    "                if not closest_row.empty:\n",
    "                    second_df.at[index, 'PartLoadMin'] = closest_row['PartLoadMin'].values[0]\n",
    "\n",
    "    # Write updated DataFrame to the second file\n",
    "    second_df.to_csv(second_file_path, index=False)\n",
    "\n",
    "    print(\"PartLoadMin values copied successfully.\")\n",
    "\n",
    "common_columns = ['PowerCapacity', 'Technology', 'PartLoadMin']\n",
    "\n",
    "copy_part_load_min_values(EU_Power_Units_Technical_Features_File_path, power_plants_clean_data_file_path, common_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff02fd-6f51-4158-88da-1eb4440ed8cd",
   "metadata": {},
   "source": [
    "3.20. MinEfficiency Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c843635-dabd-4198-b3cd-0f8c66fe0744",
   "metadata": {},
   "source": [
    "3.21. StartUpTime Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f58789-8edf-423d-bb84-96bbfe411fcf",
   "metadata": {},
   "source": [
    "3.22. CO2Intensity Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a170e4a-fd60-43a7-b3b3-21bfa947886d",
   "metadata": {},
   "source": [
    "3.23. COP Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961252c-e9e1-46c7-877a-e7d6b5e09b7a",
   "metadata": {},
   "source": [
    "3.24. RampUpRate Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c520cef-a947-44ea-99af-4f8ed6ecfc72",
   "metadata": {},
   "source": [
    "3.25. Tnominal Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48542b0-a58d-4504-a905-f03dba142436",
   "metadata": {},
   "source": [
    "3.26. coef_COP_a Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae8b74-9230-4ea8-b95f-40f4bc1fea60",
   "metadata": {},
   "source": [
    "3.27. coef_COP_b Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2302a0d-9280-417c-84a7-8fddfc3328fc",
   "metadata": {},
   "source": [
    "3.28. STOCapacity Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4656b629-633f-452b-9076-339cd331ba88",
   "metadata": {},
   "source": [
    "3.29. STOSelfDischarge Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1912685-e652-47f0-8610-2ed0ca4f600b",
   "metadata": {},
   "source": [
    "3.30. STOMaxChargingPower Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129e144-b32e-467f-9f93-184dd30f9c29",
   "metadata": {},
   "source": [
    "3.31. STOChargingEfficiency Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d089f-974c-48a0-a6a7-5b6eefdad61e",
   "metadata": {},
   "source": [
    "3.32. WaterWithdrawal Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c982cfc-2475-4bee-a1b6-0ea668d48839",
   "metadata": {},
   "source": [
    "3.33. WaterConsumption Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c2a52-8e16-4c4e-bfde-1bf33e47d9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a668b-94f8-44fc-9feb-7ba7cd3b9b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
