{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cce1e5-a0c7-4d8b-a923-da1c7224ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entsoe import EntsoePandasClient\n",
    "import pandas as pd\n",
    "\n",
    "client = EntsoePandasClient(api_key='61e5bbbb-7e80-4540-a471-bd993873aa74')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aeac48d-9eeb-4a61-893c-fbcf8de1e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.Timestamp('20230101', tz='Europe/Brussels')\n",
    "end = pd.Timestamp('20230102', tz='Europe/Brussels')\n",
    "country_code = 'DK'  # Belgium\n",
    "country_code_from = 'FR'  # France\n",
    "country_code_to = 'DE_LU' # Germany-Luxembourg\n",
    "type_marketagreement_type = 'A01'\n",
    "contract_marketagreement_type = \"A01\"\n",
    "process_type = 'A51'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc55ab86-4631-4bae-8aca-c4e78fbee843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Connection Error, retrying in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "query_unavailability_of_generation_units = client.query_unavailability_of_generation_units(country_code, start=start, end=end, docstatus=None, periodstartupdate=None, periodendupdate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0440db-1de1-4a86-9a8b-59c988a5f5cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_unavailability_of_generation_units' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\n\u001b[0;31m    query_unavailability_of_generation_units\u001b[0;36m\n",
      "\u001b[0;31mNameError\u001b[0m\u001b[0;31m:\u001b[0m name 'query_unavailability_of_generation_units' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 854\n"
     ]
    }
   ],
   "source": [
    "query_unavailability_of_generation_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68912f-03f7-49e9-b748-d705ae7a4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_unavailability_of_production_units = client.query_unavailability_of_production_units(country_code, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5e1df-e56c-4546-82f8-f22f60df2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_unavailability_of_production_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22ab56-5a19-4243-bf92-52f749003eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_installed_generation_capacity_per_units = client.query_installed_generation_capacity_per_unit(country_code, start=start, end=end, psr_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a733d-0634-45dc-88e7-97390344bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_installed_generation_capacity_per_units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee01ab6-9fee-4abb-9c6b-d388798ad5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '/home/ray/Desktop/Backup/Prueva/1.csv'\n",
    "query_installed_generation_capacity_per_units.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8affc0a0-b5a3-4c41-9dea-f05b9dfae371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9bf5d-c5dc-4cbb-bd33-eb660c48b08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_unavailability_of_production_units = client.query_unavailability_of_production_units(country_code, start, end, docstatus=None, periodstartupdate=None, periodendupdate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1873fe-f12f-46b0-9210-5978d30fbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_unavailability_of_production_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac1cbd-de8a-4450-b074-9627ccc09234",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '/home/ray/Desktop/Backup/Prueva/prueva1.csv'\n",
    "query_unavailability_of_generation_units.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00449b56-6e34-4c72-a8bc-246bc210bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = client.query_installed_generation_capacity_per_unit(country_code, start=start, end=end, psr_type=None)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd988b-767f-48fe-af25-ef9ef9073852",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path_1 = '/home/ray/Desktop/Backup/Prueva/prueva2.csv'\n",
    "data_1.to_csv(output_file_path_1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b2a02-b197-4a9a-b2e8-4741cf5b068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = client.query_generation(country_code, start=start, end=end, psr_type=None)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c771d3e-8807-41c6-896b-4107b386af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import http.client\n",
    "import csv\n",
    "\n",
    "# Set max_headers to a higher value\n",
    "http.client._MAXHEADERS = 1000\n",
    "\n",
    "# Function to download table data from a URL\n",
    "def download_table_data(url):\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Raise an error if there are too many headers\n",
    "    if len(response.headers) > 100:\n",
    "        raise Exception(\"Too many headers in the response\")\n",
    "\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table element\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extract data from the table\n",
    "    data = []\n",
    "    for row in table.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "        if row_data:  # Ensures we don't add empty rows\n",
    "            data.append(row_data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "url = 'https://transparency.entsoe.eu/generation/r2/waterReservoirsAndHydroStoragePlants/show?name=&defaultValue=false&viewType=TABLE&areaType=CTY&atch=false&dateTime.dateTime=01.01.2016+00:00|UTC|YEAR&dateTime.endDateTime=01.01.2024+00:00|UTC|YEAR&area.values=CTY|10YAT-APG------L!CTY|10YAT-APG------L'\n",
    "table_data = download_table_data(url)\n",
    "if table_data:\n",
    "    for row in table_data:\n",
    "        print(row)\n",
    "\n",
    "    output_file_path = \"/home/ray/Desktop/Backup/Prueva/prueva1.csv\"\n",
    "    with open(output_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a0e90-4ce6-47fb-8df5-0e54ceffc3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76ba89-e1fa-406f-8171-af946e496d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a101d-b598-41e7-9556-294e919a8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the variables\n",
    "start_year = 2016\n",
    "end_year = 2024\n",
    "data_year = 2023\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Change the name of the original CSV file\n",
    "new_file_name = f\"{start_year}_{end_year}.csv\"\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), new_file_name)\n",
    "os.rename(file_path, new_file_path)\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(new_file_path)\n",
    "\n",
    "# Extract the required columns\n",
    "columns_to_extract = ['Week', str(data_year)]  # Assuming the column with data_year as name is string type\n",
    "df_extracted = df[columns_to_extract]\n",
    "\n",
    "# Save the extracted columns to a new CSV file\n",
    "output_file_name = f\"{data_year}_1.csv\"\n",
    "output_file_path = os.path.join(os.path.dirname(file_path), output_file_name)\n",
    "df_extracted.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4a62e-5ade-4f93-be7e-4921d72cd152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f958f1e3-96f1-432a-ae3f-09b5b194dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the variables\n",
    "data_year = \"2023\"\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract week number from the 'Week' column\n",
    "df['Week'] = df['Week'].str.extract(r'(\\d+)').astype(int)  # Extract digits from the string and convert to int\n",
    "\n",
    "# Calculate the dates\n",
    "start_date = pd.to_datetime(f'{data_year}-01-01')  # Start from January 1st of the specified year\n",
    "df['Dispa-SET_Date'] = start_date + pd.to_timedelta((df['Week'] - 1) * 7, unit='D')  # Add the corresponding number of weeks\n",
    "\n",
    "# Convert to string in the desired format\n",
    "df['Dispa-SET_Date'] = df['Dispa-SET_Date'].dt.strftime('%Y-%m-%d 00:00:00+00:00')\n",
    "\n",
    "# Save the DataFrame back to the original CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Print the result\n",
    "print(\"DataFrame saved back to the original CSV file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac08ea-0d07-4d70-9e30-2d64bbfb4d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e6a23-b2e9-4572-9780-ac8d2afd300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Define the year\n",
    "year = 2020\n",
    "\n",
    "# Get the last day of the year\n",
    "last_day_of_year = datetime.date(year, 12, 31)\n",
    "\n",
    "# Get the ISO calendar week number of the last day of the year\n",
    "_, last_week_number, _ = last_day_of_year.isocalendar()\n",
    "\n",
    "print(f\"The number of weeks in {year} is {last_week_number}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86197d-8fe4-422a-8429-c6717b6b3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the start and end years\n",
    "start_year = 2016\n",
    "end_year = 2024\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/\"\n",
    "\n",
    "# Define the file name\n",
    "file_name = f\"{start_year}_{end_year}.csv\"\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add a new row with 'Week 54' in the first column\n",
    "df.loc[len(df)] = ['Week 54'] + [None] * (len(df.columns) - 1)\n",
    "\n",
    "# Iterate over columns (excluding the first one)\n",
    "for col in df.columns[1:]:\n",
    "    # Read the last row of the column\n",
    "    last_row_value = df.iloc[-1][col]\n",
    "    \n",
    "    # If the last row is empty\n",
    "    if pd.isna(last_row_value):\n",
    "        # Copy the value from the next column\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_first_value = df.iloc[0][next_col_name]\n",
    "            df.iloc[-1, df.columns.get_loc(col)] = next_col_first_value\n",
    "        \n",
    "        # Repeat the same process for the penultimate row\n",
    "        penultimate_row_value = df.iloc[-2][col]\n",
    "        if pd.isna(penultimate_row_value):\n",
    "            if next_col_index < len(df.columns):\n",
    "                next_col_second_value = df.iloc[1][next_col_name]\n",
    "                df.iloc[-2, df.columns.get_loc(col)] = next_col_second_value\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Process completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089172cb-e470-4923-bbaf-b56da1ec925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory path and file name\n",
    "folder_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/\"\n",
    "file_name = f\"{start_year}_{end_year}.csv\"\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add a new row with 'Week 54' in the first column\n",
    "df.loc[len(df)] = ['Week 54'] + [None] * (len(df.columns) - 1)\n",
    "\n",
    "# Iterate over the rest of the columns (from the second till the last)\n",
    "for col in df.columns[1:]:\n",
    "    # Read the penultimate and last rows of the column\n",
    "    penultimate_value = df.iloc[-2][col]\n",
    "    last_value = df.iloc[-1][col]\n",
    "\n",
    "    # If the penultimate row is empty\n",
    "    if pd.isna(penultimate_value):\n",
    "        # Copy the value from the next column to the penultimate field read\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_first_value = df.iloc[0][next_col_name]\n",
    "            df.iloc[-2, df.columns.get_loc(col)] = next_col_first_value\n",
    "\n",
    "    # Copy the value of the second field from the next column to the last field of the current column\n",
    "    if pd.isna(last_value):\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_second_value = df.iloc[1][next_col_name]\n",
    "            df.iloc[-1, df.columns.get_loc(col)] = next_col_second_value\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Process completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6685f-aa7a-494b-85d3-669663e839d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842af0be-dc8f-4c24-b9eb-ddcb057ccb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the last value of the first column from file_path_1\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "last_date_file1 = df1.iloc[-1, 0]\n",
    "\n",
    "# Read the last value of the Dispa-SET_Date column from file_path_2\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "last_date_file2 = df2['Dispa-SET_Date'].iloc[-1]\n",
    "\n",
    "# Check if last date from file_path_1 is less than last date from file_path_2\n",
    "if last_date_file1 < last_date_file2:\n",
    "    # Calculate hourly steps until reaching the last date from file_path_2\n",
    "    hourly_steps = pd.date_range(start=last_date_file1, end=last_date_file2, freq='h')\n",
    "\n",
    "    # Create a new DataFrame with hourly steps\n",
    "    df_new_rows = pd.DataFrame({'Unnamed: 0': hourly_steps})\n",
    "\n",
    "    # Append the new rows to file_path_1 DataFrame\n",
    "    df1 = pd.concat([df1, df_new_rows], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the original CSV file\n",
    "    df1.to_csv(file_path_1, index=False)\n",
    "    print(\"New rows added successfully.\")\n",
    "else:\n",
    "    print(\"No new rows need to be added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760a87c-6f58-458e-b03f-397e8984808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A  = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbadb7-7aac-4c42-a33d-7f3f66e3095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8b95c-6d5f-4919-8bb5-9494dffcaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path (replace with your actual path)\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5874760-2caa-40c7-ac4e-dbd68658d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeae031-f0b8-466a-a9a2-5451a93f344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the last field of the first column from file_path_1\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "last_date_file1 = pd.to_datetime(df1.iloc[-1, 0])\n",
    "\n",
    "# Read the last field of the 'Dispa-SET_Date' column from file_path_2\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "last_date_file2 = pd.to_datetime(df2['Dispa-SET_Date'].iloc[-1])\n",
    "\n",
    "# Check if last date from file_path_1 is less than last date from file_path_2\n",
    "if last_date_file1 < last_date_file2:\n",
    "    # Calculate hourly steps until reaching the last date from file_path_2\n",
    "    hourly_steps = pd.date_range(start=last_date_file1, end=last_date_file2, freq='1H')\n",
    "\n",
    "    # Create a new DataFrame with hourly steps\n",
    "    df_new_rows = pd.DataFrame({'Unnamed: 0': hourly_steps})\n",
    "\n",
    "    # Append the new rows to file_path_1 DataFrame\n",
    "    df1 = pd.concat([df1, df_new_rows], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the original CSV file\n",
    "    df1.to_csv(file_path_1, index=False)\n",
    "    print(f\"New rows added successfully to {file_path_1}\")\n",
    "else:\n",
    "    print(f\"No new rows need to be added to {file_path_1}\")\n",
    "\n",
    "print(\"Overall processing finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15561717-66c4-45aa-8e2e-7bc8e1a6e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path_1)\n",
    "\n",
    "# Remove rows with duplicated values in the first column\n",
    "df = df.drop_duplicates(subset=df.columns[0], keep='first')\n",
    "\n",
    "# Save the updated DataFrame back to the original CSV file\n",
    "df.to_csv(file_path_1, index=False)\n",
    "\n",
    "print(\"Duplicates removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c0470-ff88-499f-a456-636d16a58228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths and data year\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "data_year = 2023\n",
    "\n",
    "# Read CSV files\n",
    "df_1 = pd.read_csv(file_path_1)\n",
    "df_2 = pd.read_csv(file_path_2)\n",
    "\n",
    "# Get the column name corresponding to the data year\n",
    "year_column = str(data_year)\n",
    "\n",
    "# Get the minimum length of both DataFrames\n",
    "min_length = min(len(df_1), len(df_2))\n",
    "\n",
    "# Iterate over the rows of both DataFrames up to the minimum length\n",
    "for i in range(min_length):\n",
    "    # Get the value from the 'Dispa-SET_Date' column in df_2\n",
    "    date_value = df_2.at[i, 'Dispa-SET_Date']\n",
    "    # Check if the value exists in the first column of df_1\n",
    "    if date_value in df_1[df_1.columns[0]].values:\n",
    "        # Get the index where the value is found in df_1\n",
    "        index_values = df_1.index[df_1[df_1.columns[0]] == date_value]\n",
    "        # Iterate over the found index values\n",
    "        for index_value in index_values:\n",
    "            # Copy corresponding value from df_2 and paste it to the corresponding fields of the second and third columns in df_1\n",
    "            df_1.at[index_value, df_1.columns[1]] = df_2.at[i, year_column]\n",
    "            df_1.at[index_value, df_1.columns[2]] = df_2.at[i, year_column]\n",
    "    else:\n",
    "        print(f\"Date value {date_value} not found in file_path_1.\")\n",
    "\n",
    "# Save the updated DataFrame back to the original CSV file\n",
    "df_1.to_csv(file_path_1, index=False)\n",
    "print(\"Values copied successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d8b6d-fefb-4f92-84b3-821d82dd8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_1 = '/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path_1)\n",
    "\n",
    "# Perform linear interpolation along the second and third columns\n",
    "df[['HPHS', 'HDAM']] = df[['HPHS', 'HDAM']].interpolate(method='linear')\n",
    "\n",
    "# Save the interpolated DataFrame back to the CSV file\n",
    "df.to_csv(file_path_1, index=False)\n",
    "\n",
    "print(\"Linear interpolation completed for the second and third columns and saved to file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2bfa4-1ea1-4f0e-a703-29999abb16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/BG/1h/2023.csv\"\n",
    "\n",
    "# Define the data year\n",
    "data_year = 2023\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path_1)\n",
    "\n",
    "# Convert the first column to datetime\n",
    "df['Unnamed: 0'] = pd.to_datetime(df['Unnamed: 0'])\n",
    "\n",
    "# Extract the year from the date values\n",
    "df['Year'] = df['Unnamed: 0'].dt.year\n",
    "\n",
    "# Filter out rows that belong to the specified year\n",
    "df_filtered = df[df['Year'] == data_year].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "\n",
    "# Drop the 'Year' column\n",
    "df_filtered.drop(columns=['Year'], inplace=True)\n",
    "\n",
    "# Save the filtered DataFrame back to the CSV file\n",
    "df_filtered.to_csv(file_path_1, index=False)\n",
    "\n",
    "print(f\"Rows not belonging to {data_year} have been removed from {file_path_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d460d9-c0c3-45c8-904a-b41fc023e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://web-api.tp.entsoe.eu/api?securityToken=61e5bbbb-7e80-4540-a471-bd993873aa74documentType=A71&processType=A33&psrType=B02&in_Domain=10YCZ-CEPS-----N&periodStart=201512312300&periodEnd=201612312300\"\n",
    "\n",
    "# Make the GET request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Print the response content\n",
    "    print(response.text)\n",
    "else:\n",
    "    # Print an error message if the request was not successful\n",
    "    print(f\"Error: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e851d-b2c3-4a01-9235-88d4eaabbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the paths for input Excel file and output CSV file\n",
    "input_excel_path = '/home/ray/Downloads/Unavailability of Production and Generation Units_202301010000-202401010000.xlsx'\n",
    "output_csv_path = '/home/ray/Desktop/Backup/Prueva/prueva1.csv'\n",
    "\n",
    "# Load the Excel file into a pandas DataFrame\n",
    "df = pd.read_excel(input_excel_path)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(output_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b505c73-ad77-458b-b6e4-d4b82cafb320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f4786-64f1-493e-8922-5242911a5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame, skipping the first 6 rows\n",
    "df = pd.read_csv('/home/ray/Desktop/Backup/Prueva/prueva1.csv', skiprows=6)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('/home/ray/Desktop/Backup/Prueva/prueva1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30672920-8d23-4825-ae55-281fbb6e80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fd6d8-01e1-40e3-b962-cda5a7e3d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "type_marketagreement_type = 'A01'\n",
    "contract_marketagreement_type = \"A01\"\n",
    "process_type = 'A51'\n",
    "\n",
    "# Define the path to the outage factors raw data folder\n",
    "outage_factors_raw_data_folder_path = f'/home/ray/Dispa-SET_Unleash/RawData/OutageFactors/Raw_Data_Sources/{data_year}/'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(outage_factors_raw_data_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize variable to store headers\n",
    "headers = None\n",
    "\n",
    "# Iterate over each country code\n",
    "for country_code in country_list:\n",
    "    # Define the start and end timestamps for the year\n",
    "    start = pd.Timestamp(str(data_year) + '0101', tz='Europe/Brussels')\n",
    "    end = pd.Timestamp(str(data_year) + '1231', tz='Europe/Brussels')\n",
    "\n",
    "    # Query unavailability of generation units for the current country code\n",
    "    query_unavailability_of_generation_units = client.query_unavailability_of_generation_units(country_code, start=start, end=end, docstatus=None, periodstartupdate=None, periodendupdate=None)\n",
    "    \n",
    "    # Check if the resulting DataFrame is empty\n",
    "    if not query_unavailability_of_generation_units.empty:\n",
    "        # Define the output file path using the country code\n",
    "        output_file_path = f'{outage_factors_raw_data_folder_path}{country_code}.csv'\n",
    "        \n",
    "        # Save the queried data to a CSV file\n",
    "        query_unavailability_of_generation_units.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        # Update headers if needed\n",
    "        headers = query_unavailability_of_generation_units.columns\n",
    "    else:\n",
    "        # If the DataFrame is empty, create an empty CSV file with the same headers as the last file saved\n",
    "        if headers is not None:\n",
    "            empty_csv_path = f'{outage_factors_raw_data_folder_path}{country_code}.csv'\n",
    "            pd.DataFrame(columns=headers).to_csv(empty_csv_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
