{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "55cce1e5-a0c7-4d8b-a923-da1c7224ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entsoe import EntsoePandasClient\n",
    "import pandas as pd\n",
    "\n",
    "client = EntsoePandasClient(api_key='61e5bbbb-7e80-4540-a471-bd993873aa74')\n",
    "\n",
    "start = pd.Timestamp('20230101', tz='Europe/Brussels')\n",
    "end = pd.Timestamp('20231231', tz='Europe/Brussels')\n",
    "country_code = 'AT'  # Belgium\n",
    "country_code_from = 'FR'  # France\n",
    "country_code_to = 'DE_LU' # Germany-Luxembourg\n",
    "type_marketagreement_type = 'A01'\n",
    "contract_marketagreement_type = \"A01\"\n",
    "process_type = 'A01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45480c64-bb6d-48d3-a5d8-d98b8f2c749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-12-25 23:00:00+00:00    1678718.0\n",
       "2023-01-01 23:00:00+00:00    1655961.0\n",
       "2023-01-08 23:00:00+00:00    1597119.0\n",
       "2023-01-15 23:00:00+00:00    1540674.0\n",
       "2023-01-22 23:00:00+00:00    1375097.0\n",
       "2023-01-29 23:00:00+00:00    1269456.0\n",
       "2023-02-05 23:00:00+00:00    1184410.0\n",
       "2023-02-12 23:00:00+00:00    1074334.0\n",
       "2023-02-19 23:00:00+00:00    1040288.0\n",
       "2023-02-26 23:00:00+00:00     945965.0\n",
       "2023-03-05 23:00:00+00:00     818682.0\n",
       "2023-03-12 23:00:00+00:00     779143.0\n",
       "2023-03-19 23:00:00+00:00     696653.0\n",
       "2023-03-26 22:00:00+00:00     641040.0\n",
       "2023-04-02 22:00:00+00:00     592454.0\n",
       "2023-04-09 22:00:00+00:00     528185.0\n",
       "2023-04-16 22:00:00+00:00     468331.0\n",
       "2023-04-23 22:00:00+00:00     450518.0\n",
       "2023-04-30 22:00:00+00:00     462364.0\n",
       "2023-05-07 22:00:00+00:00     470984.0\n",
       "2023-05-14 22:00:00+00:00     486042.0\n",
       "2023-05-21 22:00:00+00:00     566309.0\n",
       "2023-05-28 22:00:00+00:00     687991.0\n",
       "2023-06-04 22:00:00+00:00     780684.0\n",
       "2023-06-11 22:00:00+00:00     876458.0\n",
       "2023-06-18 22:00:00+00:00     980689.0\n",
       "2023-06-25 22:00:00+00:00    1123011.0\n",
       "2023-07-02 22:00:00+00:00    1236938.0\n",
       "2023-07-09 22:00:00+00:00    1344774.0\n",
       "2023-07-16 22:00:00+00:00    1498091.0\n",
       "2023-07-23 22:00:00+00:00    1584708.0\n",
       "2023-07-30 22:00:00+00:00    1693210.0\n",
       "2023-08-06 22:00:00+00:00    1814757.0\n",
       "2023-08-13 22:00:00+00:00    1849541.0\n",
       "2023-08-20 22:00:00+00:00    1815293.0\n",
       "2023-08-27 22:00:00+00:00    1929379.0\n",
       "2023-09-03 22:00:00+00:00    1955406.0\n",
       "2023-09-10 22:00:00+00:00    1901084.0\n",
       "2023-09-17 22:00:00+00:00    1879200.0\n",
       "2023-09-24 22:00:00+00:00    1876532.0\n",
       "2023-10-01 22:00:00+00:00    1861453.0\n",
       "2023-10-08 22:00:00+00:00    1799814.0\n",
       "2023-10-15 22:00:00+00:00    1751338.0\n",
       "2023-10-22 22:00:00+00:00    1761084.0\n",
       "2023-10-29 23:00:00+00:00    1809407.0\n",
       "2023-11-05 23:00:00+00:00    1819247.0\n",
       "2023-11-12 23:00:00+00:00    1775345.0\n",
       "2023-11-19 23:00:00+00:00    1766548.0\n",
       "2023-11-26 23:00:00+00:00    1707104.0\n",
       "2023-12-03 23:00:00+00:00    1590420.0\n",
       "2023-12-10 23:00:00+00:00    1545639.0\n",
       "2023-12-17 23:00:00+00:00    1507174.0\n",
       "2023-12-24 23:00:00+00:00    1543144.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = client.query_aggregate_water_reservoirs_and_hydro_storage(country_code, start=start, end=end)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00449b56-6e34-4c72-a8bc-246bc210bf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd988b-767f-48fe-af25-ef9ef9073852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c771d3e-8807-41c6-896b-4107b386af24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Week 01', '1045237', '825978', '1269659', '1412194', '1570578', '1117055', '1311570', '1655961', '1537771']\n",
      "['Week 02', '1020559', '784999', '1239217', '1372937', '1533996', '1007379', '1260206', '1597119', '1423063']\n",
      "['Week 03', '988529', '736945', '1173769', '1326312', '1426254', '952429', '1174620', '1540674', '1260750']\n",
      "['Week 04', '952521', '647447', '1056605', '1176602', '1273793', '899576', '1037006', '1375097', '1157852']\n",
      "['Week 05', '946382', '574658', '989005', '1077808', '1156977', '853281', '962906', '1269456', '1073592']\n",
      "['Week 06', '909004', '533869', '890512', '1004592', '1153654', '748222', '933635', '1184410', '1018434']\n",
      "['Week 07', '844510', '528844', '803073', '929553', '1137108', '666611', '879665', '1074334', '936302']\n",
      "['Week 08', '787908', '525197', '681020', '862072', '1086259', '611769', '885628', '1040288', '880475']\n",
      "['Week 09', '714025', '520021', '605809', '794181', '1062706', '534981', '807447', '945965', '784374']\n",
      "['Week 10', '634969', '484480', '503536', '748352', '1015282', '454595', '693482', '818682', '663585']\n",
      "['Week 11', '550396', '474694', '427773', '709164', '961767', '401931', '639839', '779143', '570068']\n",
      "['Week 12', '449762', '467394', '367426', '651259', '931444', '349156', '609961', '696653', '477319']\n",
      "['Week 13', '436803', '439512', '323029', '594518', '862319', '328343', '564987', '641040', '413065']\n",
      "['Week 14', '433903', '411399', '324036', '541928', '738395', '316830', '542635', '592454', '395407']\n",
      "['Week 15', '422118', '391455', '322445', '457413', '665637', '275505', '514814', '528185', '394559']\n",
      "['Week 16', '397331', '393654', '322607', '405422', '684287', '269440', '473013', '468331', '399773']\n",
      "['Week 17', '373351', '378402', '384622', '448854', '721438', '292186', '403187', '450518', '340269']\n",
      "['Week 18', '367185', '376944', '462357', '451052', '708383', '321166', '376285', '462364', '-']\n",
      "['Week 19', '391199', '376608', '563938', '420343', '708108', '354573', '404104', '470984', '-']\n",
      "['Week 20', '392256', '434020', '629988', '404278', '770601', '379549', '521892', '486042', '-']\n",
      "['Week 21', '423614', '516554', '675667', '425504', '844063', '411966', '741362', '566309', '-']\n",
      "['Week 22', '518826', '630154', '774086', '508035', '923587', '433519', '849573', '687991', '-']\n",
      "['Week 23', '596714', '790971', '911618', '633327', '932650', '548224', '1039793', '780684', '-']\n",
      "['Week 24', '719299', '889162', '982583', '891204', '1011564', '738921', '1125611', '876458', '-']\n",
      "['Week 25', '832085', '977080', '1072631', '1151153', '1122407', '960466', '1160127', '980689', '-']\n",
      "['Week 26', '965186', '1109408', '1146615', '1333426', '1180014', '1091048', '1272332', '1123011', '-']\n",
      "['Week 27', '1086292', '1205375', '1211450', '1456081', '1309394', '1200078', '1403125', '1236938', '-']\n",
      "['Week 28', '1189228', '1292783', '1302507', '1516347', '1457304', '1362001', '1472437', '1344774', '-']\n",
      "['Week 29', '1227339', '1363728', '1341843', '1525238', '1518133', '1527934', '1506209', '1498091', '-']\n",
      "['Week 30', '1213870', '1445422', '1381121', '1572891', '1570448', '1618035', '1583697', '1584708', '-']\n",
      "['Week 31', '1256775', '1535390', '1444227', '1695615', '1631195', '1745758', '1715317', '1693210', '-']\n",
      "['Week 32', '1380409', '1646395', '1500726', '1746547', '1728887', '1809100', '1796278', '1814757', '-']\n",
      "['Week 33', '1461695', '1697579', '1570134', '1808417', '1749681', '1813131', '1765103', '1849541', '-']\n",
      "['Week 34', '1475904', '1723390', '1590460', '1853245', '1770731', '1808115', '1807960', '1815293', '-']\n",
      "['Week 35', '1462535', '1699862', '1669709', '1870542', '1787368', '1781623', '1873203', '1929379', '-']\n",
      "['Week 36', '1479219', '1729667', '1738514', '1853674', '1889601', '1748458', '1943380', '1955406', '-']\n",
      "['Week 37', '1490842', '1735631', '1741437', '1869218', '1881744', '1725763', '1947791', '1901084', '-']\n",
      "['Week 38', '1512694', '1724841', '1758824', '1846058', '1827376', '1770991', '1979697', '1879200', '-']\n",
      "['Week 39', '1461205', '1681783', '1796341', '1850987', '1813661', '1765433', '1936840', '1876532', '-']\n",
      "['Week 40', '1417067', '1681296', '1915721', '1851101', '1817463', '1782655', '1986419', '1861453', '-']\n",
      "['Week 41', '1364113', '1704573', '1874838', '1875628', '1884485', '1765350', '1966711', '1799814', '-']\n",
      "['Week 42', '1322498', '1652668', '1838550', '1878607', '1901884', '1729692', '1963640', '1751338', '-']\n",
      "['Week 43', '1277134', '1622939', '1810607', '1870986', '1868049', '1716410', '2001005', '1761084', '-']\n",
      "['Week 44', '1272662', '1629403', '1893120', '1871050', '1875010', '1717056', '1947915', '1809407', '-']\n",
      "['Week 45', '1269272', '1597426', '1911657', '1875843', '1901278', '1659182', '1919050', '1819247', '-']\n",
      "['Week 46', '1253268', '1550100', '1855791', '1814659', '1844820', '1557858', '1880574', '1775345', '-']\n",
      "['Week 47', '1256411', '1514230', '1725074', '1763652', '1771305', '1475039', '1874133', '1766548', '-']\n",
      "['Week 48', '1198418', '1463689', '1623054', '1719248', '1646477', '1441692', '1828143', '1707104', '-']\n",
      "['Week 49', '1108707', '1404330', '1563081', '1650410', '1484928', '1402617', '1756788', '1590420', '-']\n",
      "['Week 50', '1007832', '1380317', '1492987', '1568352', '1363739', '1323892', '1677478', '1545639', '-']\n",
      "['Week 51', '885063', '1266727', '1383749', '1527181', '1244037', '1274576', '1636954', '1507174', '-']\n",
      "['Week 52', '866343', '1259083', '1391960', '1564023', '1198183', '1297334', '1678718', '1543144', '-']\n",
      "['Week 53', '', '', '', '', '1192861', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import http.client\n",
    "import csv\n",
    "\n",
    "# Set max_headers to a higher value\n",
    "http.client._MAXHEADERS = 1000\n",
    "\n",
    "# Function to download table data from a URL\n",
    "def download_table_data(url):\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Raise an error if there are too many headers\n",
    "    if len(response.headers) > 100:\n",
    "        raise Exception(\"Too many headers in the response\")\n",
    "\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table element\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extract data from the table\n",
    "    data = []\n",
    "    for row in table.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            row_data.append(cell.text.strip())\n",
    "        if row_data:  # Ensures we don't add empty rows\n",
    "            data.append(row_data)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "url = 'https://transparency.entsoe.eu/generation/r2/waterReservoirsAndHydroStoragePlants/show?name=&defaultValue=false&viewType=TABLE&areaType=CTY&atch=false&dateTime.dateTime=01.01.2016+00:00|UTC|YEAR&dateTime.endDateTime=01.01.2024+00:00|UTC|YEAR&area.values=CTY|10YAT-APG------L!CTY|10YAT-APG------L'\n",
    "table_data = download_table_data(url)\n",
    "if table_data:\n",
    "    for row in table_data:\n",
    "        print(row)\n",
    "\n",
    "    output_file_path = \"/home/ray/Desktop/Backup/Prueva/prueva1.csv\"\n",
    "    with open(output_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3a0e90-4ce6-47fb-8df5-0e54ceffc3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b76ba89-e1fa-406f-8171-af946e496d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e2a101d-b598-41e7-9556-294e919a8883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the variables\n",
    "start_year = 2016\n",
    "end_year = 2024\n",
    "data_year = 2023\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Change the name of the original CSV file\n",
    "new_file_name = f\"{start_year}_{end_year}.csv\"\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), new_file_name)\n",
    "os.rename(file_path, new_file_path)\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(new_file_path)\n",
    "\n",
    "# Extract the required columns\n",
    "columns_to_extract = ['Week', str(data_year)]  # Assuming the column with data_year as name is string type\n",
    "df_extracted = df[columns_to_extract]\n",
    "\n",
    "# Save the extracted columns to a new CSV file\n",
    "output_file_name = f\"{data_year}_1.csv\"\n",
    "output_file_path = os.path.join(os.path.dirname(file_path), output_file_name)\n",
    "df_extracted.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4a62e-5ade-4f93-be7e-4921d72cd152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f958f1e3-96f1-432a-ae3f-09b5b194dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved back to the original CSV file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the variables\n",
    "data_year = \"2023\"\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract week number from the 'Week' column\n",
    "df['Week'] = df['Week'].str.extract(r'(\\d+)').astype(int)  # Extract digits from the string and convert to int\n",
    "\n",
    "# Calculate the dates\n",
    "start_date = pd.to_datetime(f'{data_year}-01-01')  # Start from January 1st of the specified year\n",
    "df['Dispa-SET_Date'] = start_date + pd.to_timedelta((df['Week'] - 1) * 7, unit='D')  # Add the corresponding number of weeks\n",
    "\n",
    "# Convert to string in the desired format\n",
    "df['Dispa-SET_Date'] = df['Dispa-SET_Date'].dt.strftime('%Y-%m-%d 00:00:00+00:00')\n",
    "\n",
    "# Save the DataFrame back to the original CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "# Print the result\n",
    "print(\"DataFrame saved back to the original CSV file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac08ea-0d07-4d70-9e30-2d64bbfb4d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385e6a23-b2e9-4572-9780-ac8d2afd300f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of weeks in 2020 is 53.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Define the year\n",
    "year = 2020\n",
    "\n",
    "# Get the last day of the year\n",
    "last_day_of_year = datetime.date(year, 12, 31)\n",
    "\n",
    "# Get the ISO calendar week number of the last day of the year\n",
    "_, last_week_number, _ = last_day_of_year.isocalendar()\n",
    "\n",
    "print(f\"The number of weeks in {year} is {last_week_number}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b86197d-8fe4-422a-8429-c6717b6b3c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_865509/3444759729.py:21: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df.loc[len(df)] = ['Week 54'] + [None] * (len(df.columns) - 1)\n",
      "/tmp/ipykernel_865509/3444759729.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1537771' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.iloc[-1, df.columns.get_loc(col)] = next_col_first_value\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the start and end years\n",
    "start_year = 2016\n",
    "end_year = 2024\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/\"\n",
    "\n",
    "# Define the file name\n",
    "file_name = f\"{start_year}_{end_year}.csv\"\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add a new row with 'Week 54' in the first column\n",
    "df.loc[len(df)] = ['Week 54'] + [None] * (len(df.columns) - 1)\n",
    "\n",
    "# Iterate over columns (excluding the first one)\n",
    "for col in df.columns[1:]:\n",
    "    # Read the last row of the column\n",
    "    last_row_value = df.iloc[-1][col]\n",
    "    \n",
    "    # If the last row is empty\n",
    "    if pd.isna(last_row_value):\n",
    "        # Copy the value from the next column\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_first_value = df.iloc[0][next_col_name]\n",
    "            df.iloc[-1, df.columns.get_loc(col)] = next_col_first_value\n",
    "        \n",
    "        # Repeat the same process for the penultimate row\n",
    "        penultimate_row_value = df.iloc[-2][col]\n",
    "        if pd.isna(penultimate_row_value):\n",
    "            if next_col_index < len(df.columns):\n",
    "                next_col_second_value = df.iloc[1][next_col_name]\n",
    "                df.iloc[-2, df.columns.get_loc(col)] = next_col_second_value\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Process completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "089172cb-e470-4923-bbaf-b56da1ec925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_865509/2190952509.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df.loc[len(df)] = ['Week 54'] + [None] * (len(df.columns) - 1)\n",
      "/tmp/ipykernel_865509/2190952509.py:28: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1537771' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.iloc[-2, df.columns.get_loc(col)] = next_col_first_value\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory path and file name\n",
    "folder_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/\"\n",
    "file_name = f\"{start_year}_{end_year}.csv\"\n",
    "file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Add a new row with 'Week 54' in the first column\n",
    "df.loc[len(df)] = ['Week 54'] + [None] * (len(df.columns) - 1)\n",
    "\n",
    "# Iterate over the rest of the columns (from the second till the last)\n",
    "for col in df.columns[1:]:\n",
    "    # Read the penultimate and last rows of the column\n",
    "    penultimate_value = df.iloc[-2][col]\n",
    "    last_value = df.iloc[-1][col]\n",
    "\n",
    "    # If the penultimate row is empty\n",
    "    if pd.isna(penultimate_value):\n",
    "        # Copy the value from the next column to the penultimate field read\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_first_value = df.iloc[0][next_col_name]\n",
    "            df.iloc[-2, df.columns.get_loc(col)] = next_col_first_value\n",
    "\n",
    "    # Copy the value of the second field from the next column to the last field of the current column\n",
    "    if pd.isna(last_value):\n",
    "        next_col_index = df.columns.get_loc(col) + 1\n",
    "        if next_col_index < len(df.columns):\n",
    "            next_col_name = df.columns[next_col_index]\n",
    "            next_col_second_value = df.iloc[1][next_col_name]\n",
    "            df.iloc[-1, df.columns.get_loc(col)] = next_col_second_value\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Process completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6685f-aa7a-494b-85d3-669663e839d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842af0be-dc8f-4c24-b9eb-ddcb057ccb3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check if last date from file_path_1 is less than last date from file_path_2\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m last_date_file1 \u001b[38;5;241m<\u001b[39m last_date_file2:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Calculate hourly steps until reaching the last date from file_path_2\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     hourly_steps \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(start\u001b[38;5;241m=\u001b[39mlast_date_file1\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, end\u001b[38;5;241m=\u001b[39mlast_date_file2, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Create a new DataFrame with hourly steps\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     df_new_rows \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m: hourly_steps})\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the last value of the first column from file_path_1\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "last_date_file1 = df1.iloc[-1, 0]\n",
    "\n",
    "# Read the last value of the Dispa-SET_Date column from file_path_2\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "last_date_file2 = df2['Dispa-SET_Date'].iloc[-1]\n",
    "\n",
    "# Check if last date from file_path_1 is less than last date from file_path_2\n",
    "if last_date_file1 < last_date_file2:\n",
    "    # Calculate hourly steps until reaching the last date from file_path_2\n",
    "    hourly_steps = pd.date_range(start=last_date_file1, end=last_date_file2, freq='h')\n",
    "\n",
    "    # Create a new DataFrame with hourly steps\n",
    "    df_new_rows = pd.DataFrame({'Unnamed: 0': hourly_steps})\n",
    "\n",
    "    # Append the new rows to file_path_1 DataFrame\n",
    "    df1 = pd.concat([df1, df_new_rows], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the original CSV file\n",
    "    df1.to_csv(file_path_1, index=False)\n",
    "    print(\"New rows added successfully.\")\n",
    "else:\n",
    "    print(\"No new rows need to be added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d760a87c-6f58-458e-b03f-397e8984808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A  = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bcbadb7-7aac-4c42-a33d-7f3f66e3095e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5d8b95c-6d5f-4919-8bb5-9494dffcaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path (replace with your actual path)\n",
    "file_path = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5874760-2caa-40c7-ac4e-dbd68658d7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>HPHS</th>\n",
       "      <th>HDAM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 01:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 02:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 03:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 04:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8755</th>\n",
       "      <td>2023-12-31 19:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8756</th>\n",
       "      <td>2023-12-31 20:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8757</th>\n",
       "      <td>2023-12-31 21:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>2023-12-31 22:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>2023-12-31 23:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8760 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0  HPHS  HDAM\n",
       "0     2023-01-01 00:00:00+00:00   NaN   NaN\n",
       "1     2023-01-01 01:00:00+00:00   NaN   NaN\n",
       "2     2023-01-01 02:00:00+00:00   NaN   NaN\n",
       "3     2023-01-01 03:00:00+00:00   NaN   NaN\n",
       "4     2023-01-01 04:00:00+00:00   NaN   NaN\n",
       "...                         ...   ...   ...\n",
       "8755  2023-12-31 19:00:00+00:00   NaN   NaN\n",
       "8756  2023-12-31 20:00:00+00:00   NaN   NaN\n",
       "8757  2023-12-31 21:00:00+00:00   NaN   NaN\n",
       "8758  2023-12-31 22:00:00+00:00   NaN   NaN\n",
       "8759  2023-12-31 23:00:00+00:00   NaN   NaN\n",
       "\n",
       "[8760 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eeae031-f0b8-466a-a9a2-5451a93f344c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New rows added successfully to /home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\n",
      "Overall processing finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_890063/2549163235.py:18: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_steps = pd.date_range(start=last_date_file1, end=last_date_file2, freq='1H')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "\n",
    "# Read the last field of the first column from file_path_1\n",
    "df1 = pd.read_csv(file_path_1)\n",
    "last_date_file1 = pd.to_datetime(df1.iloc[-1, 0])\n",
    "\n",
    "# Read the last field of the 'Dispa-SET_Date' column from file_path_2\n",
    "df2 = pd.read_csv(file_path_2)\n",
    "last_date_file2 = pd.to_datetime(df2['Dispa-SET_Date'].iloc[-1])\n",
    "\n",
    "# Check if last date from file_path_1 is less than last date from file_path_2\n",
    "if last_date_file1 < last_date_file2:\n",
    "    # Calculate hourly steps until reaching the last date from file_path_2\n",
    "    hourly_steps = pd.date_range(start=last_date_file1, end=last_date_file2, freq='1H')\n",
    "\n",
    "    # Create a new DataFrame with hourly steps\n",
    "    df_new_rows = pd.DataFrame({'Unnamed: 0': hourly_steps})\n",
    "\n",
    "    # Append the new rows to file_path_1 DataFrame\n",
    "    df1 = pd.concat([df1, df_new_rows], ignore_index=True)\n",
    "\n",
    "    # Save the updated DataFrame back to the original CSV file\n",
    "    df1.to_csv(file_path_1, index=False)\n",
    "    print(f\"New rows added successfully to {file_path_1}\")\n",
    "else:\n",
    "    print(f\"No new rows need to be added to {file_path_1}\")\n",
    "\n",
    "print(\"Overall processing finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15561717-66c4-45aa-8e2e-7bc8e1a6e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path_1)\n",
    "\n",
    "# Remove rows with duplicated values in the first column\n",
    "df = df.drop_duplicates(subset=df.columns[0], keep='first')\n",
    "\n",
    "# Save the updated DataFrame back to the original CSV file\n",
    "df.to_csv(file_path_1, index=False)\n",
    "\n",
    "print(\"Duplicates removed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "586c0470-ff88-499f-a456-636d16a58228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths and data year\n",
    "file_path_1 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/1h/2023.csv\"\n",
    "file_path_2 = \"/home/ray/Desktop/Backup/ReservoirLevel/AT/2023_1.csv\"\n",
    "data_year = 2023\n",
    "\n",
    "# Read CSV files\n",
    "df_1 = pd.read_csv(file_path_1)\n",
    "df_2 = pd.read_csv(file_path_2)\n",
    "\n",
    "# Get the column name corresponding to the data year\n",
    "year_column = str(data_year)\n",
    "\n",
    "# Get the minimum length of both DataFrames\n",
    "min_length = min(len(df_1), len(df_2))\n",
    "\n",
    "# Iterate over the rows of both DataFrames up to the minimum length\n",
    "for i in range(min_length):\n",
    "    # Get the value from the 'Dispa-SET_Date' column in df_2\n",
    "    date_value = df_2.at[i, 'Dispa-SET_Date']\n",
    "    # Check if the value exists in the first column of df_1\n",
    "    if date_value in df_1[df_1.columns[0]].values:\n",
    "        # Get the index where the value is found in df_1\n",
    "        index_values = df_1.index[df_1[df_1.columns[0]] == date_value]\n",
    "        # Iterate over the found index values\n",
    "        for index_value in index_values:\n",
    "            # Copy corresponding value from df_2 and paste it to the corresponding fields of the second and third columns in df_1\n",
    "            df_1.at[index_value, df_1.columns[1]] = df_2.at[i, year_column]\n",
    "            df_1.at[index_value, df_1.columns[2]] = df_2.at[i, year_column]\n",
    "    else:\n",
    "        print(f\"Date value {date_value} not found in file_path_1.\")\n",
    "\n",
    "# Save the updated DataFrame back to the original CSV file\n",
    "df_1.to_csv(file_path_1, index=False)\n",
    "print(\"Values copied successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41d8b6d-fefb-4f92-84b3-821d82dd8e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
