{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4af4c44-8f75-4f4e-9cee-8a3db92d0eda",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman;\">\n",
    "        TIME SERIES DATA PROCESSING | REAL TIME AND DAY AHEAD LOAD\n",
    "</div>\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman;\">\n",
    "    Downloading / Formatting Notebook\n",
    "<div style=\"text-align: left; margin-left: 0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Each part of the following script is used to donwload the raw data for the Real Time and Day Ahead Load Time Series Raw Data for all the european countries of the Dispa-SET_Unleash project.\n",
    "<br>\n",
    "Read explanation text cells to follow and understand all the process until final results were got stept by step.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    1. Notebook Set Up\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Importing needed libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a590ae9c-56f6-4dbf-a4f4-9f39b4fa21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from entsoe import EntsoePandasClient\n",
    "from entsoe.exceptions import NoMatchingDataError  # Ensure this line is included\n",
    "from entsoe.exceptions import NoMatchingDataError, InvalidBusinessParameterError  # Import relevant exceptions\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone, utc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de47724-25dd-456b-90ed-5b0f0f73cbb5",
   "metadata": {},
   "source": [
    " <div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    2. ENTSO-E RESTful API.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Connecting with the ENTSO-E API Tool.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- To donwload the neeeded data using the API tool, is mandatory to use a token autentication to connect and make the future request of data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6899aaf-90da-463a-b5e6-6092997b2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = EntsoePandasClient(api_key='61e5bbbb-7e80-4540-a471-bd993873aa74')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2e76f-2442-4fdd-b2db-b3b71f3f544b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    3. Dispa-SET_Unleash Folder Path\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Determinning dynamically the zone_folder_path based on the location of the \"Dispa-SET_Unleash\" folder relative to the current working directory.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- If the \"Dispa-SET_Unleash\" folder is copied to a different machine or location, the dispaSET_unleash_folder_path variable will automatically adjust accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8926675d-6398-4db7-80cd-42c829b7b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory of \"Dispa-SET_Unleash\"\n",
    "dispaSET_unleash_parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Get the path to the \"Dispa-SET_Unleash\" folder\n",
    "dispaSET_unleash_folder_path = os.path.dirname(dispaSET_unleash_parent_directory)\n",
    "\n",
    "# Construct the dispaSET_unleash_folder_name variable\n",
    "dispaSET_unleash_folder_name = os.path.basename(dispaSET_unleash_folder_path)\n",
    "\n",
    "print(\"dispaSET_unleash_folder_name:\", dispaSET_unleash_folder_name)\n",
    "print(\"dispaSET_unleash_folder_path:\", dispaSET_unleash_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38b8cd-b666-4738-9813-f1fbeb216ab7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    4. Usefull Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Entering a value to all the variables which content are going to be used in some of the next stages of this script. \n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- Indicate the year of all data is referring to in the variable data_year.\n",
    "<br>\n",
    "- The universal_standar_time variable is going to be used to download all the time series data in this horary zone. Additionally as each european country belongs a particular time sector the corresponding time series data related to its time sector are going to be downloaded as well but in a different file.\n",
    "<br>\n",
    "- Additionally there are some default parameters that has to be defined to the correct working and calling to the ENTSO-E downloading functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17cff013-7efc-43b0-a572-8f32f1f9a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year to which data refers to:\n",
    "data_year = 2023\n",
    "data_year_1 = data_year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe94548-6223-4ace-9793-b857b53e2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_realtime_folder_path: /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/\n",
      "load_dayAhead_folder_path: /home/ray/Dispa-SET_Unleash/RawData/Load_DayAhead/\n",
      "load_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/\n"
     ]
    }
   ],
   "source": [
    "# Additional string to be appended\n",
    "additional_path = \"/RawData/Load_RealTime/\"\n",
    "additional_path_1 = \"/RawData/Load_DayAhead/\"\n",
    "additional_path_2 = \"/RawData/Load_RealTime/Raw_Data_Sources/\"\n",
    "\n",
    "# Construct the Outage_Factors_folder_path variable\n",
    "load_realtime_folder_path = dispaSET_unleash_folder_path + additional_path\n",
    "\n",
    "# Construct the Outage_Factors_folder_path variable\n",
    "load_dayAhead_folder_path = dispaSET_unleash_folder_path + additional_path_1\n",
    "\n",
    "# Construct the Outage_Factors_Raw_Data_folder_path variable\n",
    "load_raw_data_folder_path = dispaSET_unleash_folder_path + additional_path_2\n",
    "\n",
    "print(\"load_realtime_folder_path:\", load_realtime_folder_path)\n",
    "print(\"load_dayAhead_folder_path:\", load_dayAhead_folder_path)\n",
    "print(\"load_raw_data_folder_path:\", load_raw_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab19470a-140b-4a86-a0c9-32f845c3497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "type_marketagreement_type = 'A01'\n",
    "contract_marketagreement_type = \"A01\"\n",
    "process_type = 'A51'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610c8df-68ce-40aa-af2e-d67c3b2df7a0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    5. Country List Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Defining the list of countries according to the available data. \n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- Just those countries that interchange flows with other countries different of the ones modelled in Dispa-SET are defined in the list.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9485328c-cd02-43c9-a78d-ad5c22cff2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of country codes\n",
    "load_data_country_list = [\"AT\", \"BE\", \"BG\", \"CH\", \"CY\", \"CZ\", \"DE\", \"DK\", \"EE\", \"GR\", \"ES\", \"FI\", \"FR\", \"HR\", \"HU\", \n",
    "                          \"IE\", \"IT\", \"LT\", \"LU\", \"LV\", \"MT\", \"NL\", \"NO\", \"PL\", \"PT\", \"RO\", \"SE\", \"SI\", \"SK\", \"UK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f959e45-e5f2-4856-831d-454684b52816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/country_list.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define the directory and file path\n",
    "file_name = 'country_list.csv'\n",
    "file_path = os.path.join(load_raw_data_folder_path, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(load_raw_data_folder_path, exist_ok=True)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(load_data_country_list, columns=['Country_From'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to '{file_path}'\")\n",
    "load_data_country_list_file = file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e96e3b-c1f7-4dd1-b455-107f8f37d533",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2d3ed6b-4f59-4092-a51c-e1e71c833e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name:                              Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path:                              /home/ray/Dispa-SET_Unleash\n",
      "data_year:                                                 2023\n",
      "load_realtime_folder_path:                                 /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/\n",
      "load_dayAhead_folder_path:                                 /home/ray/Dispa-SET_Unleash/RawData/Load_DayAhead/\n",
      "load_raw_data_folder_path:                                 /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/\n",
      "load_data_country_list_file:                               /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/country_list.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"dispaSET_unleash_folder_name:                              {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path:                              {dispaSET_unleash_folder_path}\")\n",
    "print (f\"data_year:                                                 {data_year}\")\n",
    "print (f\"load_realtime_folder_path:                                 {load_realtime_folder_path}\")   \n",
    "print (f\"load_dayAhead_folder_path:                                 {load_dayAhead_folder_path}\")\n",
    "print (f\"load_raw_data_folder_path:                                 {load_realtime_folder_path}\")\n",
    "print (f\"load_data_country_list_file:                               {load_data_country_list_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88b8d7-8432-4155-a9c7-c1731043ee24",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Defining the sub-folders where all the load raw data is saved. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14f38fa4-bfc9-44fd-862e-18fecb61998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/country_list.csv' with new subfolders created.\n"
     ]
    }
   ],
   "source": [
    "# Convert data_year to string if it's not already\n",
    "data_year = str(data_year)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(load_data_country_list_file)\n",
    "\n",
    "# Ensure the column 'Country_From' exists\n",
    "if 'Country_From' not in df.columns:\n",
    "    raise ValueError(\"Column 'Country_From' does not exist in the CSV file\")\n",
    "\n",
    "# Define the base directory where subfolders will be created\n",
    "base_directory = os.path.join(load_raw_data_folder_path, data_year)\n",
    "\n",
    "# Create a list to hold the paths of the created subfolders\n",
    "country_folder_paths = []\n",
    "\n",
    "# Create subfolders and save their paths\n",
    "for country in df['Country_From']:\n",
    "    # Create the subfolder path\n",
    "    subfolder_path = os.path.join(base_directory, country)\n",
    "    \n",
    "    # Create the subfolder if it doesn't exist\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    \n",
    "    # Append the subfolder path to the list\n",
    "    country_folder_paths.append(subfolder_path)\n",
    "\n",
    "# Add the new column 'Country_Folder' to the DataFrame\n",
    "df['Country_Folder'] = country_folder_paths\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(load_data_country_list_file, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved to '{load_data_country_list_file}' with new subfolders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51abf96-5094-435e-948f-5e1ea885518f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    6. Raw Data Download\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Donwloading the real time and day ahead load raw data. \n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- The load data is downloaded in separate files for each country.\n",
    "<br>\n",
    "- Since the Acronym of Grece in the downloaded data is 'GR' and the Dispa-SET format for the country is 'EL'. All the needed changes in the used variables are done.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7a37faa-8f4e-4cf6-98af-f10f1bab227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for AT saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/AT/2023_1.csv'\n",
      "Data for BE saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/BE/2023_1.csv'\n",
      "Data for BG saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/BG/2023_1.csv'\n",
      "Data for CH saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/CH/2023_1.csv'\n",
      "No matching data or invalid parameters for CY. Error: \n",
      "Data for CZ saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/CZ/2023_1.csv'\n",
      "Data for DE saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/DE/2023_1.csv'\n",
      "Data for DK saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/DK/2023_1.csv'\n",
      "Data for EE saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/EE/2023_1.csv'\n",
      "Data for GR saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/GR/2023_1.csv'\n",
      "Data for ES saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/ES/2023_1.csv'\n",
      "Data for FI saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/FI/2023_1.csv'\n",
      "Data for FR saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/FR/2023_1.csv'\n",
      "Data for HR saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/HR/2023_1.csv'\n",
      "Data for HU saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/HU/2023_1.csv'\n",
      "Data for IE saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/IE/2023_1.csv'\n",
      "Data for IT saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/IT/2023_1.csv'\n",
      "Data for LT saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/LT/2023_1.csv'\n",
      "Data for LU saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/LU/2023_1.csv'\n",
      "Data for LV saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/LV/2023_1.csv'\n",
      "No matching data or invalid parameters for MT. Error: \n",
      "Data for NL saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/NL/2023_1.csv'\n",
      "Data for NO saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/NO/2023_1.csv'\n",
      "Data for PL saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/PL/2023_1.csv'\n",
      "Data for PT saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/PT/2023_1.csv'\n",
      "Data for RO saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/RO/2023_1.csv'\n",
      "Data for SE saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/SE/2023_1.csv'\n",
      "Data for SI saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/SI/2023_1.csv'\n",
      "Data for SK saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/SK/2023_1.csv'\n",
      "Data for UK saved to '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/UK/2023_1.csv'\n",
      "Using /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/AT/2023_1.csv for handling errors.\n",
      "Error file created for CY at '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/CY/2023_1.csv'\n",
      "Error file created for MT at '/home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/MT/2023_1.csv'\n",
      "All data processing completed.\n"
     ]
    }
   ],
   "source": [
    "# Define the start and end timestamps\n",
    "start = pd.Timestamp(f'{data_year}0101', tz='Europe/Brussels')\n",
    "end = pd.Timestamp(f'{data_year_1}0101', tz='Europe/Brussels')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(load_data_country_list_file)\n",
    "\n",
    "# Ensure the necessary columns exist\n",
    "required_columns = ['Country_From', 'Country_Folder']\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' does not exist in the CSV file\")\n",
    "\n",
    "# Add a new column for the raw data file paths\n",
    "df['Raw_Data_File_Path'] = \"\"\n",
    "\n",
    "# Lists to keep track of countries with connection errors and other errors\n",
    "retry_list = []\n",
    "error_list = []\n",
    "\n",
    "# Function to check if data is valid\n",
    "def is_valid_data(data):\n",
    "    return data is not None and not data.empty\n",
    "\n",
    "# Function to process each country\n",
    "def process_country(country_code, country_folder):\n",
    "    try:\n",
    "        # Query load and forecast data\n",
    "        load_forecast_data = client.query_load_and_forecast(country_code, start=start, end=end)\n",
    "        \n",
    "        # Check if the data is valid\n",
    "        if is_valid_data(load_forecast_data):\n",
    "            # Convert the index to a column\n",
    "            load_forecast_data = load_forecast_data.reset_index()\n",
    "            \n",
    "            # Define the output file path\n",
    "            output_file = os.path.join(country_folder, f'{data_year}_1.csv')\n",
    "            \n",
    "            # Save the DataFrame to a CSV file, including the index as a column\n",
    "            load_forecast_data.to_csv(output_file, index=False)\n",
    "\n",
    "            print(f\"Data for {country_code} saved to '{output_file}'\")\n",
    "            return output_file  # Return the path of the file\n",
    "        else:\n",
    "            print(f\"No valid data for {country_code}. Skipping.\")\n",
    "            return None\n",
    "    except ConnectionError:\n",
    "        print(f\"Connection error for {country_code}. Adding to retry list.\")\n",
    "        retry_list.append((country_code, country_folder))\n",
    "        return None\n",
    "    except (NoMatchingDataError, InvalidBusinessParameterError) as e:\n",
    "        print(f\"No matching data or invalid parameters for {country_code}. Error: {str(e)}\")\n",
    "        error_list.append((country_code, country_folder))\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred for {country_code}. Error: {str(e)}\")\n",
    "        error_list.append((country_code, country_folder))\n",
    "        return None\n",
    "\n",
    "# Process each country\n",
    "first_successful_file = None\n",
    "for index, row in df.iterrows():\n",
    "    country_code = row['Country_From']\n",
    "    country_folder = row['Country_Folder']\n",
    "    \n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(country_folder, exist_ok=True)\n",
    "    \n",
    "    # Process the country and get the path of the output file\n",
    "    output_file = process_country(country_code, country_folder)\n",
    "    \n",
    "    if output_file:\n",
    "        # Update the DataFrame with the path to the downloaded file\n",
    "        df.at[index, 'Raw_Data_File_Path'] = output_file\n",
    "        \n",
    "        # Track the first successful file for error handling\n",
    "        if not first_successful_file:\n",
    "            first_successful_file = output_file\n",
    "\n",
    "# Retry process for countries with connection errors\n",
    "if retry_list:\n",
    "    print(\"Retrying for countries with connection errors...\")\n",
    "    for country_code, country_folder in retry_list:\n",
    "        output_file = process_country(country_code, country_folder)\n",
    "        if output_file:\n",
    "            # Update the DataFrame with the path to the downloaded file\n",
    "            df.loc[df['Country_From'] == country_code, 'Raw_Data_File_Path'] = output_file\n",
    "\n",
    "# Handle countries with other errors by copying data from the first successful file\n",
    "if first_successful_file:\n",
    "    print(f\"Using {first_successful_file} for handling errors.\")\n",
    "    successful_data = pd.read_csv(first_successful_file)\n",
    "    first_column = successful_data.iloc[:, 0]  # First column data\n",
    "    second_and_third_headers = successful_data.columns[1:3].tolist()  # Second and third column headers\n",
    "    \n",
    "    for country_code, country_folder in error_list:\n",
    "        error_output_file = os.path.join(country_folder, f'{data_year}_1.csv')\n",
    "        # Create a DataFrame with the first column\n",
    "        error_data = pd.DataFrame({successful_data.columns[0]: first_column})\n",
    "        # Add dummy columns for the second and third headers\n",
    "        for header in second_and_third_headers:\n",
    "            error_data[header] = pd.Series(dtype='float64')\n",
    "        # Save the DataFrame to a CSV file\n",
    "        error_data.to_csv(error_output_file, index=False)\n",
    "        print(f\"Error file created for {country_code} at '{error_output_file}'\")\n",
    "        # Update the DataFrame with the path to the error file\n",
    "        df.loc[df['Country_From'] == country_code, 'Raw_Data_File_Path'] = error_output_file\n",
    "\n",
    "# Save the updated DataFrame with the file paths\n",
    "df.to_csv(load_data_country_list_file, index=False)\n",
    "\n",
    "print(\"All data processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ff7d655-1f47-4729-ac12-9550c08e5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements made and file saved: /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/country_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1147198/2568165819.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace('GR', 'EL') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(load_data_country_list_file)\n",
    "\n",
    "# Replace 'GR' with 'EL' in the entire DataFrame\n",
    "df = df.applymap(lambda x: x.replace('GR', 'EL') if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(load_data_country_list_file, index=False)\n",
    "\n",
    "print(f\"Replacements made and file saved: {load_data_country_list_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f06c431d-6d5f-464e-914b-5a4116440fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed directory: /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/GR to /home/ray/Dispa-SET_Unleash/RawData/Load_RealTime/Raw_Data_Sources/2023/EL\n"
     ]
    }
   ],
   "source": [
    "# Walk through the directory tree\n",
    "for root, dirs, files in os.walk(load_raw_data_folder_path, topdown=False):  \n",
    "    # Rename files\n",
    "    for name in files:\n",
    "        if 'GR' in name:\n",
    "            new_name = name.replace('GR', 'EL')\n",
    "            old_file_path = os.path.join(root, name)\n",
    "            new_file_path = os.path.join(root, new_name)\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed file: {old_file_path} to {new_file_path}\")\n",
    "\n",
    "    # Rename directories\n",
    "    for name in dirs:\n",
    "        if 'GR' in name:\n",
    "            new_name = name.replace('GR', 'EL')\n",
    "            old_dir_path = os.path.join(root, name)\n",
    "            new_dir_path = os.path.join(root, new_name)\n",
    "            os.rename(old_dir_path, new_dir_path)\n",
    "            print(f\"Renamed directory: {old_dir_path} to {new_dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3884da9-281b-4848-ae8c-1b1755b1ec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated load_data_country_list: ['AT', 'BE', 'BG', 'CH', 'CY', 'CZ', 'DE', 'DK', 'EE', 'EL', 'ES', 'FI', 'FR', 'HR', 'HU', 'IE', 'IT', 'LT', 'LU', 'LV', 'MT', 'NL', 'NO', 'PL', 'PT', 'RO', 'SE', 'SI', 'SK', 'UK']\n"
     ]
    }
   ],
   "source": [
    "# Function to replace 'GR' with 'EL'\n",
    "def replace_gr_with_el(lst):\n",
    "    return ['EL' if x == 'GR' else x for x in lst]\n",
    "\n",
    "# Applying the function to both lists\n",
    "load_data_country_list = replace_gr_with_el(load_data_country_list)\n",
    "\n",
    "# Print the updated lists\n",
    "print(\"Updated load_data_country_list:\", load_data_country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d762b-23f9-4ab8-a9d9-b432c56115a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79735f3f-d695-4d59-870c-7b4e6d5a034b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5ce13-5bba-4125-a931-42fbfe689fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df23be0-9c27-4ecc-8364-4c4650ad5569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b2da2-7919-449d-b6e5-bb383cc8d9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76015ee6-b0ed-41e7-8930-9d3826c8ac82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c680d5-512f-49cf-9876-27ccce98129a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459d66d-3079-42e2-95ba-6d90d1d9ff2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8835c832-e4be-4216-a857-c826dfce40d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/AL.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/BA.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/BG.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/BY.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/EE.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL/EL.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/FI.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/HR.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/HU.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/IT.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/LT.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/LV.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/MK.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/MT.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/PL.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RO.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/RU.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RS.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/SK.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/TR.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/UA.csv'\n",
      "All data has been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the column 'Country_Folder' exists\n",
    "if 'Country_Folder' not in df.columns:\n",
    "    raise ValueError(\"Column 'Country_Folder' does not exist in the CSV file\")\n",
    "\n",
    "# Function to join CSV files in a directory\n",
    "def join_csv_files_in_directory(directory_path):\n",
    "    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    # Read all CSV files into DataFrames\n",
    "    dataframes = {csv_file: pd.read_csv(os.path.join(directory_path, csv_file)) for csv_file in csv_files}\n",
    "    \n",
    "    # Find the CSV file with the largest number of rows\n",
    "    largest_file = max(dataframes, key=lambda x: len(dataframes[x]))\n",
    "    base_df = dataframes[largest_file].iloc[:, :2].copy()\n",
    "    base_df.columns = [base_df.columns[0], largest_file.replace('.csv', '')]\n",
    "    \n",
    "    # Merge the other CSV files based on the first column\n",
    "    for csv_file, df in dataframes.items():\n",
    "        if csv_file == largest_file:\n",
    "            continue\n",
    "        temp_df = df.iloc[:, [0, 1]]\n",
    "        temp_df.columns = [temp_df.columns[0], csv_file.replace('.csv', '')]\n",
    "        base_df = pd.merge(base_df, temp_df, on=base_df.columns[0], how='left')\n",
    "    \n",
    "    return base_df\n",
    "\n",
    "# Create a new column for the paths of the new CSV files\n",
    "df['Country_File_Path'] = ''\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    country_folder = row['Country_Folder']\n",
    "    \n",
    "    # Join CSV files in the directory\n",
    "    joined_df = join_csv_files_in_directory(country_folder)\n",
    "    \n",
    "    if joined_df is not None:\n",
    "        # Define the output file path\n",
    "        output_file = os.path.join(country_folder, f\"{os.path.basename(country_folder)}.csv\")\n",
    "        \n",
    "        # Save the joined DataFrame to a new CSV file\n",
    "        joined_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Update the DataFrame with the path of the new CSV file\n",
    "        df.at[index, 'Country_File_Path'] = output_file\n",
    "\n",
    "        print(f\"Joined CSV file saved to '{output_file}'\")\n",
    "\n",
    "# Save the updated DataFrame back to the main CSV file\n",
    "df.to_csv(cross_border_flows_country_list_file, index=False)\n",
    "\n",
    "print(\"All data has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d48b0461-7364-4b74-93f3-0a396a13b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/AL.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/BA.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/BG.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/BY.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/EE.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL/EL.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/FI.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/HR.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/HU.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/IT.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/LT.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/LV.csv'\n",
      "File path 'nan' does not exist or is empty. Skipping...\n",
      "File path 'nan' does not exist or is empty. Skipping...\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/MK.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/MT.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/PL.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RO.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/RU.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RS.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/SK.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/TR.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/UA.csv'\n",
      "All CSV files have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Read the main CSV file into a DataFrame\n",
    "df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the required columns exist\n",
    "if 'Country_From' not in df.columns or 'Country_File_Path' not in df.columns:\n",
    "    raise ValueError(\"The CSV file must contain 'Country_From' and 'Country_File_Path' columns.\")\n",
    "\n",
    "# Function to update the headers of a CSV file\n",
    "def update_csv_headers(file_path, new_header_prefix):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    csv_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get the current headers\n",
    "    current_headers = csv_df.columns.tolist()\n",
    "    \n",
    "    # Create new headers for columns from the second column onward\n",
    "    new_headers = [current_headers[0]] + [f\"{new_header_prefix} -> {col}\" for col in current_headers[1:]]\n",
    "    \n",
    "    # Update the DataFrame with the new headers\n",
    "    csv_df.columns = new_headers\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    csv_df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated headers in '{file_path}'\")\n",
    "\n",
    "# Iterate through each row in the main DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    country_from = row['Country_From']\n",
    "    country_file_path = row['Country_File_Path']\n",
    "    \n",
    "    # Check if the file path is not empty and exists\n",
    "    if pd.notna(country_file_path) and os.path.exists(country_file_path):\n",
    "        update_csv_headers(country_file_path, country_from)\n",
    "    else:\n",
    "        print(f\"File path '{country_file_path}' does not exist or is empty. Skipping...\")\n",
    "\n",
    "print(\"All CSV files have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80667de7-f250-40b5-a1ab-ed3f2d57628a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    7. Raw Data Format\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Addapting the time step data to the UTC for all the countries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7963855-5bd9-4488-b5f5-40de4a3b4182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/AL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/BA.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/BG.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/BY.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/EE.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL/EL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/FI.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/HR.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/HU.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/IT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/LT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/LV.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/MK.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/MT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/PL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RO.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/RU.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RS.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/SK.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/TR.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/UA.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Define the function to convert time to UTC\n",
    "def convert_to_utc(time_str):\n",
    "    local_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "    utc_time = local_time.astimezone(pytz.utc)\n",
    "    return utc_time.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# Process each CSV file\n",
    "for file_path in country_list_df['Country_File_Path'].dropna():\n",
    "    # Ensure the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the 'index' column exists\n",
    "    if 'index' not in df.columns:\n",
    "        print(f\"'index' column not found in file: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert the 'index' column to UTC\n",
    "    df['index'] = df['index'].apply(convert_to_utc)\n",
    "    \n",
    "    # Save the updated CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d670d7e-6222-4fb1-baf0-eb038853da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/AL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/BA.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/BG.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/BY.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/EE.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL/EL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/FI.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/HR.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/HU.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/IT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/LT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/LV.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/MK.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/MT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/PL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RO.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/RU.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RS.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/SK.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/TR.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/UA.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Function to update the year in the 'index' column\n",
    "def update_index_year(df, data_year):\n",
    "    # Ensure the 'index' column exists\n",
    "    if 'index' not in df.columns:\n",
    "        raise ValueError(\"'index' column not found in DataFrame\")\n",
    "    \n",
    "    # Update the year in the 'index' column\n",
    "    df['index'] = df['index'].apply(lambda x: f\"{data_year}{x[4:]}\" if str(x)[:4] != str(data_year) else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each CSV file specified in the 'Country_File_Path' column\n",
    "for file_path in country_list_df['Country_File_Path'].dropna():\n",
    "    # Ensure the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure there are enough rows to move the first four rows to the last\n",
    "    if len(df) < 4:\n",
    "        print(f\"Not enough rows to process in file: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract the first four rows (excluding headers)\n",
    "    first_four_rows = df.iloc[:4].copy()\n",
    "    \n",
    "    # Drop the first four rows from the DataFrame\n",
    "    df = df.iloc[4:].reset_index(drop=True)\n",
    "    \n",
    "    # Append the first_four_rows to the end of the DataFrame\n",
    "    df = pd.concat([df, first_four_rows]).reset_index(drop=True)\n",
    "    \n",
    "    # Update the 'index' column year\n",
    "    df = update_index_year(df, data_year)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12470b-5500-4767-860b-76dff3017dc1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    7. Cross Border Flows Clean File\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Joining all the cros border flows data to a single csv file with named as the analized year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db2fd6f2-ef2c-4b13-9031-7a147b12f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Process each CSV file specified in the 'Country_File_Path' column\n",
    "file_paths = country_list_df['Country_File_Path'].dropna().tolist()\n",
    "\n",
    "# Identify the CSV file with the largest number of rows\n",
    "max_rows = 0\n",
    "base_df = None\n",
    "for file_path in file_paths:\n",
    "    # Ensure the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > max_rows:\n",
    "            max_rows = len(df)\n",
    "            base_df = df.copy()\n",
    "\n",
    "# If no base_df was found, raise an error\n",
    "if base_df is None:\n",
    "    raise ValueError(\"No valid CSV files found.\")\n",
    "\n",
    "# Initialize the combined DataFrame with the first column from the base DataFrame\n",
    "combined_df = pd.DataFrame(base_df.iloc[:, 0])\n",
    "combined_df.columns = [base_df.columns[0]]  # Keep the original name of the first column\n",
    "\n",
    "# Add data from each CSV file to the combined DataFrame\n",
    "for file_path in file_paths:\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Merge the data based on the first column\n",
    "        combined_df = pd.merge(combined_df, df, on=base_df.columns[0], how='left')\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file named after the data_year variable\n",
    "output_file_path = os.path.join(cross_border_flows_folder_path, f\"{data_year}.csv\")\n",
    "combined_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Combined CSV file saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6caaf-602e-40a1-b46a-ae1620b6d9fc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Dividing the clean data in time stepts of 15 minutes, 30 minutes, and 1 hour.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff2aa826-0a03-4dd0-a746-53771e73cf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/1h/2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1100939/2621242742.py:17: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  resampled_df = df.set_index('index').resample(interval).first().reset_index()\n",
      "/tmp/ipykernel_1100939/2621242742.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  resampled_df = df.set_index('index').resample(interval).first().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/30min/2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1100939/2621242742.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  resampled_df = df.set_index('index').resample(interval).first().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/15min/2017.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = os.path.join(cross_border_flows_folder_path, f'{data_year}.csv')\n",
    "\n",
    "# Create the new directories\n",
    "intervals = ['1h', '30min', '15min']\n",
    "for interval in intervals:\n",
    "    os.makedirs(os.path.join(cross_border_flows_folder_path, interval), exist_ok=True)\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Convert the 'index' column to datetime\n",
    "df['index'] = pd.to_datetime(df['index'], format='%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# Function to extract rows at a specific time step and save to a new CSV file\n",
    "def extract_and_save(df, interval, folder_name):\n",
    "    # Resample the DataFrame\n",
    "    resampled_df = df.set_index('index').resample(interval).first().reset_index()\n",
    "    \n",
    "    # Define the new file path\n",
    "    new_file_path = os.path.join(cross_border_flows_folder_path, folder_name, f'{data_year}.csv')\n",
    "    \n",
    "    # Save the resampled DataFrame to the new CSV file\n",
    "    resampled_df.to_csv(new_file_path, index=False)\n",
    "    print(f\"File saved: {new_file_path}\")\n",
    "\n",
    "# Extract and save rows at different time steps\n",
    "extract_and_save(df, '1H', '1h')\n",
    "extract_and_save(df, '30T', '30min')\n",
    "extract_and_save(df, '15T', '15min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635513e-ee86-4c66-8df3-65c1e37a7797",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Copying the time already formated Cross Border Flows data to the main Dispa-SET data base dirtectory\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "118d9e2f-718d-4bbd-ae71-6acf9243de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_path_2 = \"/Database/CrossBorderFlows/\"\n",
    "\n",
    "# Construct the power_plants_raw_data_folder_path variable\n",
    "cross_border_flows_data_base_folder_path = dispaSET_unleash_folder_path + additional_path_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "425eda04-4885-4185-845c-c8ce803aa211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/1h/2017.csv to /home/ray/Dispa-SET_Unleash/Database/CrossBorderFlows/1h/2017.csv\n",
      "Copied /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/30min/2017.csv to /home/ray/Dispa-SET_Unleash/Database/CrossBorderFlows/30min/2017.csv\n",
      "Copied /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/15min/2017.csv to /home/ray/Dispa-SET_Unleash/Database/CrossBorderFlows/15min/2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the subfolder names\n",
    "subfolders = ['1h', '30min', '15min']\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(data_year, source_base_path, dest_base_path, subfolders):\n",
    "    for subfolder in subfolders:\n",
    "        source_path = os.path.join(source_base_path, subfolder, f\"{data_year}.csv\")\n",
    "        dest_folder_path = os.path.join(dest_base_path, subfolder)\n",
    "\n",
    "        # Create the destination subfolder if it does not exist\n",
    "        os.makedirs(dest_folder_path, exist_ok=True)\n",
    "\n",
    "        dest_path = os.path.join(dest_folder_path, f\"{data_year}.csv\")\n",
    "        \n",
    "        # Copy the file\n",
    "        if os.path.isfile(source_path):\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            print(f\"Copied {source_path} to {dest_path}\")\n",
    "        else:\n",
    "            print(f\"File {source_path} does not exist\")\n",
    "\n",
    "# Call the function\n",
    "copy_files(data_year, cross_border_flows_folder_path, cross_border_flows_data_base_folder_path, subfolders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
