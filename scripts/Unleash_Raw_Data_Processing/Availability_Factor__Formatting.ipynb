{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246cc501-e52d-4a23-bfca-a9b4ed11d37d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman; color: skyblue\">\n",
    "TIME SERIES DATA PROCESSING\n",
    "<br>\n",
    "AVAILABILITY FACTOR | SCALED OUTFLOWS\n",
    "</div>\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Main Formatting Notebook\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Each part of the following script was used to proccess the raw data for the Availability Factor Time Series Raw Data for all the european countries of the Dispa-SET_Unleash project.\n",
    "<br>\n",
    "Read explanation text cells to follow and understand all the process until final results were got stept by step.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    1. Notebook Set Up\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    Importing needed libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb71f838-06e5-4102-a2b3-8ea0a349c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import http.client\n",
    "from multiprocessing import Pool\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24f933-f4b0-4965-8ccd-c6cc7fa953ec",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    2. Dispa-SET_Unleash Folder Path\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    Determinning dynamically the zone_folder_path based on the location of the \"Dispa-SET_Unleash\" folder relative to the current working directory.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "If the \"Dispa-SET_Unleash\" folder is copied to a different machine or location, the dispaSET_unleash_folder_path variable will automatically adjust accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c9fd200-8d82-4a17-bb81-43fe5e092298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory of \"Dispa-SET_Unleash\"\n",
    "dispaSET_unleash_parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Get the path to the \"Dispa-SET_Unleash\" folder\n",
    "dispaSET_unleash_folder_path = os.path.dirname(dispaSET_unleash_parent_directory)\n",
    "\n",
    "# Construct the dispaSET_unleash_folder_name variable\n",
    "dispaSET_unleash_folder_name = os.path.basename(dispaSET_unleash_folder_path)\n",
    "\n",
    "print(\"dispaSET_unleash_folder_name:\", dispaSET_unleash_folder_name)\n",
    "print(\"dispaSET_unleash_folder_path:\", dispaSET_unleash_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14876099-b52c-4401-aae6-ed4eb77eafcf",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    3. Usefull Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Entering a value to all the variables which content are going to be used in some of the next stages of this script. \n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Indicate the year of all data is referring to in the variable data_year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ad3eeb7-bb90-46b7-8cc7-633f2425d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year to which data refers to:\n",
    "data_year = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3410eec-c0b7-4702-9f57-b9ab23c95a96",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "The four standar_time variables (UTC, WET/WEST, CET/CEST and EET/EST) can be defined in order to use any of the raw data since it is available for each of standard times.\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "To set this just uncomment the corresponding line leaving the others commented.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5daf69c-1298-4cad-8c69-05f37279f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Universal standad time:\n",
    "standard_time = 'UTC'\n",
    "\n",
    "# Western European Time:\n",
    "#standard_time = 'WET_WEST'\n",
    "\n",
    "# Central European Time:\n",
    "#standard_time = 'CET_CEST'\n",
    "\n",
    "# Eastern European Time:\n",
    "#standard_time = 'EET_EST'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0252978e-8b7c-4fc2-b7dd-b6bb27b1afed",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Defining the countries and its acronyms modeled in Dispa-SET\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0790eb47-7796-44de-9e13-7005259a405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists of countries and standard times\n",
    "countries = [\n",
    "    \"Austria\", \"Belgium\", \"Bulgaria\", \"Switzerland\", \"Cyprus\", \"Czech Republic\",\n",
    "    \"Germany\", \"Denmark\", \"Estonia\", \"Greece\", \"Spain\", \"Finland\", \"France\",\n",
    "    \"Croatia\", \"Hungary\", \"Ireland\", \"Italy\", \"Lithuania\", \"Luxembourg\", \"Latvia\",\n",
    "    \"Malta\", \"Netherlands\", \"Norway\", \"Poland\", \"Portugal\", \"Romania\", \"Sweden\",\n",
    "    \"Slovenia\", \"Slovakia\", \"United Kingdom\"\n",
    "]\n",
    "\n",
    "dispaSET_codes = [\"AT\", \"BE\", \"BG\", \"CH\", \"CY\", \"CZ\", \"DE\", \"DK\", \"EE\", \"EL\", \"ES\", \"FI\", \"FR\", \"HR\", \"HU\", \n",
    "                  \"IE\", \"IT\", \"LT\", \"LU\", \"LV\", \"MT\", \"NL\", \"NO\", \"PL\", \"PT\", \"RO\", \"SE\", \"SI\", \"SK\", \"UK\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3db5f-01a4-4065-b5d7-2664113877bc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "4. Availability Factor and Scaled Outflow and Raw Data Directories Definition\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Defining the path to the folders that are going to content all the data realted to the availability factor and scaled outflow time series.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Futher the raw data is going to be used to get both time series, availability factor and scaled inflow. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08da4c07-ae5c-4a7f-9a43-90a8f7b89422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard_time_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/\n",
      "availability_factors_folder_path: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/\n",
      "scaled_outflows_folder_path: /home/ray/Dispa-SET_Unleash/RawData/HydroData/ScaledInflows/ScaledOutflows\n",
      "raw_rata_source_folder_path: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/\n"
     ]
    }
   ],
   "source": [
    "# Additional string to be appended\n",
    "additional_path = \"/RawData/AvailabilityFactors/\"\n",
    "additional_path_1 = \"/RawData/HydroData/ScaledInflows/ScaledOutflows\"\n",
    "additional_path_2 = \"Time_Series_Raw_Data_Source/\"\n",
    "#additional_path_3 = os.path.join(additional_path_2, standard_time)\n",
    "\n",
    "# Construct the standard_time_data_folder_path variable\n",
    "standard_time_data_folder_path = dispaSET_unleash_folder_path + additional_path\n",
    "\n",
    "# Construct the Availability_Factor_folder_path variable\n",
    "availability_factors_folder_path = dispaSET_unleash_folder_path + additional_path\n",
    "\n",
    "# Construct the Scaled_OutFlows_folder_path variable\n",
    "scaled_outflows_folder_path = dispaSET_unleash_folder_path + additional_path_1\n",
    "\n",
    "# Construct the Raw Data Source variable\n",
    "raw_rata_source_folder_path = availability_factors_folder_path + additional_path_2\n",
    "\n",
    "print(\"standard_time_data_folder_path:\", standard_time_data_folder_path)\n",
    "print(\"availability_factors_folder_path:\", availability_factors_folder_path)\n",
    "print(\"scaled_outflows_folder_path:\", scaled_outflows_folder_path)\n",
    "print(\"raw_rata_source_folder_path:\", raw_rata_source_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53432b2-0e56-40e5-bd89-0a9cebbea75f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    4.1. Back Up Directory\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    Saving the original files into a Back up folder.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Since in the next steps of the processing data new subfolders and files are going to be created, the original ones are saved in a back up foldet to return them as its default content ones the process will be finished.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61af5f27-b145-471e-8873-f3977f9769bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup_folder_path: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors_backup/\n",
      "Backup created at /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors_backup/\n"
     ]
    }
   ],
   "source": [
    "# Define the paths\n",
    "additional_path_6 = '/RawData/AvailabilityFactors_backup/'\n",
    "\n",
    "# Construct the backup_folder_path variable\n",
    "backup_folder_path = dispaSET_unleash_folder_path + additional_path_6\n",
    "\n",
    "print(\"backup_folder_path:\", backup_folder_path)\n",
    "\n",
    "# Create a backup of the directory\n",
    "if os.path.exists(backup_folder_path):\n",
    "    shutil.rmtree(backup_folder_path)  # Remove any existing backup if necessary\n",
    "\n",
    "shutil.copytree(availability_factors_folder_path, backup_folder_path)\n",
    "\n",
    "print(f\"Backup created at {backup_folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e16ba9-cd83-4cd6-84b1-b20508069e59",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    5. Master File\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Defining a csv file where all the needed paths and auxiliary information in the next processing steps are going to be saved.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "The name of the csv file is defined by the variable 'standard_time_data_file_name' and its path is defined by the varibale 'standard_time_data_file_path'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0ad456d-2e41-4170-8833-fe1e915aeb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Standard_Time_Data_Source.csv\n",
      "/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/\n"
     ]
    }
   ],
   "source": [
    "standard_time_data_file_name = 'Standard_Time_Data_Source.csv'\n",
    "\n",
    "standard_time_data_file_path = os.path.join(standard_time_data_folder_path, standard_time_data_file_name)\n",
    "\n",
    "# Create the CSV file if it doesn't exist\n",
    "if not os.path.exists(standard_time_data_file_path):\n",
    "    open(standard_time_data_file_path, 'w').close()\n",
    "\n",
    "print(standard_time_data_file_path)\n",
    "\n",
    "def extract_folder_path(file_path):\n",
    "  \"\"\"Extracts the folder path from a file path.\n",
    "\n",
    "  Args:\n",
    "    file_path: The full path to the file.\n",
    "\n",
    "  Returns:\n",
    "    The folder path as a string.\n",
    "  \"\"\"\n",
    "\n",
    "  return os.path.dirname(file_path)\n",
    "\n",
    "print(standard_time_data_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9300ff-68ee-46ea-8a76-b94d7d95cfc0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "980baf3b-68b1-4d32-939c-d3e5b29cacc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name:                              Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path:                              /home/ray/Dispa-SET_Unleash\n",
      "data_year:                                                 2024\n",
      "standard_time:                                             UTC\n",
      "standard_time_data_folder_path:                            /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/\n",
      "availability_factors_folder_path:                          /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/\n",
      "scaled_outflows_folder_path:                               /home/ray/Dispa-SET_Unleash/RawData/HydroData/ScaledInflows/ScaledOutflows\n",
      "standard_time_data_file_path:                              /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Standard_Time_Data_Source.csv\n",
      "standard_time_data_file_name:                              Standard_Time_Data_Source.csv\n",
      "raw_rata_source_folder_path:                               /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/\n"
     ]
    }
   ],
   "source": [
    "print (f\"dispaSET_unleash_folder_name:                              {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path:                              {dispaSET_unleash_folder_path}\")\n",
    "print (f\"data_year:                                                 {data_year}\")\n",
    "print (f\"standard_time:                                             {standard_time}\")\n",
    "print (f\"standard_time_data_folder_path:                            {standard_time_data_folder_path}\")\n",
    "print (f\"availability_factors_folder_path:                          {availability_factors_folder_path}\")\n",
    "print (f\"scaled_outflows_folder_path:                               {scaled_outflows_folder_path}\")\n",
    "print (f\"standard_time_data_file_path:                              {standard_time_data_file_path}\")\n",
    "print (f\"standard_time_data_file_name:                              {standard_time_data_file_name}\")\n",
    "print (f\"raw_rata_source_folder_path:                               {raw_rata_source_folder_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbaddd76-e00b-49bb-8565-d673e678e649",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Writting the 'Country', 'Dispa-SET_Code', 'Standard_Time' and 'Data_Year' columns to the 'standard_time_data_file_path'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b70bc8-59f6-4802-b5e5-ddb80df262bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Standard_Time_Data_Source.csv with 30 rows.\n"
     ]
    }
   ],
   "source": [
    "def create_csv_file(standard_time_data_file_path, countries, dispaSET_codes, standard_time, data_year):\n",
    "    \"\"\"Creates a CSV file with columns for Country, Dispa-SET_Code, Standard_Time, and Data_Year.\n",
    "\n",
    "    Args:\n",
    "        standard_time_data_file_path: The path to the CSV file to be created.\n",
    "        countries: A list of country names.\n",
    "        dispaSET_codes: A list of Dispa-SET codes.\n",
    "        standard_time: The standard time value.\n",
    "        data_year: The year to be included in the new column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure lists have the same length by truncating or padding with None\n",
    "    min_length = min(len(countries), len(dispaSET_codes))\n",
    "    countries = countries[:min_length]\n",
    "    dispaSET_codes = dispaSET_codes[:min_length]\n",
    "\n",
    "    data = {\n",
    "        'Country': countries,\n",
    "        'Dispa-SET_Code': dispaSET_codes,\n",
    "        'Standard_Time': [standard_time] * min_length,\n",
    "        'Data_Year': [data_year] * min_length\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(standard_time_data_file_path, index=False)\n",
    "\n",
    "    # Print a message indicating the file has been created\n",
    "    print(f\"CSV file created at {standard_time_data_file_path} with {min_length} rows.\")\n",
    "\n",
    "# Example usage\n",
    "# standard_time_data_file_path = '/path/to/standard_time_data.csv'\n",
    "# countries = ['Country1', 'Country2', 'Country3']\n",
    "# dispaSET_codes = ['Code1', 'Code2', 'Code3']\n",
    "# standard_time = '2023-01-01T00:00:00Z'\n",
    "# data_year = '2023'\n",
    "create_csv_file(standard_time_data_file_path, countries, dispaSET_codes, standard_time, data_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2740f8-5d19-4b57-8b18-bd543a391771",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Writting the 'Time_Series_Raw_Data_File_Path' column to the 'standard_time_data_file_path'.\n",
    "<br>\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "This columns specify the path to the corresponding raw data csv file sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "919afe0b-892a-41dd-a537-77e4d9937917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Standard_Time_Data_Source.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Create the new column with the required file paths\n",
    "df['Time_Series_Raw_Data_File_Path'] = df.apply(\n",
    "    lambda row: f\"{raw_rata_source_folder_path}{standard_time}/{row['Dispa-SET_Code']}/{data_year}_1.csv\", axis=1)\n",
    "\n",
    "# Save the updated DataFrame back to a CSV file\n",
    "df.to_csv(standard_time_data_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved to {standard_time_data_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df48da-ac8a-4ce1-8fb5-cb9e3fba7b9a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Extracting the corresponding raw data csv file sources to be processed.\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Updating the 'Time_Series_Raw_Data_File_Path' of the 'standard_time_data_file_path' with the extracted raw data csv file sources paths.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f2544d1-2525-4615-881d-43235863a284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/AT/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/AT/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/BE/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BE/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/BG/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BG/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/CH/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CH/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/CY/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CY/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/CZ/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CZ/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/DE/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/DE/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/DK/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/DK/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/EE/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/EE/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/EL/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/EL/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/ES/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/ES/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/FI/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/FI/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/FR/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/FR/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/HR/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/HR/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/HU/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/HU/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/IE/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/IE/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/IT/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/IT/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/LT/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LT/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/LU/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LU/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/LV/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LV/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/MT/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/MT/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/NL/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/NL/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/NO/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/NO/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/PL/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/PL/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/PT/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/PT/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/RO/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/RO/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/SE/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SE/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/SI/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SI/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/SK/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SK/2024_1.csv\n",
      "Copying file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/Time_Series_Raw_Data_Source/UTC/UK/2024_1.csv to /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/UK/2024_1.csv\n"
     ]
    }
   ],
   "source": [
    "def copy_and_organize_files(standard_time_data_file_path, standard_time_data_folder_path):\n",
    "  \"\"\"Copies and organizes CSV files based on information in a CSV file.\n",
    "\n",
    "  Args:\n",
    "    standard_time_data_file_path: Path to the CSV file containing file paths and codes.\n",
    "    standard_time_data_folder_path: Path to the destination folder.\n",
    "  \"\"\"\n",
    "\n",
    "  df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "    source_file_path = row['Time_Series_Raw_Data_File_Path']\n",
    "    dispa_set_code = row['Dispa-SET_Code']\n",
    "    destination_folder = os.path.join(standard_time_data_folder_path, dispa_set_code)\n",
    "\n",
    "    if not os.path.exists(destination_folder):\n",
    "      os.makedirs(destination_folder)\n",
    "\n",
    "    destination_file_path = os.path.join(destination_folder, os.path.basename(source_file_path))\n",
    "\n",
    "    print(f\"Copying file: {source_file_path} to {destination_file_path}\")\n",
    "    shutil.copy2(source_file_path, destination_file_path)\n",
    "\n",
    "    df.at[index, 'Time_Series_Raw_Data_File_Path'] = destination_file_path\n",
    "\n",
    "  df.to_csv(standard_time_data_file_path, index=False)\n",
    "\n",
    "copy_and_organize_files(standard_time_data_file_path, standard_time_data_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4185e3-c92d-4df6-a8b9-0f200ec56c76",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Writing to the 'standard_time_data_file_path' the path to the country folders that content all the data realted to the availability factor and scaled inflow time series in order to use them for next processing stages.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "452a2722-6f61-43ff-90c4-4aa6d2586905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zone_Folder_Path column added successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file containing the paths\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Create a new column 'Zone_Folder_Path'\n",
    "df['Zone_Folder_Path'] = df['Time_Series_Raw_Data_File_Path'].apply(lambda x: os.path.dirname(x))\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(standard_time_data_file_path, index=False)\n",
    "\n",
    "print(\"Zone_Folder_Path column added successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dade082-bece-4e5d-af91-e180a84d9d35",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    6. Raw Data Source File Processing\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Adding column names.\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "All the raw data were downloaded without columns identification, the corresponding header to each column is added.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b015927a-be79-46ff-8743-48e42ad7fa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/AT/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BE/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BG/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CH/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CY/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CZ/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/DE/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/DK/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/EE/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/EL/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/ES/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/FI/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/FR/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/HR/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/HU/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/IE/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/IT/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LT/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LU/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LV/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/MT/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/NL/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/NO/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/PL/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/PT/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/RO/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SE/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SI/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SK/2024_1.csv'.\n",
      "Empty row added at the beginning of file '/home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/UK/2024_1.csv'.\n",
      "Empty rows added and headers copied successfully to the first empty row of all files.\n"
     ]
    }
   ],
   "source": [
    "# Define the headers\n",
    "headers = [\n",
    "    \"MTU\", \"Biomass_Actual_Aggregated\", \"Biomass_Actual_Consumption\",\n",
    "    \"Fossil_Brown_coal-Lignite_Actual_Aggregated\", \"Fossil_Brown_coal-Lignite_Actual_Consumption\",\n",
    "    \"Fossil_Coal-derived_gas_Actual_Aggregated\", \"Fossil_Coal-derived_gas_Actual_Consumption\",\n",
    "    \"Fossil_Gas_Actual_Aggregated\", \"Fossil_Gas_Actual_Consumption\",\n",
    "    \"Fossil_Hard_coal_Actual_Aggregated\", \"Fossil_Hard_coal_Actual_Consumption\",\n",
    "    \"Fossil_Oil_Actual_Aggregated\", \"Fossil_Oil_Actual_Consumption\",\n",
    "    \"Fossil_Oil_shale_Actual_Aggregated\", \"Fossil_Oil_shale_Actual_Consumption\",\n",
    "    \"Fossil_Peat_Actual_Aggregated\", \"Fossil_Peat_Actual_Consumption\",\n",
    "    \"Geothermal_Actual_Aggregated\", \"Geothermal_Actual_Consumption\",\n",
    "    \"Hydro_Pumped_Storage_Actual_Aggregated\", \"Hydro_Pumped_Storage_Actual_Consumption\",\n",
    "    \"Hydro_Run-of-river_and_poundage_Actual_Aggregated\", \"Hydro_Run-of-river_and_poundage_Actual_Consumption\",\n",
    "    \"Hydro_Water_Reservoir_Actual_Aggregated\", \"Hydro_Water_Reservoir_Actual_Consumption\",\n",
    "    \"Marine_Actual_Aggregated\", \"Marine_Actual_Consumption\",\n",
    "    \"Nuclear_Actual_Aggregated\", \"Nuclear_Actual_Consumption\",\n",
    "    \"Other_Actual_Aggregated\", \"Other_Actual_Consumption\",\n",
    "    \"Other_renewable_Actual_Aggregated\", \"Other_renewable_Actual_Consumption\",\n",
    "    \"Solar_Actual_Aggregated\", \"Solar_Actual_Consumption\",\n",
    "    \"Waste_Actual_Aggregated\", \"Waste_Actual_Consumption\",\n",
    "    \"Wind_Offshore_Actual_Aggregated\", \"Wind_Offshore_Actual_Consumption\",\n",
    "    \"Wind_Onshore_Actual_Aggregated\", \"Wind_Onshore_Actual_Consumption\"\n",
    "]\n",
    "\n",
    "# Read the CSV file containing the paths\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "success = True\n",
    "\n",
    "# Iterate through each file path in the column 'Time_Series_Raw_Data_File_Path'\n",
    "for index, row in df.iterrows():\n",
    "    file_path = row['Time_Series_Raw_Data_File_Path']\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Open the existing CSV file in read mode\n",
    "        with open(file_path, 'r', newline='') as file:\n",
    "            # Read the existing content\n",
    "            reader = csv.reader(file)\n",
    "            rows = list(reader)\n",
    "\n",
    "        # Insert a new empty row at the beginning\n",
    "        rows.insert(0, [])\n",
    "\n",
    "        # Write the updated content back to the CSV file\n",
    "        with open(file_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(rows)\n",
    "        \n",
    "        print(f\"Empty row added at the beginning of file '{file_path}'.\")\n",
    "\n",
    "        # Read the file content again after adding the empty row\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Find the index of the first empty row\n",
    "        empty_row_index = next((i for i, line in enumerate(lines) if line.strip() == \"\"), None)\n",
    "\n",
    "        # If an empty row is found, copy the headers to it\n",
    "        if empty_row_index is not None:\n",
    "            lines[empty_row_index] = ','.join(headers) + '\\n'\n",
    "\n",
    "            # Write the updated lines back to the file\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.writelines(lines)\n",
    "        else:\n",
    "            print(f\"No empty row found in file '{file_path}'. Headers not copied.\")\n",
    "    else:\n",
    "        success = False\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "if success:\n",
    "    print(\"Empty rows added and headers copied successfully to the first empty row of all files.\")\n",
    "else:\n",
    "    print(\"Some errors occurred while processing the files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d8951-144a-489a-86bd-03004b3d0403",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color: skyblue\">\n",
    "6.2. Raw Data Time Resolution\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Identifying the time spept of all downloaded files.\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Additional columns that indicates the Year, Mounth, Day, Hour and minute of the data are added to the files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9a7fb12-d29d-4180-b3a8-e1dfa0f9b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/AT/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BE/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BG/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CH/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CY/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CZ/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/DE/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/DK/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/EE/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/EL/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/ES/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/FI/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/FR/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/HR/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/HU/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/IE/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/IT/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LT/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LU/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LV/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/MT/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/NL/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/NO/2024_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1280822/3375416199.py:7: DtypeWarning: Columns (29,30,31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_df = pd.read_csv(file_path)\n",
      "/tmp/ipykernel_1280822/3375416199.py:7: DtypeWarning: Columns (29,30,31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  file_df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/PL/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/PT/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/RO/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SE/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SI/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SK/2024_1.csv\n",
      "Updated file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/UK/2024_1.csv\n",
      "All files updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file containing the paths\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate through each file path in the 'Time_Series_Raw_Data_File_Path' column\n",
    "for file_path in df['Time_Series_Raw_Data_File_Path']:\n",
    "    # Read the CSV file\n",
    "    file_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert the 'MTU' column to string to avoid errors with .str accessor\n",
    "    file_df['MTU'] = file_df['MTU'].astype(str)\n",
    "\n",
    "    # Add new columns 'Year', 'Month', 'Day', and 'Time_1', 'Time_2'\n",
    "    file_df['Year'] = data_year\n",
    "    file_df['Month'] = ''\n",
    "    file_df['Day'] = ''\n",
    "    file_df['Hour'] = file_df['MTU'].str[:2]  # Extract the first two characters of 'MTU' column\n",
    "    file_df['Minute'] = file_df['MTU'].str[3:5]  # Extract the third and fourth characters of 'MTU' column\n",
    "\n",
    "    # Set the value of the first row in the 'Month' column to '01'\n",
    "    # file_df.loc[0, 'Month'] = '01'\n",
    "    # file_df.loc[0, 'Day'] = '01'\n",
    "\n",
    "    # Reorder the columns to have 'Day' before 'Time'\n",
    "    file_df = file_df[['Year', 'Month', 'Day', 'Hour', 'Minute'] + [col for col in file_df.columns if col not in ['Year', 'Month', 'Day', 'Hour', 'Minute']]]\n",
    "\n",
    "    # Write the updated DataFrame back to the CSV file\n",
    "    file_df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Updated file: {file_path}\")\n",
    "\n",
    "print(\"All files updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232fb77b-30eb-485f-922c-7412ec0732b9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Extracting the time stept data.\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The rows corresponding to a time resolution of 1 hour, 30 minutes and 15 minutes are extrated and added to new files with the suffix _1h, _30min, and _15min.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1b314cc-2641-4763-872a-da3a1a9829e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/AT/2024_1.csv\n",
      "File has 35040 or 34544 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BE/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BG/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CH/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CY/2024_1.csv\n",
      "File has 0 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CZ/2024_1.csv\n",
      "File has 0 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/DE/2024_1.csv\n",
      "File has 25920 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/DK/2024_1.csv\n",
      "File has 3888 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/EE/2024_1.csv\n",
      "File has 0 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/EL/2024_1.csv\n",
      "File has 1680 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/ES/2024_1.csv\n",
      "File has 35040 or 34544 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/FI/2024_1.csv\n",
      "File has 35040 or 34544 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/FR/2024_1.csv\n",
      "File has 9720 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/HR/2024_1.csv\n",
      "File has 35040 or 34544 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/HU/2024_1.csv\n",
      "File has 35040 or 34544 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/IE/2024_1.csv\n",
      "File has 17520 or 17522 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/IT/2024_1.csv\n",
      "File has 8856 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LT/2024_1.csv\n",
      "File has 12024 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LU/2024_1.csv\n",
      "File has 35040 or 34544 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/LV/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/MT/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/NL/2024_1.csv\n",
      "File has 35040 or 34544 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/NO/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/PL/2024_1.csv\n",
      "File has 23328 rows. No specific conditions matched. Processing as default...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/PT/2024_1.csv\n",
      "File has 23328 rows. No specific conditions matched. Processing as default...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1280822/1939109200.py:5: DtypeWarning: Columns (34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n",
      "/tmp/ipykernel_1280822/1939109200.py:5: DtypeWarning: Columns (34,35,36,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(csv_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/RO/2024_1.csv\n",
      "File has 35040 or 34544 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SE/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SI/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/SK/2024_1.csv\n",
      "File has 8760 or 8761 rows. Processing...\n",
      "Processed successfully.\n",
      "Processing file: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/UK/2024_1.csv\n",
      "File has 17520 or 17522 rows. Processing...\n",
      "Processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def process_csv_file(csv_file_path):\n",
    "    print(f\"Processing file: {csv_file_path}\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Count the number of rows\n",
    "    num_rows = len(df)\n",
    "    \n",
    "    # Define file name without extension\n",
    "    file_name_no_ext = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "    \n",
    "    # Get the directory of the current CSV file\n",
    "    base_dir = os.path.dirname(csv_file_path)\n",
    "    \n",
    "    # Check conditions and process accordingly\n",
    "    if num_rows in [35040, 35136, 34544]:\n",
    "        print(f\"File has 35040 or 34544 rows. Processing...\")\n",
    "        process_35040_34544(df, file_name_no_ext, base_dir)\n",
    "    elif num_rows in [17520, 17522, 17568, 17570]:\n",
    "        print(f\"File has 17520 or 17522 rows. Processing...\")\n",
    "        process_17520_17522(df, file_name_no_ext, base_dir)\n",
    "    elif num_rows in [8760, 8761, 8784, 8785]:\n",
    "        print(f\"File has 8760 or 8761 rows. Processing...\")\n",
    "        process_8760_8761(df, file_name_no_ext, base_dir)\n",
    "    else:\n",
    "        print(f\"File has {num_rows} rows. No specific conditions matched. Processing as default...\")\n",
    "        process_default(df, file_name_no_ext, base_dir)\n",
    "\n",
    "def process_35040_34544(df, file_name_no_ext, base_dir):\n",
    "    # Create new file paths\n",
    "    suffixes = ['_1h', '_30min', '_15min']\n",
    "    new_file_paths = [os.path.join(base_dir, file_name_no_ext + suffix + '.csv') for suffix in suffixes]\n",
    "    \n",
    "    # Write data to new files\n",
    "    for new_file_path in new_file_paths:\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "    \n",
    "    # Filter rows and write to corresponding files\n",
    "    df_minute_zero = df[df['Minute'] == 0]\n",
    "    df_minute_zero.to_csv(os.path.join(base_dir, file_name_no_ext + '_1h.csv'), index=False)\n",
    "    \n",
    "    df_minute_zero_or_thirty = df[df['Minute'].isin([0, 30])]\n",
    "    df_minute_zero_or_thirty.to_csv(os.path.join(base_dir, file_name_no_ext + '_30min.csv'), index=False)\n",
    "    \n",
    "    print(\"Processed successfully.\")\n",
    "\n",
    "def process_17520_17522(df, file_name_no_ext, base_dir):\n",
    "    # Create new file paths\n",
    "    suffixes = ['_1h', '_30min']\n",
    "    new_file_paths = [os.path.join(base_dir, file_name_no_ext + suffix + '.csv') for suffix in suffixes]\n",
    "    \n",
    "    # Write data to new files\n",
    "    for new_file_path in new_file_paths:\n",
    "        df.to_csv(new_file_path, index=False)\n",
    "    \n",
    "    # Filter rows and write to corresponding files\n",
    "    df_minute_zero = df[df['Minute'] == 0]\n",
    "    df_minute_zero.to_csv(os.path.join(base_dir, file_name_no_ext + '_1h.csv'), index=False)\n",
    "    \n",
    "    print(\"Processed successfully.\")\n",
    "\n",
    "def process_8760_8761(df, file_name_no_ext, base_dir):\n",
    "    # Create new file path\n",
    "    new_file_path = os.path.join(base_dir, file_name_no_ext + '_1h.csv')\n",
    "    \n",
    "    # Write data to new file\n",
    "    df.to_csv(new_file_path, index=False)\n",
    "    \n",
    "    print(\"Processed successfully.\")\n",
    "\n",
    "def process_default(df, file_name_no_ext, base_dir):\n",
    "    # Create new file path\n",
    "    new_file_path = os.path.join(base_dir, file_name_no_ext + '_1h.csv')\n",
    "    \n",
    "    # Filter rows and write to corresponding files\n",
    "    df_minute_zero = df[df['Minute'] == 0]\n",
    "    df_minute_zero.to_csv(os.path.join(base_dir, file_name_no_ext + '_1h.csv'), index=False)\n",
    "    \n",
    "    print(\"Processed successfully.\")\n",
    "\n",
    "# File path\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "standard_time_data_df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each CSV file specified in 'Time_Series_Raw_Data_File_Path'\n",
    "for csv_file_path in standard_time_data_df['Time_Series_Raw_Data_File_Path']:\n",
    "    process_csv_file(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d047155-4814-484a-a5f5-fa24d3edbc6f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Filling the correspondig time stept.\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "For the next stepts of the formating process it is necesary identify the path of the files where all the data of the 30 Dispa-SET countries are going to be storaged differencing them into time stepts (1 hour, 30 min, and 15 min)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1a3149e-a4fb-4238-8c7c-d266baffb0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define file paths and variables\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "#data_year = 2023\n",
    "\n",
    "# Read the standard time data CSV file\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Define function to search for files in directory\n",
    "def search_files(directory, prefix):\n",
    "    for file in os.listdir(directory):\n",
    "        if file.startswith(prefix):\n",
    "            return os.path.join(directory, file)\n",
    "    return None\n",
    "\n",
    "# Add new columns\n",
    "df[f\"{data_year}_1h_File_Path\"] = \"\"\n",
    "df[f\"{data_year}_30min_File_Path\"] = \"\"\n",
    "df[f\"{data_year}_15min_File_Path\"] = \"\"\n",
    "\n",
    "# Iterate over Zone_Folder_Path column\n",
    "for index, row in df.iterrows():\n",
    "    zone_folder_path = row['Zone_Folder_Path']\n",
    "    if os.path.exists(zone_folder_path):\n",
    "        # Search for files in the directory\n",
    "        hour_file_path = search_files(zone_folder_path, f\"{data_year}_1_1h.csv\")\n",
    "        if hour_file_path:\n",
    "            df.at[index, f\"{data_year}_1h_File_Path\"] = hour_file_path\n",
    "            \n",
    "        min30_file_path = search_files(zone_folder_path, f\"{data_year}_1_30min.csv\")\n",
    "        if min30_file_path:\n",
    "            df.at[index, f\"{data_year}_30min_File_Path\"] = min30_file_path\n",
    "            \n",
    "        min15_file_path = search_files(zone_folder_path, f\"{data_year}_1_15min.csv\")\n",
    "        if min15_file_path:\n",
    "            df.at[index, f\"{data_year}_15min_File_Path\"] = min15_file_path\n",
    "\n",
    "# Save the updated DataFrame back to the same CSV file\n",
    "df.to_csv(standard_time_data_file_path, index=False)\n",
    "\n",
    "print(\"CSV file updated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a565b48-b4dc-4282-b9dd-0db9a369ce85",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Fulling the corresponding time step to each file.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Not all the countries have raw data in a resolution of 15 mimuntes, 30 minutes or 1 hour, so, all the next three cells all the files of the corresponding time stept are going to be fullfilling in their corresponding columns of Minute, Hour, Day and Month of the year specified in the variable data_year.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "<br>\n",
    "1 Hour time stept.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b00cf786-5e98-4348-97b0-0b5a6f6e71fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file updated successfully: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/AT/2024_1_1h.csv\n",
      "CSV file updated successfully: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BE/2024_1_1h.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1280822/3201936043.py:19: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  dates = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')\n",
      "/tmp/ipykernel_1280822/3201936043.py:19: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  dates = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')\n",
      "/tmp/ipykernel_1280822/3201936043.py:19: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  dates = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file updated successfully: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/BG/2024_1_1h.csv\n",
      "CSV file updated successfully: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CH/2024_1_1h.csv\n",
      "CSV file updated successfully: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CY/2024_1_1h.csv\n",
      "CSV file updated successfully: /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors/CZ/2024_1_1h.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1280822/3201936043.py:19: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  dates = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')\n",
      "/tmp/ipykernel_1280822/3201936043.py:19: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  dates = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')\n",
      "/tmp/ipykernel_1280822/3201936043.py:19: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  dates = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')\n",
      "/tmp/ipykernel_1280822/3201936043.py:19: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  dates = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (8784) does not match length of index (6480)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m hours \u001b[38;5;241m=\u001b[39m [date\u001b[38;5;241m.\u001b[39mhour \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m dates]  \u001b[38;5;66;03m# Cycle through hours (0-23)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Update the DataFrame with the generated data\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m months\n\u001b[1;32m     28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDay\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m days\n\u001b[1;32m     29\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m hours\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     com\u001b[38;5;241m.\u001b[39mrequire_length_match(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (8784) does not match length of index (6480)"
     ]
    }
   ],
   "source": [
    "# Specify the year\n",
    "year = data_year\n",
    "\n",
    "# Load the CSV file containing paths\n",
    "df_paths = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_paths.iterrows():\n",
    "    # Get the path from the corresponding column\n",
    "    file_path_column_name = f\"{year}_1h_File_Path\"  # Dynamically construct the column name based on the year\n",
    "    file_path = row[file_path_column_name]\n",
    "    \n",
    "    # Check if the path exists and is not NaN\n",
    "    if isinstance(file_path, str) and os.path.exists(file_path):\n",
    "        # Load the existing CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Generate a date range for the entire year\n",
    "        dates = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:00:00', freq='H')\n",
    "\n",
    "        # Extract month, day, and hour from the date range\n",
    "        months = [date.month for date in dates]\n",
    "        days = [date.day for date in dates]\n",
    "        hours = [date.hour % 24 for date in dates]  # Cycle through hours (0-23)\n",
    "\n",
    "        # Update the DataFrame with the generated data\n",
    "        df['Month'] = months\n",
    "        df['Day'] = days\n",
    "        df['Hour'] = hours\n",
    "\n",
    "        # Save the updated DataFrame back to the same CSV file, overwriting the original file\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f\"CSV file updated successfully: {file_path}\")\n",
    "    else:\n",
    "        print(f\"No valid path specified in row {index + 1}. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302bf2aa-6b82-429e-8800-6f5276f0fc07",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "30 Minutes time stept.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b9a5a-5481-4bba-bb12-b317b5272300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the year\n",
    "year = data_year\n",
    "\n",
    "# Load the CSV file containing paths\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "df_paths = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_paths.iterrows():\n",
    "    # Dynamically construct the column name based on the year\n",
    "    file_path_column_name = f\"{year}_30min_File_Path\"\n",
    "    \n",
    "    # Get the path from the corresponding column\n",
    "    file_path = row[file_path_column_name]\n",
    "    \n",
    "    # Check if the path exists and is not NaN\n",
    "    if isinstance(file_path, str) and os.path.exists(file_path):\n",
    "        # Load the existing CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Generate a date range for the entire year with a time step of 30 minutes\n",
    "        dates_30min = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:59:59', freq='30T')\n",
    "\n",
    "        # Extract month, day, and hour from the date range with 30-minute time step\n",
    "        months_30min = [date.month for date in dates_30min]\n",
    "        days_30min = [date.day for date in dates_30min]\n",
    "        hours_30min = [date.hour % 24 for date in dates_30min]  # Cycle through hours (0-23)\n",
    "\n",
    "        # Update the DataFrame with the generated data for 30-minute time step\n",
    "        df['Month'] = months_30min\n",
    "        df['Day'] = days_30min\n",
    "        df['Hour'] = hours_30min\n",
    "\n",
    "        # Save the updated DataFrame back to the same CSV file, overwriting the original file\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f\"CSV file updated successfully with 30-minute time step: {file_path}\")\n",
    "    else:\n",
    "        print(f\"No valid path specified in row {index + 1}. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbadb67-0e86-4573-a9a0-ceec35dc3036",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "15 Minutes time stept.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06094f3a-e066-4025-b912-7379ad230f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the year\n",
    "year = data_year\n",
    "\n",
    "# Load the CSV file containing paths\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "df_paths = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_paths.iterrows():\n",
    "    # Dynamically construct the column name based on the year\n",
    "    file_path_column_name = f\"{year}_15min_File_Path\"\n",
    "    \n",
    "    # Get the path from the corresponding column\n",
    "    file_path = row[file_path_column_name]\n",
    "    \n",
    "    # Check if the path exists and is not NaN\n",
    "    if isinstance(file_path, str) and os.path.exists(file_path):\n",
    "        # Load the existing CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Generate a date range for the entire year with a time step of 15 minutes\n",
    "        dates_15min = pd.date_range(start=f'{year}-01-01 00:00:00', end=f'{year}-12-31 23:59:59', freq='15T')\n",
    "\n",
    "        # Extract month, day, and hour from the date range with 15-minute time step\n",
    "        months_15min = [date.month for date in dates_15min]\n",
    "        days_15min = [date.day for date in dates_15min]\n",
    "        hours_15min = [date.hour % 24 for date in dates_15min]  # Cycle through hours (0-23)\n",
    "\n",
    "        # Update the DataFrame with the generated data for 15-minute time step\n",
    "        df['Month'] = months_15min\n",
    "        df['Day'] = days_15min\n",
    "        df['Hour'] = hours_15min\n",
    "\n",
    "        # Save the updated DataFrame back to the same CSV file, overwriting the original file\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "        print(f\"CSV file updated successfully with 15-minute time step: {file_path}\")\n",
    "    else:\n",
    "        print(f\"No valid path specified in row {index + 1}. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2c96b-81ae-40d7-aa17-ca7a775b71e6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Adding the Dispa-SET Time Stept format.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 1.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The format that Dispa-SET reads the time stept is the following, 0000-00-00 00:00:00+00:00 where the first part represents the date and the second part represents the time, so the next script add to each row of the correspongind file (1 hour file, 30 minutes file and 15 minutes file for each country) the time stept acording this format.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb85fe1-c972-45f9-9813-f267d78d788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create Dispa_SET_Time_Step column\n",
    "def create_Dispa_SET_Time_Step(row):\n",
    "    # Ensure values are integers (or default to 0 if NaN) before formatting\n",
    "    year = int(row['Year']) if pd.notna(row['Year']) else 0\n",
    "    month = int(row['Month']) if pd.notna(row['Month']) else 0\n",
    "    day = int(row['Day']) if pd.notna(row['Day']) else 0\n",
    "    hour = int(row['Hour']) if pd.notna(row['Hour']) else 0\n",
    "    minute = int(row['Minute']) if pd.notna(row['Minute']) else 0\n",
    "    \n",
    "    # Ensure two-digit format for Day, Month, Hour, and Minute\n",
    "    date_part = f\"{year:04d}-{month:02d}-{day:02d}\"\n",
    "    time_part = f\"{hour:02d}:{minute:02d}:00+00:00\"\n",
    "    return date_part + \" \" + time_part\n",
    "\n",
    "# Specify the year\n",
    "#data_year = 2023\n",
    "\n",
    "# Load the CSV file containing file paths\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "df_paths = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_paths.iterrows():\n",
    "    # Get the file paths from the corresponding columns\n",
    "    hour_file_path = row[f\"{data_year}_1h_File_Path\"]\n",
    "    min_30_file_path = row[f\"{data_year}_30min_File_Path\"]\n",
    "    min_15_file_path = row[f\"{data_year}_15min_File_Path\"]\n",
    "    \n",
    "    # Process each file path if it exists\n",
    "    for file_path in [hour_file_path, min_30_file_path, min_15_file_path]:\n",
    "        # Check if the path exists and is not NaN\n",
    "        if isinstance(file_path, str) and os.path.exists(file_path):\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Apply the function to create the Dispa_SET_Time_Step column\n",
    "            df['Dispa_SET_Time_Step'] = df.apply(create_Dispa_SET_Time_Step, axis=1)\n",
    "            \n",
    "            # Save the modified DataFrame back to the CSV file\n",
    "            df.to_csv(file_path, index=False)\n",
    "            \n",
    "            print(f\"CSV file updated successfully: {file_path}\")\n",
    "        else:\n",
    "            print(f\"No valid path specified in row {index + 1}. Skipping...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27790e2f-a114-4918-88df-9410063f1e36",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Erasing the unnecesary columns and ordering.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffdf5aa-df5e-4efb-a8aa-2c2018580ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to process each CSV file\n",
    "def process_csv_file(row, time_step_column):\n",
    "    csv_file = row[time_step_column]\n",
    "    if pd.notnull(csv_file) and os.path.exists(csv_file):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_csv = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Drop specified columns: Year, Month, Day, Hour, Minute, MTU\n",
    "        columns_to_drop = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'MTU']\n",
    "        df_csv = df_csv.drop(columns=columns_to_drop, errors='ignore')\n",
    "        \n",
    "        # Move the column 'Dispa_SET_Time_Step' to the first position\n",
    "        if 'Dispa_SET_Time_Step' in df_csv.columns:\n",
    "            columns = list(df_csv.columns)\n",
    "            columns.remove('Dispa_SET_Time_Step')\n",
    "            df_csv = df_csv[['Dispa_SET_Time_Step'] + columns]\n",
    "        \n",
    "        # Save the changes back to the CSV file\n",
    "        df_csv.to_csv(csv_file, index=False)\n",
    "        \n",
    "# Define the path to the CSV file containing the file paths\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Read the CSV file containing the paths\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Convert data_year to a string\n",
    "data_year = str(data_year)\n",
    "\n",
    "# Define the column names for the file paths\n",
    "hour_file_column = data_year + '_1h_File_Path'\n",
    "thirty_min_file_column = data_year + '_30min_File_Path'\n",
    "fifteen_min_file_column = data_year + '_15min_File_Path'\n",
    "\n",
    "# Process the one-hour file\n",
    "df.apply(lambda row: process_csv_file(row, hour_file_column), axis=1)\n",
    "\n",
    "# Process the thirty-minute file\n",
    "df.apply(lambda row: process_csv_file(row, thirty_min_file_column), axis=1)\n",
    "\n",
    "# Process the fifteen-minute file\n",
    "df.apply(lambda row: process_csv_file(row, fifteen_min_file_column), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b3806-b2cc-4d9f-8f0a-61b16de98347",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "6.3. Raw Data Subdirectories Creation\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Creating the sub directories where the raw data is going to be splited by technology type.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Once the raw data file has the appropiaded time stept format, the same has to be divided to has the corresponding time serie of each technology type isolated.\n",
    "<br>\n",
    "A sub folder named as the corresponding technology type are goint to be created to each country.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c372832-320d-49b2-9fbd-ae54f336edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the headers to be added\n",
    "new_headers = [\n",
    "    \"Biomass_Actual_Aggregated\", \"Biomass_Actual_Consumption\",\n",
    "    \"Fossil_Brown_coal-Lignite_Actual_Aggregated\", \"Fossil_Brown_coal-Lignite_Actual_Consumption\",\n",
    "    \"Fossil_Coal-derived_gas_Actual_Aggregated\", \"Fossil_Coal-derived_gas_Actual_Consumption\",\n",
    "    \"Fossil_Gas_Actual_Aggregated\", \"Fossil_Gas_Actual_Consumption\",\n",
    "    \"Fossil_Hard_coal_Actual_Aggregated\", \"Fossil_Hard_coal_Actual_Consumption\",\n",
    "    \"Fossil_Oil_Actual_Aggregated\", \"Fossil_Oil_Actual_Consumption\",\n",
    "    \"Fossil_Oil_shale_Actual_Aggregated\", \"Fossil_Oil_shale_Actual_Consumption\",\n",
    "    \"Fossil_Peat_Actual_Aggregated\", \"Fossil_Peat_Actual_Consumption\",\n",
    "    \"Geothermal_Actual_Aggregated\", \"Geothermal_Actual_Consumption\",\n",
    "    \"Hydro_Pumped_Storage_Actual_Aggregated\", \"Hydro_Pumped_Storage_Actual_Consumption\",\n",
    "    \"Hydro_Run-of-river_and_poundage_Actual_Aggregated\", \"Hydro_Run-of-river_and_poundage_Actual_Consumption\",\n",
    "    \"Hydro_Water_Reservoir_Actual_Aggregated\", \"Hydro_Water_Reservoir_Actual_Consumption\",\n",
    "    \"Marine_Actual_Aggregated\", \"Marine_Actual_Consumption\",\n",
    "    \"Nuclear_Actual_Aggregated\", \"Nuclear_Actual_Consumption\",\n",
    "    \"Other_Actual_Aggregated\", \"Other_Actual_Consumption\",\n",
    "    \"Other_renewable_Actual_Aggregated\", \"Other_renewable_Actual_Consumption\",\n",
    "    \"Solar_Actual_Aggregated\", \"Solar_Actual_Consumption\",\n",
    "    \"Waste_Actual_Aggregated\", \"Waste_Actual_Consumption\",\n",
    "    \"Wind_Offshore_Actual_Aggregated\", \"Wind_Offshore_Actual_Consumption\",\n",
    "    \"Wind_Onshore_Actual_Aggregated\", \"Wind_Onshore_Actual_Consumption\"\n",
    "]\n",
    "\n",
    "# Define the path to the CSV file containing the file paths\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Read the CSV file containing the paths\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Extract the zone folder path from the row\n",
    "    zone_folder_path = row['Zone_Folder_Path']\n",
    "    \n",
    "    # Create a new folder for each zone folder path\n",
    "    for header in new_headers:\n",
    "        new_folder_path = os.path.join(zone_folder_path, header)\n",
    "        if not os.path.exists(new_folder_path):\n",
    "            os.makedirs(new_folder_path)\n",
    "        else:\n",
    "            print(f\"Folder '{new_folder_path}' already exists.\")\n",
    "            \n",
    "        # Add the new column with the folder path to the DataFrame\n",
    "        df.at[index, header] = new_folder_path\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "df.to_csv(standard_time_data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691955d-3242-418c-8712-b696dcc57be6",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "6.4. Raw Data Subdirectories Divition\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Splitting each raw data file of each country into a single file by technology type.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Each new csv file is goning to be saved in the corresponding folder created in the previous cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ad5ab-d347-4551-8098-66f25f491932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the standard time data CSV file\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Load the standard time data CSV file\n",
    "standard_time_df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Get the data year from the variable\n",
    "#data_year = \"2023\"  # Example value\n",
    "\n",
    "# Iterate over each row in the standard time data\n",
    "for index, row in standard_time_df.iterrows():\n",
    "    # Extract the file paths from the specified columns\n",
    "    hour_file_path = row[data_year + \"_1h_File_Path\"]\n",
    "    thirty_min_file_path = row[data_year + \"_30min_File_Path\"]\n",
    "    fifteen_min_file_path = row[data_year + \"_15min_File_Path\"]\n",
    "    \n",
    "    # Iterate over the file paths and apply the processing code\n",
    "    for file_path in [hour_file_path, thirty_min_file_path, fifteen_min_file_path]:\n",
    "        # Skip if the file path is empty\n",
    "        if pd.isna(file_path):\n",
    "            continue\n",
    "        \n",
    "        # Load the CSV file\n",
    "        csv_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Get the headers\n",
    "        headers = csv_df.columns\n",
    "        \n",
    "        # Create a folder for each second column\n",
    "        for i in range(1, len(headers)):\n",
    "            second_column_name = headers[i]\n",
    "            folder_name = second_column_name.strip().replace(' ', '_')\n",
    "            folder_path = os.path.join(os.path.dirname(file_path), folder_name)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "        # Iterate over each pair of columns and save them into corresponding folders\n",
    "        for i in range(len(headers) - 1):\n",
    "            first_column = headers[0]\n",
    "            second_column = headers[i + 1]\n",
    "            new_df = csv_df[[first_column, second_column]]\n",
    "            \n",
    "            # Get the folder name for the second column\n",
    "            folder_name = second_column.strip().replace(' ', '_')\n",
    "            folder_path = os.path.join(os.path.dirname(file_path), folder_name)\n",
    "            \n",
    "            # Get the base file name without extension\n",
    "            base_file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            \n",
    "            # Create the new file path\n",
    "            new_file_path = os.path.join(folder_path, f\"{base_file_name}.csv\")\n",
    "            \n",
    "            # Save the new DataFrame to a CSV file inside the folder\n",
    "            new_df.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06724fab-37e0-4e29-9472-e73fb5da2b85",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Replacing non numeric values in the separated files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d93ed-4566-421e-b31a-c075db4b2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files(new_headers, standard_time_data_file_path):\n",
    "  \"\"\"\n",
    "  Processes CSV files, replacing non-numeric values in the second column with '0'.\n",
    "\n",
    "  Args:\n",
    "    new_headers: A list of column names.\n",
    "    standard_time_data_file_path: Path to the CSV file containing folder paths.\n",
    "  \"\"\"\n",
    "\n",
    "  df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "  for column in new_headers:\n",
    "    folder_paths = df[column]\n",
    "\n",
    "    for path in folder_paths:\n",
    "      if pd.notnull(path):\n",
    "        csv_files = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.csv')]\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "          data = pd.read_csv(csv_file)\n",
    "          data[data.columns[1]] = pd.to_numeric(data[data.columns[1]], errors='coerce').fillna(0)\n",
    "          data.to_csv(csv_file, index=False)\n",
    "          print(f\"Processed {csv_file}\")\n",
    "\n",
    "process_csv_files(new_headers, standard_time_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf707b0-59eb-4c7a-bbfc-029b0752be79",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "7. Total Installed Capacity per Production Type\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Creating the files contents of the total installed capacity per production type for each one of the coutries modelled in Dispa-SET.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The source of the data belongs the ENTSO E portal web.\n",
    "<br>\n",
    "The information is extracted from the local directory:\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 4.0em; font-weight: unbold; font-size: 12px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    '/Local/Directory/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Total_Installed_Capacity_per_Production_Type/'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973a364-fd2a-46cd-83e5-762afeb7b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_path_3 = \"Total_Installed_Capacity_per_Production_Type/\"\n",
    "total_instaled_capacity_per_production_type_folder_path = availability_factors_folder_path + additional_path_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279e7c8-ab3b-4969-ac74-54cb25459649",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69bb1b0-9e17-42c9-94d7-723da96a6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"dispaSET_unleash_folder_name:                              {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path:                              {dispaSET_unleash_folder_path}\")\n",
    "print (f\"data_year:                                                 {data_year}\")\n",
    "print (f\"standard_time:                                             {standard_time}\")\n",
    "print (f\"standard_time_data_folder_path:                            {standard_time_data_folder_path}\")\n",
    "print (f\"availability_factors_folder_path:                          {availability_factors_folder_path}\")\n",
    "print (f\"scaled_outflows_folder_path:                               {scaled_outflows_folder_path}\")\n",
    "print (f\"standard_time_data_file_path:                              {standard_time_data_file_path}\")\n",
    "print (f\"standard_time_data_file_name:                              {standard_time_data_file_name}\")\n",
    "print (f\"raw_rata_source_folder_path:                               {raw_rata_source_folder_path}\")\n",
    "print (f\"total_instaled_capacity_per_production_type_folder_path:   {total_instaled_capacity_per_production_type_folder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8074a1e-99c1-4ba2-bf9e-1c6d06e5e9f7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Extracting and naming the total installed capacity raw data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05542391-801c-4bb7-bbb1-0017686cc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_and_rename_files(dispaSET_codes, availability_factors_folder_path, total_instaled_capacity_per_production_type_folder_path, data_year, standard_time_data_file_path):\n",
    "    \"\"\"\n",
    "    Copies and renames CSV files based on specified parameters, and writes the paths to a new column in the standard time data CSV.\n",
    "\n",
    "    Args:\n",
    "        dispaSET_codes (list): A list of country codes.\n",
    "        availability_factors_folder_path (str): Path to the destination folder.\n",
    "        total_instaled_capacity_per_production_type_folder_path (str): Path to the source folder.\n",
    "        data_year (str): The year to be included in the new file name.\n",
    "        standard_time_data_file_path (str): Path to the standard time data CSV file.\n",
    "    \"\"\"\n",
    "    # List to store the paths of the copied files\n",
    "    copied_files_paths = []\n",
    "\n",
    "    for code in dispaSET_codes:\n",
    "        source_file = os.path.join(total_instaled_capacity_per_production_type_folder_path, code, 'Total_Installed_Capacity_per_Production_Type.csv')\n",
    "        destination_folder = os.path.join(availability_factors_folder_path, code)\n",
    "        destination_file = os.path.join(destination_folder, f\"{data_year}_Total_Installed_Capacity_per_Production_Type.csv\")\n",
    "\n",
    "        if not os.path.exists(destination_folder):\n",
    "            os.makedirs(destination_folder)\n",
    "\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        print(f\"Copied file from {source_file} to {destination_file}\")\n",
    "\n",
    "        # Append the path of the copied file to the list\n",
    "        copied_files_paths.append(destination_file)\n",
    "\n",
    "        # Print a message indicating the file was processed\n",
    "        print(f\"Processed CSV file for {code}\")\n",
    "\n",
    "    # Read the standard time data CSV file\n",
    "    standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "    # Add the new column with the copied files paths\n",
    "    standard_time_data['Production_Type_Total_Installed_Capacity_File_Path'] = pd.Series(copied_files_paths)\n",
    "\n",
    "    # Write the updated dataframe back to the CSV file\n",
    "    standard_time_data.to_csv(standard_time_data_file_path, index=False)\n",
    "    print(f\"Updated {standard_time_data_file_path} with new file paths.\")\n",
    "\n",
    "# Example usage\n",
    "# dispaSET_codes = ['CODE1', 'CODE2', 'CODE3']\n",
    "# availability_factors_folder_path = '/path/to/availability_factors'\n",
    "# total_instaled_capacity_per_production_type_folder_path = '/path/to/total_instaled_capacity'\n",
    "# data_year = '2023'\n",
    "# standard_time_data_file_path = '/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data_Source.csv'\n",
    "copy_and_rename_files(dispaSET_codes, availability_factors_folder_path, total_instaled_capacity_per_production_type_folder_path, data_year, standard_time_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883bc2c8-2f35-4f3a-9cbc-b12da7571467",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Giving the corresponding headers to the columns of each downloaded file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac477006-af14-4098-a2ab-dc54f4d283e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add headers to CSV files\n",
    "def add_headers_to_csv(csv_file_path):\n",
    "    # Specify the headers\n",
    "    headers = ['Production_Type', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024']\n",
    "    \n",
    "    # Read the existing data from the CSV file\n",
    "    with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        rows = list(reader)\n",
    "\n",
    "    # Add headers to the existing data\n",
    "    rows.insert(0, headers)\n",
    "\n",
    "    # Write the updated data back to the CSV file\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# Function to process the standard time data\n",
    "def process_standard_time_data(standard_time_data_file_path):\n",
    "    # Open the standard time data file\n",
    "    with open(standard_time_data_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        # Process each row in the standard time data\n",
    "        for row in reader:\n",
    "            # Get the path of the CSV file\n",
    "            csv_file_path = row['Production_Type_Total_Installed_Capacity_File_Path']\n",
    "            \n",
    "            # Add headers to the CSV file\n",
    "            add_headers_to_csv(csv_file_path)\n",
    "\n",
    "# Specify the path to the CSV file containing the standard time data\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Process the standard time data\n",
    "process_standard_time_data(standard_time_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879a78e-b77a-454d-bc43-97f2dd153eea",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Harmonizing the generation type column fields \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3adbbf-b1d9-47a7-98ba-1dbfecc970fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file containing the file paths\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "df_paths = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each row to process the files\n",
    "for index, row in df_paths.iterrows():\n",
    "    folder_path = row['Zone_Folder_Path']\n",
    "    data_year = row['Data_Year']\n",
    "\n",
    "    # Construct the file path\n",
    "    file_name = f\"{data_year}_Total_Installed_Capacity_per_Production_Type.csv\"\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Replace ' ' with '_' and '/' with '-'\n",
    "    df['Production_Type'] = df['Production_Type'].str.replace(' ', '_').str.replace('/', '-')\n",
    "\n",
    "    # Save the modified DataFrame back to the original CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    # Print a confirmation message for each file\n",
    "    print(f\"Production types in {file_name} have been processed and saved back to the original file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b256da-0e8c-4f68-9eca-79be8d5f1375",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Getting the paths to each total installed capacity file for each country.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56cf4b0-8453-43da-afb4-26036319667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "#data_year = 2024\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Read the standard time data CSV file\n",
    "standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Function to get the file path for each row\n",
    "def get_csv_file_path(row):\n",
    "    zone_folder_path = row['Zone_Folder_Path']\n",
    "    csv_file_name = f\"{data_year}_Total_Installed_Capacity_per_Production_Type.csv\"\n",
    "    csv_file_path = os.path.join(zone_folder_path, csv_file_name)\n",
    "    return csv_file_path\n",
    "\n",
    "# Add a new column 'Production_Type_Total_Installed_Capacity_File_Path' with file paths\n",
    "standard_time_data['Production_Type_Total_Installed_Capacity_File_Path'] = standard_time_data.apply(get_csv_file_path, axis=1)\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "standard_time_data.to_csv(standard_time_data_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed149ee-cf15-4720-a3fc-a4bb3da2866b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Fullfilling missing years to each total installed capacity file for each country.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19750725-8808-4717-a90c-3344bf55853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file containing the file paths\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Read the CSV file\n",
    "standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each file path in the 'Production_Type_Total_Installed_Capacity_file_path' column\n",
    "for index, row in standard_time_data.iterrows():\n",
    "    # Get the file path\n",
    "    csv_file_path = row['Production_Type_Total_Installed_Capacity_File_Path']\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Iterate over columns from '2015' to '2024'\n",
    "    for column in df.columns[1:]:\n",
    "        # Iterate over rows in the column\n",
    "        for index, value in df[column].items():\n",
    "            # Check if the value is non-numeric or empty\n",
    "            if str(value).strip() in ('n/e', 'N/A', '', '0'):\n",
    "                # Get the corresponding value from the previous column\n",
    "                prev_value = df.at[index, df.columns[df.columns.get_loc(column) - 1]]\n",
    "                # Check if the previous value is numeric\n",
    "                if str(prev_value).strip().replace('.', '').isdigit():\n",
    "                    # Copy the previous value to the current field\n",
    "                    df.at[index, column] = prev_value\n",
    "\n",
    "    # Save the modified DataFrame back to the CSV file\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Print a completion message\n",
    "print(\"Missing or non-numeric values in the CSV files have been filled based on the corresponding values from the previous columns where applicable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3419c53-75a4-4877-8667-0db72e0af634",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Filling the empty or 'n/e' or 'N/A' fields.\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a7b9f-6488-4ad4-9154-ee85ada64639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(standard_time_data_file_path):\n",
    "    \"\"\"Fills missing values in CSV files with '0'.\n",
    "\n",
    "    Args:\n",
    "        standard_time_data_file_path (str): Path to the standard time data CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        file_path = row['Production_Type_Total_Installed_Capacity_File_Path']\n",
    "        data = pd.read_csv(file_path)\n",
    "        data.fillna(0, inplace=True)\n",
    "        data = data.replace('n/e', 0)\n",
    "        data = data.replace('N/A', 0)\n",
    "        data.to_csv(file_path, index=False)\n",
    "        print(f\"Processed file: {file_path}\")\n",
    "\n",
    "fill_missing_values(standard_time_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735922fe-9912-4048-9f9b-63f3dc3c71e1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color:skyblue\">\n",
    "8. Availability Factor File\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Getting the time series value per technology type.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "All the raw data files  of generation and already separated by technology type is divided by the total installed capacity of the corresponding technology.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aaf9fe-9adc-49a4-8b5f-82c0f2d05dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "# Convert data_year to a string\n",
    "data_year = str(data_year)\n",
    "#standard_time_data_file_path = '/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv'\n",
    "\n",
    "# Read the CSV file containing file paths\n",
    "df_production_type = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_production_type.iterrows():\n",
    "    # Extract the file path for the current production type\n",
    "    file_path = row['Production_Type_Total_Installed_Capacity_File_Path']\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert 'data_year' column to numeric type\n",
    "    df[data_year] = pd.to_numeric(df[data_year], errors='coerce').fillna(0)\n",
    "\n",
    "    # Iterate over each row\n",
    "    for index, row in df.iterrows():\n",
    "        value = row[data_year]\n",
    "        production_type = row['Production_Type']\n",
    "\n",
    "        # Find the corresponding folder\n",
    "        folder_name = production_type.replace(' ', '_')\n",
    "        aggregated_folder = os.path.join(os.path.dirname(file_path), f\"{folder_name}_Actual_Aggregated\")\n",
    "        consumption_folder = os.path.join(os.path.dirname(file_path), f\"{folder_name}_Actual_Consumption\")\n",
    "\n",
    "        # Process files in aggregated folder\n",
    "        if os.path.exists(aggregated_folder):\n",
    "            for filename in os.listdir(aggregated_folder):\n",
    "                if filename.endswith('.csv'):\n",
    "                    file_path_aggregated = os.path.join(aggregated_folder, filename)\n",
    "                    df_aggregated = pd.read_csv(file_path_aggregated)\n",
    "\n",
    "                    # Convert second column to numeric type\n",
    "                    df_aggregated.iloc[:, 1] = pd.to_numeric(df_aggregated.iloc[:, 1], errors='coerce').fillna(0)\n",
    "\n",
    "                    # Check if value is not 'n/e', 'N/A', 0, or empty\n",
    "                    if value not in ['n/e', 'N/A', 0, '']:\n",
    "                        # Perform division operation\n",
    "                        if value != 0:\n",
    "                            df_aggregated.iloc[:, 1] /= value\n",
    "                        else:\n",
    "                            # Handle division by zero\n",
    "                            pass\n",
    "                    else:\n",
    "                        # Fill second column with 0\n",
    "                        df_aggregated.iloc[:, 1] = 0\n",
    "\n",
    "                    new_file_name = f\"Availabilty_Factor_{filename}\"\n",
    "                    df_aggregated.to_csv(os.path.join(aggregated_folder, new_file_name), index=False)\n",
    "\n",
    "        # Process files in consumption folder\n",
    "        if os.path.exists(consumption_folder):\n",
    "            for filename in os.listdir(consumption_folder):\n",
    "                if filename.endswith('.csv'):\n",
    "                    file_path_consumption = os.path.join(consumption_folder, filename)\n",
    "                    df_consumption = pd.read_csv(file_path_consumption)\n",
    "\n",
    "                    # Convert second column to numeric type\n",
    "                    df_consumption.iloc[:, 1] = pd.to_numeric(df_consumption.iloc[:, 1], errors='coerce').fillna(0)\n",
    "\n",
    "                    # Check if value is not 'n/e', 'N/A', 0, or empty\n",
    "                    if value not in ['n/e', 'N/A', 0, '']:\n",
    "                        # Perform division operation\n",
    "                        if value != 0:\n",
    "                            df_consumption.iloc[:, 1] /= value\n",
    "                        else:\n",
    "                            # Handle division by zero\n",
    "                            pass\n",
    "                    else:\n",
    "                        # Fill second column with 0\n",
    "                        df_consumption.iloc[:, 1] = 0\n",
    "\n",
    "                    new_file_name = f\"Availabilty_Factor_{filename}\"\n",
    "                    df_consumption.to_csv(os.path.join(consumption_folder, new_file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e42b3-4fb1-4a4c-89a1-a175b4e702ee",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Creating the subfolders and the csv files that will content the final data.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Into each country folder, it will be created a subfolder named under the corresponding time step of the data. e.g. '1h', and '15min' or '30min' being possible that some countries has the 3 files or just 1 or 2. \n",
    "<br>\n",
    "This is due to the availability of the data in the sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f43764-60db-4b8f-9f2b-b2a392446a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "#standard_time_data_file_path = '/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv'\n",
    "#data_year = '2023'\n",
    "path_columns = ['Zone_Folder_Path', 'Biomass_Actual_Aggregated', 'Biomass_Actual_Consumption', 'Fossil_Brown_coal-Lignite_Actual_Aggregated',\n",
    "                'Fossil_Brown_coal-Lignite_Actual_Consumption', 'Fossil_Coal-derived_gas_Actual_Aggregated', 'Fossil_Coal-derived_gas_Actual_Consumption',\n",
    "                'Fossil_Gas_Actual_Aggregated',\t'Fossil_Gas_Actual_Consumption', 'Fossil_Hard_coal_Actual_Aggregated', 'Fossil_Hard_coal_Actual_Consumption',\n",
    "                'Fossil_Oil_Actual_Aggregated',\t'Fossil_Oil_Actual_Consumption', 'Fossil_Oil_shale_Actual_Aggregated', 'Fossil_Oil_shale_Actual_Consumption',\n",
    "                'Fossil_Peat_Actual_Aggregated', 'Fossil_Peat_Actual_Consumption', 'Geothermal_Actual_Aggregated', 'Geothermal_Actual_Consumption', \n",
    "                'Hydro_Pumped_Storage_Actual_Aggregated', 'Hydro_Pumped_Storage_Actual_Consumption', 'Hydro_Run-of-river_and_poundage_Actual_Aggregated',\n",
    "                'Hydro_Run-of-river_and_poundage_Actual_Consumption', 'Hydro_Water_Reservoir_Actual_Aggregated', 'Hydro_Water_Reservoir_Actual_Consumption',\n",
    "                'Marine_Actual_Aggregated',\t'Marine_Actual_Consumption', 'Nuclear_Actual_Aggregated', 'Nuclear_Actual_Consumption', 'Other_Actual_Aggregated',\n",
    "                'Other_Actual_Consumption',\t'Other_renewable_Actual_Aggregated', 'Other_renewable_Actual_Consumption', 'Solar_Actual_Aggregated',\n",
    "                'Solar_Actual_Consumption',\t'Waste_Actual_Aggregated', 'Waste_Actual_Consumption', 'Wind_Offshore_Actual_Aggregated', 'Wind_Offshore_Actual_Consumption',\n",
    "                'Wind_Onshore_Actual_Aggregated', 'Wind_Onshore_Actual_Consumption'\n",
    "    \n",
    "]  # Replace with your column names\n",
    "\n",
    "# Read the standard time data CSV file\n",
    "standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each path column\n",
    "for column in path_columns:\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for index, row in standard_time_data.iterrows():\n",
    "        # Extract the folder path\n",
    "        folder_path = row[column]\n",
    "        \n",
    "        # Check if the folder exists\n",
    "        if os.path.exists(folder_path):\n",
    "            # Count the number of existing CSV files in the folder\n",
    "            existing_files = []\n",
    "            for time_interval in ['1h', '15min', '30min']:\n",
    "                file_name = f\"{data_year}_1_{time_interval}.csv\"\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.exists(file_path):\n",
    "                    existing_files.append(time_interval)\n",
    "            \n",
    "            # Create subfolders based on the number of existing files\n",
    "            for time_interval in existing_files:\n",
    "                subfolder_path = os.path.join(folder_path, time_interval)\n",
    "                os.makedirs(subfolder_path, exist_ok=True)\n",
    "                \n",
    "                # Create a CSV file inside the subfolder with the name of data_year\n",
    "                csv_file_path = os.path.join(subfolder_path, f\"{data_year}.csv\")\n",
    "                with open(csv_file_path, 'w') as f:\n",
    "                    f.write(\"This is a sample CSV file.\")\n",
    "                    # Print a message indicating the creation of the file\n",
    "                print(f\"File '{data_year}.csv' created in '{subfolder_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4da59-680c-4437-9264-d154b5e4c748",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Copying the data processed to a separated file accordingly the technology type.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5954413-d73c-4bba-a788-066cd8b9dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_csv_content(data_year, standard_time_data_file_path):\n",
    "    \"\"\"\n",
    "    Copy content from availability factor CSV files to destination CSV files based on specified conditions.\n",
    "\n",
    "    Args:\n",
    "        data_year (int): The year integer.\n",
    "        standard_time_data_file_path (str): Path to the standard time data CSV file.\n",
    "    \"\"\"\n",
    "    # Read the standard time data CSV file\n",
    "    df_standard_time = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "    # Define the columns to process\n",
    "    columns_to_process = [\n",
    "        'Biomass_Actual_Aggregated', 'Biomass_Actual_Consumption',\n",
    "        'Fossil_Brown_coal-Lignite_Actual_Aggregated', 'Fossil_Brown_coal-Lignite_Actual_Consumption',\n",
    "        'Fossil_Coal-derived_gas_Actual_Aggregated', 'Fossil_Coal-derived_gas_Actual_Consumption',\n",
    "        'Fossil_Gas_Actual_Aggregated', 'Fossil_Gas_Actual_Consumption',\n",
    "        'Fossil_Hard_coal_Actual_Aggregated', 'Fossil_Hard_coal_Actual_Consumption',\n",
    "        'Fossil_Oil_Actual_Aggregated', 'Fossil_Oil_Actual_Consumption',\n",
    "        'Fossil_Oil_shale_Actual_Aggregated', 'Fossil_Oil_shale_Actual_Consumption',\n",
    "        'Fossil_Peat_Actual_Aggregated', 'Fossil_Peat_Actual_Consumption',\n",
    "        'Geothermal_Actual_Aggregated', 'Geothermal_Actual_Consumption',\n",
    "        'Hydro_Pumped_Storage_Actual_Aggregated', 'Hydro_Pumped_Storage_Actual_Consumption',\n",
    "        'Hydro_Run-of-river_and_poundage_Actual_Aggregated', 'Hydro_Run-of-river_and_poundage_Actual_Consumption',\n",
    "        'Hydro_Water_Reservoir_Actual_Aggregated', 'Hydro_Water_Reservoir_Actual_Consumption',\n",
    "        'Marine_Actual_Aggregated', 'Marine_Actual_Consumption',\n",
    "        'Nuclear_Actual_Aggregated', 'Nuclear_Actual_Consumption',\n",
    "        'Other_Actual_Aggregated', 'Other_Actual_Consumption',\n",
    "        'Other_renewable_Actual_Aggregated', 'Other_renewable_Actual_Consumption',\n",
    "        'Solar_Actual_Aggregated', 'Solar_Actual_Consumption',\n",
    "        'Waste_Actual_Aggregated', 'Waste_Actual_Consumption',\n",
    "        'Wind_Offshore_Actual_Aggregated', 'Wind_Offshore_Actual_Consumption',\n",
    "        'Wind_Onshore_Actual_Aggregated', 'Wind_Onshore_Actual_Consumption'\n",
    "    ]\n",
    "\n",
    "    # Define the subfolder mapping\n",
    "    subfolder_mapping = {\n",
    "        '1h': f'Availabilty_Factor_{data_year}_1_1h.csv',\n",
    "        '15min': f'Availabilty_Factor_{data_year}_1_15min.csv',\n",
    "        '30min': f'Availabilty_Factor_{data_year}_1_30min.csv'\n",
    "    }\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for index, row in df_standard_time.iterrows():\n",
    "        for column in columns_to_process:\n",
    "            if column in row:\n",
    "                base_path = row[column]\n",
    "\n",
    "                # Iterate over each subfolder\n",
    "                for subfolder, source_file_name in subfolder_mapping.items():\n",
    "                    source_file_path = os.path.join(base_path, source_file_name)\n",
    "                    dest_folder_path = os.path.join(base_path, subfolder)\n",
    "                    dest_file_path = os.path.join(dest_folder_path, f'{data_year}.csv')\n",
    "\n",
    "                    # Check if the source CSV file exists\n",
    "                    if os.path.exists(source_file_path):\n",
    "                        # Ensure the destination subfolder exists\n",
    "                        os.makedirs(dest_folder_path, exist_ok=True)\n",
    "\n",
    "                        # Copy content from source to destination file, overwriting if exists\n",
    "                        shutil.copy2(source_file_path, dest_file_path)\n",
    "                        print(f\"Copied from '{source_file_path}' to '{dest_file_path}'\")\n",
    "                    else:\n",
    "                        print(f\"Source file does not exist: '{source_file_path}'\")\n",
    "\n",
    "copy_csv_content(data_year, standard_time_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3aaac7-ff2b-4388-b84c-ecd9699d7f5e",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Giving the corresponding headers to the csv files that will content the final data.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The Availability Factor Time Series have a determined header according the renewable techology type (HROR, PHOT, WTON, WOTF) , those are used to be read by Dispa-SET.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340659df-c406-49e9-b144-6b2537353b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_headers(data_year, standard_time_data_file_path):\n",
    "    \"\"\"\n",
    "    Replace headers in CSV files based on specified conditions without erasing the existing data.\n",
    "\n",
    "    Args:\n",
    "        data_year (str): The year string.\n",
    "        standard_time_data_file_path (str): Path to the standard time data CSV file.\n",
    "    \"\"\"\n",
    "    # Read the standard time data CSV file\n",
    "    standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "    # Define header mappings\n",
    "    header_mapping = {\n",
    "        'Hydro_Run-of-river_and_poundage_Actual_Aggregated': ['', 'HROR'],\n",
    "        'Solar_Actual_Aggregated': ['', 'PHOT'],\n",
    "        'Wind_Offshore_Actual_Aggregated': ['', 'WTOF'],\n",
    "        'Wind_Onshore_Actual_Aggregated': [' ', 'WTON']\n",
    "    }\n",
    "\n",
    "    # Iterate over each column in the dataframe\n",
    "    for column in standard_time_data.columns:\n",
    "        if column in header_mapping:\n",
    "            # Iterate over each folder path in the column\n",
    "            for path in standard_time_data[column]:\n",
    "                # Iterate over each subfolder\n",
    "                for subfolder in ['1h', '15min', '30min']:\n",
    "                    # Construct the path to the CSV file\n",
    "                    csv_file_path = os.path.join(path, subfolder, f'{data_year}.csv')\n",
    "                    \n",
    "                    # Check if the CSV file exists\n",
    "                    if os.path.exists(csv_file_path):\n",
    "                        # Read the existing data from the CSV file\n",
    "                        df = pd.read_csv(csv_file_path)\n",
    "                        \n",
    "                        # Define the new headers\n",
    "                        headers = header_mapping[column]\n",
    "                        # Update the headers of the DataFrame\n",
    "                        new_headers = headers + list(df.columns[len(headers):])\n",
    "                        df.columns = new_headers\n",
    "                        \n",
    "                        # Write the DataFrame with new headers back to the CSV file\n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        \n",
    "                        print(f\"Headers replaced for '{csv_file_path}'\")\n",
    "\n",
    "replace_headers(data_year, standard_time_data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b39f06-b167-41b8-bb4f-a554a5fec10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94cbc3df-50ce-4b3d-a4f5-f3a57d1233be",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Controling the range (min = 0, max = 1) of the time series.\n",
    "<div/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed678a-7d2a-41c4-ac16-fee3bd4419f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the standard time data CSV file\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate through each row in the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    solar_path = row['Solar_Actual_Aggregated']\n",
    "    \n",
    "    # Define the subfolders we are interested in\n",
    "    subfolders = ['1h', '15min', '30min']\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        # Construct the path to the subfolder's CSV file\n",
    "        csv_file_path = os.path.join(solar_path, subfolder, f\"{data_year}.csv\")\n",
    "        \n",
    "        # Check if the CSV file exists\n",
    "        if os.path.exists(csv_file_path):\n",
    "            # Read the CSV file\n",
    "            subfolder_df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            # Check for any value in the 'PHOT' column that is greater than 1 and set it to 1\n",
    "            subfolder_df['PHOT'] = subfolder_df['PHOT'].apply(lambda x: 1 if x > 1 else x)\n",
    "            \n",
    "            # Save the updated CSV file\n",
    "            subfolder_df.to_csv(csv_file_path, index=False)\n",
    "            print(f\"Updated {csv_file_path} where PHOT values were greater than 1.\")\n",
    "        else:\n",
    "            print(f\"CSV file {csv_file_path} does not exist.\")\n",
    "\n",
    "print(\"All files have been processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06387de-4d68-4d86-898b-81822c758e46",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "8.2 Final Availability Factor Time Series\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Getting the Availability Factor Time Series file.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "All the data is copied to the single csv file in a sub-folder with the time stept inside the country folder (AT, BE, CH... etc) .\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf84c3e-7563-4c03-9386-9de826c15469",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_year = \"2023\"\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Read the CSV file to get the source file paths\n",
    "df_standard_time = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each row of the specified columns\n",
    "for index, row in df_standard_time.iterrows():\n",
    "    # Extract the source file paths from the current row\n",
    "    file_path_1 = row['Zone_Folder_Path']\n",
    "    file_path_2 = row['Wind_Onshore_Actual_Aggregated']\n",
    "    file_path_3 = row['Wind_Offshore_Actual_Aggregated']\n",
    "    file_path_4 = row['Solar_Actual_Aggregated']\n",
    "    file_path_5 = row['Hydro_Run-of-river_and_poundage_Actual_Aggregated']\n",
    "    \n",
    "    # Define the subfolders to iterate over\n",
    "    subfolders = ['1h', '15min', '30min']\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        # Construct the file paths for each subfolder\n",
    "        csv_file_path_1 = os.path.join(file_path_1, subfolder, f\"{data_year}.csv\")\n",
    "        csv_file_path_2 = os.path.join(file_path_2, subfolder, f\"{data_year}.csv\")\n",
    "        csv_file_path_3 = os.path.join(file_path_3, subfolder, f\"{data_year}.csv\")\n",
    "        csv_file_path_4 = os.path.join(file_path_4, subfolder, f\"{data_year}.csv\")\n",
    "        csv_file_path_5 = os.path.join(file_path_5, subfolder, f\"{data_year}.csv\")\n",
    "        \n",
    "        # Check if CSV files exist in file_path_2, file_path_3, file_path_4, and file_path_5\n",
    "        if os.path.exists(csv_file_path_2) and os.path.exists(csv_file_path_3) \\\n",
    "            and os.path.exists(csv_file_path_4) and os.path.exists(csv_file_path_5):\n",
    "            \n",
    "            # Read the contents of the CSV files\n",
    "            df_2 = pd.read_csv(csv_file_path_2)\n",
    "            df_3 = pd.read_csv(csv_file_path_3)\n",
    "            df_4 = pd.read_csv(csv_file_path_4)\n",
    "            df_5 = pd.read_csv(csv_file_path_5)\n",
    "            \n",
    "            # Combine the contents of the CSV files\n",
    "            combined_df = pd.concat([df_2.iloc[:, :2], df_3.iloc[:, 1], df_4.iloc[:, 1], df_5.iloc[:, 1]], axis=1)\n",
    "            \n",
    "            # Delete the destination CSV file if it exists\n",
    "            if os.path.exists(csv_file_path_1):\n",
    "                os.remove(csv_file_path_1)\n",
    "            \n",
    "            # Write the combined DataFrame to the CSV file in file_path_1\n",
    "            combined_df.to_csv(csv_file_path_1, index=False)\n",
    "            \n",
    "            print(f\"Contents from CSV files written to '{csv_file_path_1}'\")\n",
    "        else:\n",
    "            print(f\"One or more CSV files do not exist at the specified paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15522fc-2eda-4f01-9079-66f65a04efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "#standard_time_data_file_path = '/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Assuming data_year is already defined\n",
    "#data_year = \"2023\"\n",
    "\n",
    "# Columns to check\n",
    "columns_to_check = ['WTON', 'WTOF', 'PHOT', 'HROR']\n",
    "\n",
    "# List of subfolders to check\n",
    "subfolders = ['1h', '15min', '30min']\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in standard_time_data.iterrows():\n",
    "    # Get the folder path from the 'Zone_Folder_Path' column\n",
    "    folder_path = row['Zone_Folder_Path']\n",
    "    \n",
    "    # Iterate over subfolders\n",
    "    for subfolder in subfolders:\n",
    "        csv_file_path = os.path.join(folder_path, subfolder, f\"{data_year}.csv\")\n",
    "        \n",
    "        # Check if CSV file exists\n",
    "        if os.path.exists(csv_file_path):\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            # Iterate over specified columns\n",
    "            for column in columns_to_check:\n",
    "                # Convert column values to numeric\n",
    "                df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "                \n",
    "                # Check if the value is greater than 1\n",
    "                df.loc[df[column] > 1, column] = 1\n",
    "            \n",
    "            # Write the modified DataFrame back to the CSV file\n",
    "            df.to_csv(csv_file_path, index=False)\n",
    "            \n",
    "            print(f\"Values in CSV file '{csv_file_path}' modified successfully\")\n",
    "        else:\n",
    "            print(f\"CSV file '{csv_file_path}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93045efd-3ac2-4777-a62c-d865d180902f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The final Availability Factor Time Series is for each Country modeled in Dispa-SET is located in the following local directory:\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "'/Local/Path/to/Dispas-SET/RawData/AvailabiltyFactors/'\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 4.0em; font-weight: unbold; font-size: 12px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Inside this path there are folders with the acronym of each country modelled in Dipsa-SET. i.e. AT, BE, CH.... UK\n",
    "<br>\n",
    "Inside each of this folders, there are sub folders named by the time stept of the time series. i.e. 1h, 30min and/or 15min.\n",
    "<br>\n",
    "Inside these sub folders, it is going to be found the corresponding time series .csv file named with the year of the data. e.g. 2023.csv\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "8.3 Availability Factor Files Final Location\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Copying the already processed data to the Dispa-SET data base folder.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "'/Local/Path/to/Dispas-SET/Database/AvailabiltyFactors/'\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aefdd7-0377-45ec-9831-d402797cbb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_path_4 = \"/Database/AvailabilityFactors/\"\n",
    "\n",
    "# Construct the power_plants_raw_data_folder_path variable\n",
    "availability_factor_data_base_folder_path = dispaSET_unleash_folder_path + additional_path_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa320f-91f7-4573-b64a-f5dad6d43164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_csv_files(data_year, dispaSET_codes, availability_factors_folder_path, availability_factor_data_base_folder_path):\n",
    "    \"\"\"Copies CSV files from source to destination, replacing existing files if necessary.\n",
    "\n",
    "    Args:\n",
    "        data_year (int): The year used in the file names.\n",
    "        dispaSET_codes (list): List of country codes corresponding to folders.\n",
    "        availability_factors_folder_path (str): Path to the source directory.\n",
    "        availability_factor_data_base_folder_path (str): Path to the destination directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over each country code\n",
    "    for code in dispaSET_codes:\n",
    "        source_folder = os.path.join(availability_factors_folder_path, code)\n",
    "        destination_folder = os.path.join(availability_factor_data_base_folder_path, code)\n",
    "\n",
    "        # Iterate over each subfolder (1h, 15min, 30min)\n",
    "        for subfolder in ['1h', '15min', '30min']:\n",
    "            source_subfolder = os.path.join(source_folder, subfolder)\n",
    "            destination_subfolder = os.path.join(destination_folder, subfolder)\n",
    "            \n",
    "            # Check if the source subfolder exists\n",
    "            if os.path.exists(source_subfolder):\n",
    "                # Ensure the destination subfolder exists, create it if it doesn't\n",
    "                if not os.path.exists(destination_subfolder):\n",
    "                    os.makedirs(destination_subfolder)\n",
    "                \n",
    "                # Define the CSV file name\n",
    "                csv_file_name = f\"{data_year}.csv\"\n",
    "                \n",
    "                source_file = os.path.join(source_subfolder, csv_file_name)\n",
    "                destination_file = os.path.join(destination_subfolder, csv_file_name)\n",
    "                \n",
    "                # Check if the source file exists\n",
    "                if os.path.exists(source_file):\n",
    "                    # Copy the file to the destination, replacing it if it exists\n",
    "                    shutil.copy2(source_file, destination_file)\n",
    "                    print(f\"Copied file: {source_file} to {destination_file}\")\n",
    "                else:\n",
    "                    print(f\"Source file does not exist: {source_file}\")\n",
    "            else:\n",
    "                print(f\"Source subfolder does not exist: {source_subfolder}\")\n",
    "\n",
    "# Call the function to perform the copying\n",
    "copy_csv_files(data_year, dispaSET_codes, availability_factors_folder_path, availability_factor_data_base_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b3b5b-8544-467a-ad1c-cf12c0fbc08d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color:skyblue\">\n",
    "9. Scaled Outflows File\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Creating the content folders for the scaled outflows data for each country.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The created folders are storaged in the directory.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "/local/path/to/Dispa-SET_Unleash/RawData/HydroData/ScaledInflows/ScaledOutflows\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98e6c7-e209-4435-99af-859f2a0836b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the standard time data file\n",
    "standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Create a new column 'Scaled_outflow_Folder_Path' with default values\n",
    "standard_time_data['Scaled_Outflow_Folder_Path'] = ''\n",
    "\n",
    "# Create a new column 'Availability_Factors_Folder_Path' with default values\n",
    "standard_time_data['Availability_Factors_Folder_Path'] = ''\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in standard_time_data.iterrows():\n",
    "    # Get the value of the 'Dispa-SET_Code' column\n",
    "    dispa_set_code = row['Dispa-SET_Code']\n",
    "    \n",
    "    # Create the scaled outflows folder path\n",
    "    scaled_outflows_path = os.path.join(scaled_outflows_folder_path, dispa_set_code)\n",
    "    \n",
    "    # Create the availability factors folder path\n",
    "    availability_factors_path = os.path.join(availability_factors_folder_path, dispa_set_code)\n",
    "    \n",
    "    # Create the folder for scaled outflows if it doesn't exist\n",
    "    if not os.path.exists(scaled_outflows_path):\n",
    "        os.makedirs(scaled_outflows_path)\n",
    "        print(f\"Scaled outflows folder created: {scaled_outflows_path}\")\n",
    "    \n",
    "    # Update the 'Scaled_Outflow_Folder_Path' column with the folder path\n",
    "    standard_time_data.at[index, 'Scaled_Outflow_Folder_Path'] = scaled_outflows_path\n",
    "    \n",
    "    # Update the 'Availability_Factors_Folder_Path' column with the availability factors path\n",
    "    standard_time_data.at[index, 'Availability_Factors_Folder_Path'] = availability_factors_path\n",
    "\n",
    "# Save the modified DataFrame back to the CSV file\n",
    "standard_time_data.to_csv(standard_time_data_file_path, index=False)\n",
    "\n",
    "print(\"Folder paths added to the CSV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9204b-3264-4775-aee3-099ef2c26191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Function to extract folders from a path\n",
    "def extract_folders(path):\n",
    "    if pd.notnull(path):\n",
    "        return set(os.listdir(path))\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "# Function to create folders for each path in a given column\n",
    "def create_folders_for_paths(row):\n",
    "    availability_folders = extract_folders(row['Availability_Factors_Folder_Path'])\n",
    "    for folder in ['1h', '15min', '30min']:\n",
    "        if folder in availability_folders:\n",
    "            folder_path = os.path.join(row['Scaled_Outflow_Folder_Path'], folder)\n",
    "            if not os.path.exists(folder_path):\n",
    "                os.makedirs(folder_path)\n",
    "                print(f\"Created folder: {folder_path}\")\n",
    "\n",
    "# Apply the function to each row\n",
    "df.apply(create_folders_for_paths, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2d701-613e-4862-94fa-8893dbc6d45d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Giving the corresponding headers to the csv files that will content the final data.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The Scaled Inflows Time Series have a determined header according the techology type (HPHS, HDAM), those are used to be read by Dispa-SET.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ae7c3-bb32-4599-8478-36b6484bd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_headers(data_year, standard_time_data_file_path):\n",
    "    \"\"\"\n",
    "    Replace headers in CSV files based on specified conditions without erasing the existing data.\n",
    "\n",
    "    Args:\n",
    "        data_year (str): The year string.\n",
    "        standard_time_data_file_path (str): Path to the standard time data CSV file.\n",
    "    \"\"\"\n",
    "    # Read the standard time data CSV file\n",
    "    standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "    # Define header mappings\n",
    "    header_mapping = {\n",
    "        'Hydro_Pumped_Storage_Actual_Aggregated': ['', 'HPHS'],\n",
    "        'Hydro_Water_Reservoir_Actual_Aggregated': ['', 'HDAM']\n",
    "\n",
    "    }\n",
    "\n",
    "    # Iterate over each column in the dataframe\n",
    "    for column in standard_time_data.columns:\n",
    "        if column in header_mapping:\n",
    "            # Iterate over each folder path in the column\n",
    "            for path in standard_time_data[column]:\n",
    "                # Iterate over each subfolder\n",
    "                for subfolder in ['1h', '15min', '30min']:\n",
    "                    # Construct the path to the CSV file\n",
    "                    csv_file_path = os.path.join(path, subfolder, f'{data_year}.csv')\n",
    "                    \n",
    "                    # Check if the CSV file exists\n",
    "                    if os.path.exists(csv_file_path):\n",
    "                        # Read the existing data from the CSV file\n",
    "                        df = pd.read_csv(csv_file_path)\n",
    "                        \n",
    "                        # Define the new headers\n",
    "                        headers = header_mapping[column]\n",
    "                        # Update the headers of the DataFrame\n",
    "                        new_headers = headers + list(df.columns[len(headers):])\n",
    "                        df.columns = new_headers\n",
    "                        \n",
    "                        # Write the DataFrame with new headers back to the CSV file\n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        \n",
    "                        print(f\"Headers replaced for '{csv_file_path}'\")\n",
    "\n",
    "replace_headers(data_year, standard_time_data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb2ade-f02c-4b12-a5d0-6427ac5e948d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Getting the final Scaled Ouflows Time Series file.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "All the data is copied to the single csv file in a sub-folder with the time stept inside the country folder (AT, BE, CH... etc) .\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97590d-6b2a-4a5e-b94d-39689c307b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_year = \"2023\"\n",
    "#standard_time_data_file_path = \"/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv\"\n",
    "\n",
    "# Read the CSV file to get the source file paths\n",
    "df_standard_time = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Iterate over each row of the specified columns\n",
    "for index, row in df_standard_time.iterrows():\n",
    "    # Extract the source file paths from the current row\n",
    "    file_path_1 = row['Scaled_Outflow_Folder_Path']\n",
    "    file_path_2 = row['Hydro_Pumped_Storage_Actual_Aggregated']\n",
    "    file_path_3 = row['Hydro_Water_Reservoir_Actual_Aggregated']\n",
    "    \n",
    "    # Define the subfolders to iterate over\n",
    "    subfolders = ['1h', '15min', '30min']\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        # Construct the file paths for each subfolder\n",
    "        csv_file_path_1 = os.path.join(file_path_1, subfolder, f\"{data_year}.csv\")\n",
    "        csv_file_path_2 = os.path.join(file_path_2, subfolder, f\"{data_year}.csv\")\n",
    "        csv_file_path_3 = os.path.join(file_path_3, subfolder, f\"{data_year}.csv\")\n",
    "        \n",
    "        # Check if CSV files exist in file_path_2, file_path_3\n",
    "        if os.path.exists(csv_file_path_2) and os.path.exists(csv_file_path_3):\n",
    "            \n",
    "            # Read the contents of the CSV files\n",
    "            df_2 = pd.read_csv(csv_file_path_2)\n",
    "            df_3 = pd.read_csv(csv_file_path_3)\n",
    "           \n",
    "            \n",
    "            # Combine the contents of the CSV files\n",
    "            combined_df = pd.concat([df_2.iloc[:, :2], df_3.iloc[:, 1]], axis=1)\n",
    "            \n",
    "            # Delete the destination CSV file if it exists\n",
    "            if os.path.exists(csv_file_path_1):\n",
    "                os.remove(csv_file_path_1)\n",
    "            \n",
    "            # Write the combined DataFrame to the CSV file in file_path_1\n",
    "            combined_df.to_csv(csv_file_path_1, index=False)\n",
    "            \n",
    "            print(f\"Contents from CSV files written to '{csv_file_path_1}'\")\n",
    "        else:\n",
    "            print(f\"One or more CSV files do not exist at the specified paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd40843-617e-4b57-bb1c-e05bd61fcfe7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Verifying the scaled outflow factor value.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Since the factor value has to be between 0 and 1, the same has to be verified to be in the range.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83373d7e-d674-49b6-a303-2439d9e9a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "#standard_time_data_file_path = '/home/ray/Dispa-SET_Unleash/RawData/AvailabiltyFactors/Standard_Time_Data.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "standard_time_data = pd.read_csv(standard_time_data_file_path)\n",
    "\n",
    "# Assuming data_year is already defined\n",
    "#data_year = \"2023\"\n",
    "\n",
    "# Columns to check\n",
    "columns_to_check = ['HPHS', 'HDAM']\n",
    "\n",
    "# List of subfolders to check\n",
    "subfolders = ['1h', '15min', '30min']\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in standard_time_data.iterrows():\n",
    "    # Get the folder path from the 'Zone_Folder_Path' column\n",
    "    folder_path = row['Scaled_Outflow_Folder_Path']\n",
    "    \n",
    "    # Iterate over subfolders\n",
    "    for subfolder in subfolders:\n",
    "        csv_file_path = os.path.join(folder_path, subfolder, f\"{data_year}.csv\")\n",
    "        \n",
    "        # Check if CSV file exists\n",
    "        if os.path.exists(csv_file_path):\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            # Iterate over specified columns\n",
    "            for column in columns_to_check:\n",
    "                # Convert column values to numeric\n",
    "                df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "                \n",
    "                # Check if the value is greater than 1\n",
    "                df.loc[df[column] > 1, column] = 1\n",
    "            \n",
    "            # Write the modified DataFrame back to the CSV file\n",
    "            df.to_csv(csv_file_path, index=False)\n",
    "            \n",
    "            print(f\"Values in CSV file '{csv_file_path}' modified successfully\")\n",
    "        else:\n",
    "            print(f\"CSV file '{csv_file_path}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee6d40-7ad2-4821-991b-a63986370921",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The final Scaled Outflows Time Series is for each Country modeled in Dispa-SET is located in the following local directory:\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "'/Local/Path/to/Dispas-SET/RawData/HydroData/ScaledInflows/ScaledOutFlows/'\n",
    "<br>\n",
    "Inside this path there are folders with the acronym of each country modelled in Dipsa-SET. i.e. AT, BE, CH.... UK\n",
    "<br>\n",
    "Inside each of this folders, there are sub folders named by the time stept of the time series. i.e. 1h, 30min and/or 15min.\n",
    "<br>\n",
    "Inside these sub folders, it is going to be found the corresponding time series .csv file named with the year of the data. e.g. 2023.csv\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Additionally, the total installed capacity per production type file is also requiered to format the scaled inflows time series.\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 2.0em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman; color:skyblue\">\n",
    "The corresponding files that content this data are going to be copied to the scalled inflows folders.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Defining the destination folders to the total hydro installed capacity data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e7683-eb23-4de0-ab29-b4269a79c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_path_7 = \"/RawData/HydroData/ScaledInflows/Total_Hydro_Installed_Capacity_per_Production_Type/\"\n",
    "\n",
    "# Construct the total_hydro_installed_capacity_folder_path variable\n",
    "total_hydro_installed_capacity_folder_path = dispaSET_unleash_folder_path + additional_path_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd8b52-c1c7-44e4-8ad5-c17f46a4785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_hydro_installed_capacity_folder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef874c9-ccb5-4aca-bdc4-8fe16a0da19d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Copying the needed data to the destination folders.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153561ce-9db7-46eb-bfd2-4266e52a2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hydro_data(dispaSET_codes, availability_factors_folder_path, total_hydro_installed_capacity_folder_path, data_year):\n",
    "  \"\"\"\n",
    "  Extracts hydro data from CSV files and saves to new files.\n",
    "\n",
    "  Args:\n",
    "    dispaSET_codes: A list of dispaSET codes.\n",
    "    availability_factors_folder_path: The path to the availability factors folder.\n",
    "    total_hydro_installed_capacity_folder_path: The path to the output folder.\n",
    "    data_year: The data year.\n",
    "  \"\"\"\n",
    "\n",
    "  for code in dispaSET_codes:\n",
    "    input_file = os.path.join(availability_factors_folder_path, code, f\"{data_year}_Total_Installed_Capacity_per_Production_Type.csv\")\n",
    "    output_folder = os.path.join(total_hydro_installed_capacity_folder_path, code)\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "      os.makedirs(output_folder)\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "    hydro_data = df[df['Production_Type'].isin(['Hydro_Water_Reservoir', 'Hydro_Pumped_Storage'])]\n",
    "    output_file = os.path.join(output_folder, f\"{data_year}_Total_Hydro_Installed_Capacity_per_Production_Type.csv\")\n",
    "    hydro_data.to_csv(output_file, index=False)\n",
    "    print(f\"Extracted hydro data from {input_file} to {output_file}\")\n",
    "\n",
    "extract_hydro_data(dispaSET_codes, availability_factors_folder_path, total_hydro_installed_capacity_folder_path, data_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79d9d0-b007-48cd-9a8b-63bddeb5814c",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 17px; font-family: TimesNewRoman; color:skyblue\">\n",
    "10. Availability Factors Folder Back Up\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "Once all the formating process was done the Availability Factor Folder is restored to its defoult state.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "328600fc-92a5-4b96-bb20-5be1bff117d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory restored to original state from /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors_backup/\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(availability_factors_folder_path):\n",
    "    shutil.rmtree(availability_factors_folder_path)  # Remove the current directory\n",
    "shutil.copytree(backup_folder_path, availability_factors_folder_path)\n",
    "\n",
    "print(f\"Directory restored to original state from {backup_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93878cc7-8d15-47ad-a940-26a4932344b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backup folder /home/ray/Dispa-SET_Unleash/RawData/AvailabilityFactors_backup/ deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(backup_folder_path)\n",
    "print(f\"Backup folder {backup_folder_path} deleted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb064bd2-a04d-4f4e-8eea-7eab4cbf4c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
