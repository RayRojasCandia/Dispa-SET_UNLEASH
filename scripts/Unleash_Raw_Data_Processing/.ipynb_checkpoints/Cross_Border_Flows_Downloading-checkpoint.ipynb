{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1b304a-dd34-4e3e-ac28-5f54db43a844",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 20px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    TIME SERIES DATA PROCESSING\n",
    "    <br>\n",
    "    CROSS BORDER FLOWS\n",
    "<div style=\"text-align: center; margin-left: 0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color: skyblue\">\n",
    "    Raw Data Downloading Notebook\n",
    "</div>\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color: skyblue\">\n",
    "Each part of the following script was used to download the raw data for the Cross Border Flows Time Series Raw Data for all the european countries of the Dispa-SET_Unleash project.\n",
    "<br>\n",
    "Read explanation text cells to follow and understand all the process until final results were got stept by step.\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"text-align: justify; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    1. Notebook Set Up\n",
    "</div>\n",
    "<div style=\"text-align: justify; margin-left: 0.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman; color:skyblue\">\n",
    "    Importing needed libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a590ae9c-56f6-4dbf-a4f4-9f39b4fa21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from entsoe import EntsoePandasClient\n",
    "from entsoe.exceptions import NoMatchingDataError  # Ensure this line is included\n",
    "from entsoe.exceptions import NoMatchingDataError, InvalidBusinessParameterError  # Import relevant exceptions\n",
    "import os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone, utc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de47724-25dd-456b-90ed-5b0f0f73cbb5",
   "metadata": {},
   "source": [
    " <div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    2. ENTSO-E RESTful API.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Connecting with the ENTSO-E API Tool.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- To donwload the neeeded data using the API tool, is mandatory to use a token autentication to connect and make the future request of data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6899aaf-90da-463a-b5e6-6092997b2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = EntsoePandasClient(api_key='61e5bbbb-7e80-4540-a471-bd993873aa74')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd2e76f-2442-4fdd-b2db-b3b71f3f544b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    3. Dispa-SET_Unleash Folder Path\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "    Determinning dynamically the zone_folder_path based on the location of the \"Dispa-SET_Unleash\" folder relative to the current working directory.\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- If the \"Dispa-SET_Unleash\" folder is copied to a different machine or location, the dispaSET_unleash_folder_path variable will automatically adjust accordingly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8926675d-6398-4db7-80cd-42c829b7b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name: Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path: /home/ray/Dispa-SET_Unleash\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory of \"Dispa-SET_Unleash\"\n",
    "dispaSET_unleash_parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Get the path to the \"Dispa-SET_Unleash\" folder\n",
    "dispaSET_unleash_folder_path = os.path.dirname(dispaSET_unleash_parent_directory)\n",
    "\n",
    "# Construct the dispaSET_unleash_folder_name variable\n",
    "dispaSET_unleash_folder_name = os.path.basename(dispaSET_unleash_folder_path)\n",
    "\n",
    "print(\"dispaSET_unleash_folder_name:\", dispaSET_unleash_folder_name)\n",
    "print(\"dispaSET_unleash_folder_path:\", dispaSET_unleash_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38b8cd-b666-4738-9813-f1fbeb216ab7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    4. Usefull Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Entering a value to all the variables which content are going to be used in some of the next stages of this script. \n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- Indicate the year of all data is referring to in the variable data_year.\n",
    "<br>\n",
    "- The universal_standar_time variable is going to be used to download all the time series data in this horary zone. Additionally as each european country belongs a particular time sector the corresponding time series data related to its time sector are going to be downloaded as well but in a different file.\n",
    "<br>\n",
    "- Additionally there are some default parameters that has to be defined to the correct working and calling to the ENTSO-E downloading functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17cff013-7efc-43b0-a572-8f32f1f9a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year to which data refers to:\n",
    "data_year = 2017\n",
    "data_year_1 = data_year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe94548-6223-4ace-9793-b857b53e2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_border_flows_folder_path: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/\n",
      "cross_border_flows_raw_data_folder_path: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/\n"
     ]
    }
   ],
   "source": [
    "# Additional string to be appended\n",
    "additional_path = \"/RawData/CrossBorderFlows/\"\n",
    "additional_path_1 = \"/RawData/CrossBorderFlows/Raw_Data_Sources/\"\n",
    "\n",
    "# Construct the Outage_Factors_folder_path variable\n",
    "cross_border_flows_folder_path = dispaSET_unleash_folder_path + additional_path\n",
    "\n",
    "# Construct the Outage_Factors_Raw_Data_folder_path variable\n",
    "cross_border_flows_raw_data_folder_path = dispaSET_unleash_folder_path + additional_path_1\n",
    "\n",
    "print(\"cross_border_flows_folder_path:\", cross_border_flows_folder_path)\n",
    "print(\"cross_border_flows_raw_data_folder_path:\", cross_border_flows_raw_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab19470a-140b-4a86-a0c9-32f845c3497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other parameters\n",
    "type_marketagreement_type = 'A01'\n",
    "contract_marketagreement_type = \"A01\"\n",
    "process_type = 'A51'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610c8df-68ce-40aa-af2e-d67c3b2df7a0",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    5. Country List Variable Definition\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Defining the list of countries according to the available data. \n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- Just those countries that interchange flows with other countries different of the ones modelled in Dispa-SET are defined in the list.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9485328c-cd02-43c9-a78d-ad5c22cff2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of country codes\n",
    "cross_border_flows_per_unit_country_list = [\"AL\", \"BA\", \"BG\", \"BY\", \"EE\", \"GR\", \"FI\", \"HR\", \"HU\", \"IT\", \"LT\", \"LV\",\n",
    "                                            \"MD\", \"ME\", \"MK\", \"MT\", \"PL\", \"RO\", \"RU\", \"RS\", \"SK\", \"TR\", \"UA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f959e45-e5f2-4856-831d-454684b52816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/country_list.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define the directory and file path\n",
    "file_name = 'country_list.csv'\n",
    "file_path = os.path.join(cross_border_flows_raw_data_folder_path, file_name)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(cross_border_flows_raw_data_folder_path, exist_ok=True)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(cross_border_flows_per_unit_country_list, columns=['Country_From'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame saved to '{file_path}'\")\n",
    "cross_border_flows_country_list_file = file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e96e3b-c1f7-4dd1-b455-107f8f37d533",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right; margin-left: 3.0em; font-weight: unbold; font-size: 14px; font-family: TimesNewRoman;\">\n",
    "    Tracking Variables. \n",
    "    <br>\n",
    "    <div style=\"text-align: right; margin-left: 1.50em; font-weight: unbold; font-size: 13px; font-family: TimesNewRoman;\">\n",
    "    This cells are just to confirm all the file names, file paths and other information related to the data being processed.\n",
    "    <br>\n",
    "  Also are used to ensure the inputs for next cells in order to avoid to re-enter the same information each time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d3ed6b-4f59-4092-a51c-e1e71c833e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispaSET_unleash_folder_name:                              Dispa-SET_Unleash\n",
      "dispaSET_unleash_folder_path:                              /home/ray/Dispa-SET_Unleash\n",
      "data_year:                                                 2017\n",
      "cross_border_flows_folder_path:                            /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/\n",
      "cross_border_flows_raw_data_folder_path:                   /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/\n",
      "cross_border_flows_country_list_file:                      /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/country_list.csv\n"
     ]
    }
   ],
   "source": [
    "print (f\"dispaSET_unleash_folder_name:                              {dispaSET_unleash_folder_name}\")\n",
    "print (f\"dispaSET_unleash_folder_path:                              {dispaSET_unleash_folder_path}\")\n",
    "print (f\"data_year:                                                 {data_year}\")\n",
    "print (f\"cross_border_flows_folder_path:                            {cross_border_flows_folder_path}\")   \n",
    "print (f\"cross_border_flows_raw_data_folder_path:                   {cross_border_flows_raw_data_folder_path}\")\n",
    "print (f\"cross_border_flows_country_list_file:                      {cross_border_flows_country_list_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88b8d7-8432-4155-a9c7-c1731043ee24",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Defining the sub-folders where all the cross border flows raw data is saved. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f38fa4-bfc9-44fd-862e-18fecb61998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/country_list.csv' with new subfolders created.\n"
     ]
    }
   ],
   "source": [
    "# Convert data_year to string if it's not already\n",
    "data_year = str(data_year)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the column 'Country_From' exists\n",
    "if 'Country_From' not in df.columns:\n",
    "    raise ValueError(\"Column 'Country_From' does not exist in the CSV file\")\n",
    "\n",
    "# Define the base directory where subfolders will be created\n",
    "base_directory = os.path.join(cross_border_flows_raw_data_folder_path, data_year)\n",
    "\n",
    "# Create a list to hold the paths of the created subfolders\n",
    "country_folder_paths = []\n",
    "\n",
    "# Create subfolders and save their paths\n",
    "for country in df['Country_From']:\n",
    "    # Create the subfolder path\n",
    "    subfolder_path = os.path.join(base_directory, country)\n",
    "    \n",
    "    # Create the subfolder if it doesn't exist\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    \n",
    "    # Append the subfolder path to the list\n",
    "    country_folder_paths.append(subfolder_path)\n",
    "\n",
    "# Add the new column 'Country_Folder' to the DataFrame\n",
    "df['Country_Folder'] = country_folder_paths\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(cross_border_flows_country_list_file, index=False)\n",
    "\n",
    "print(f\"Updated CSV file saved to '{cross_border_flows_country_list_file}' with new subfolders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c69f72-aa17-4308-8525-710e0c91ae83",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Defining the neighbor countries. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c06590d-9da6-40fa-b17c-745801e846b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appended to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/country_list.csv' as new columns\n"
     ]
    }
   ],
   "source": [
    "# Raw data as a multiline string\n",
    "data = \"\"\"\n",
    "GR\n",
    "HR\n",
    "MK, RS, TR\n",
    "LT\n",
    "RU\n",
    "AL, MK, TR\n",
    "RU\n",
    "BA, RS\n",
    "RS, UA\n",
    "MT, ME\n",
    "BY, RU\n",
    "RU\n",
    "RO\n",
    "IT\n",
    "BG, GR\n",
    "IT\n",
    "UA\n",
    "MD, RS, UA\n",
    "EE, FI, LV, LT\n",
    "BG, HR, HU, RO\n",
    "UA\n",
    "BG, GR\n",
    "HU, PL, RO, SK\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into lines\n",
    "lines = data.strip().split(\"\\n\")\n",
    "\n",
    "# Initialize a list to hold the data\n",
    "data_list = []\n",
    "\n",
    "# Process each line\n",
    "for line in lines:\n",
    "    if line.strip() == \"\":\n",
    "        # If the line is empty, add 11 empty strings\n",
    "        data_list.append([\"\"] * 4)\n",
    "    else:\n",
    "        neighbors = line.split(\", \")\n",
    "        data_list.append(neighbors)\n",
    "\n",
    "# Create the DataFrame for new data\n",
    "new_df = pd.DataFrame(data_list, columns=[f\"Neighbor_{i}\" for i in range(1, 5)])\n",
    "\n",
    "# Path to the existing CSV file\n",
    "existing_csv_file_path = cross_border_flows_country_list_file\n",
    "\n",
    "# Read the existing CSV file into a DataFrame\n",
    "existing_df = pd.read_csv(existing_csv_file_path)\n",
    "\n",
    "# Concatenate the existing DataFrame with the new DataFrame horizontally\n",
    "combined_df = pd.concat([existing_df, new_df], axis=1)\n",
    "\n",
    "# Save the combined DataFrame back to the CSV file\n",
    "combined_df.to_csv(existing_csv_file_path, index=False)\n",
    "print(f\"Data appended to '{existing_csv_file_path}' as new columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51abf96-5094-435e-948f-5e1ea885518f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    6. Raw Data Download\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Donwloading the cross border flows raw data. \n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 2.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "- The crows border flow data is downloaded in separate files for each country.\n",
    "<br>\n",
    "- Since the Acronym of Grece in the downloaded data is 'GR' and the Dispa-SET format for the country is 'EL'. All the needed changes in the used variables are done.\n",
    "<br>\n",
    "- The downloaded files will be joined into a single csv file under the name of the country which the flow comes from.\n",
    "<br>\n",
    "- The headers of these joined csv files are changed accordign the Dispa-SET cross border flow data format e.g. BE -> DE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7a37faa-8f4e-4cf6-98af-f10f1bab227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data for AL to GR saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/GR.csv'\n",
      "Data for BA to HR saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/HR.csv'\n",
      "Data for BG to MK saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/MK.csv'\n",
      "Data for BG to RS saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/RS.csv'\n",
      "Data for BG to TR saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/TR.csv'\n",
      "Data for BY to LT saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/LT.csv'\n",
      "Data for EE to RU saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/RU.csv'\n",
      "Data for GR to AL saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/GR/AL.csv'\n",
      "Data for GR to MK saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/GR/MK.csv'\n",
      "Data for GR to TR saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/GR/TR.csv'\n",
      "Data for FI to RU saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/RU.csv'\n",
      "Data for HR to BA saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/BA.csv'\n",
      "Data for HR to RS saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/RS.csv'\n",
      "Data for HU to RS saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/RS.csv'\n",
      "Data for HU to UA saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/UA.csv'\n",
      "Data for IT to MT saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/MT.csv'\n",
      "No matching data for IT to ME for the period 2017-01-01 00:00:00+01:00 to 2018-01-01 00:00:00+01:00. Skipping.\n",
      "Data for LT to BY saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/BY.csv'\n",
      "Invalid business parameter for LT to RU. This pair may not be supported by the API.\n",
      "Data for LV to RU saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/RU.csv'\n",
      "No matching data for MD to RO for the period 2017-01-01 00:00:00+01:00 to 2018-01-01 00:00:00+01:00. Skipping.\n",
      "No matching data for ME to IT for the period 2017-01-01 00:00:00+01:00 to 2018-01-01 00:00:00+01:00. Skipping.\n",
      "Data for MK to BG saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/BG.csv'\n",
      "Data for MK to GR saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/GR.csv'\n",
      "Data for MT to IT saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/IT.csv'\n",
      "Data for PL to UA saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/UA.csv'\n",
      "No matching data for RO to MD for the period 2017-01-01 00:00:00+01:00 to 2018-01-01 00:00:00+01:00. Skipping.\n",
      "Data for RO to RS saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RS.csv'\n",
      "Data for RO to UA saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/UA.csv'\n",
      "Data for RU to EE saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/EE.csv'\n",
      "Data for RU to FI saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/FI.csv'\n",
      "Data for RU to LV saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/LV.csv'\n",
      "Invalid business parameter for RU to LT. This pair may not be supported by the API.\n",
      "Data for RS to BG saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/BG.csv'\n",
      "Data for RS to HR saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/HR.csv'\n",
      "Data for RS to HU saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/HU.csv'\n",
      "Data for RS to RO saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RO.csv'\n",
      "Data for SK to UA saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/UA.csv'\n",
      "Data for TR to BG saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/BG.csv'\n",
      "Data for TR to GR saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/GR.csv'\n",
      "Data for UA to HU saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/HU.csv'\n",
      "Data for UA to PL saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/PL.csv'\n",
      "Data for UA to RO saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/RO.csv'\n",
      "Data for UA to SK saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/SK.csv'\n",
      "All data has been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the start and end timestamps using the data_year variable\n",
    "start = pd.Timestamp(f'{data_year}0101', tz='Europe/Brussels')\n",
    "end = pd.Timestamp(f'{data_year_1}0101', tz='Europe/Brussels')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the necessary columns exist\n",
    "required_columns = ['Country_From', 'Country_Folder'] + [f'Neighbor_{i}' for i in range(1, 5)]\n",
    "for col in required_columns:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"Column '{col}' does not exist in the CSV file\")\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    country_code_from = row['Country_From']\n",
    "    country_folder = row['Country_Folder']\n",
    "    \n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(country_folder, exist_ok=True)\n",
    "    \n",
    "    # Iterate through each neighbor column\n",
    "    for neighbor_col in [f'Neighbor_{i}' for i in range(1, 5)]:\n",
    "        country_code_to = row[neighbor_col]\n",
    "        \n",
    "        # Skip if the neighbor field is empty\n",
    "        if pd.isna(country_code_to):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Query crossborder flows\n",
    "            net_transfer = client.query_crossborder_flows(\n",
    "                country_code_from, country_code_to, start=start, end=end\n",
    "            )\n",
    "            \n",
    "            # Convert the index to a column\n",
    "            net_transfer = net_transfer.reset_index()\n",
    "            \n",
    "            # Define the output file path\n",
    "            output_file = os.path.join(country_folder, f'{country_code_to}.csv')\n",
    "            \n",
    "            # Save the DataFrame to a CSV file, including the index as a column\n",
    "            net_transfer.to_csv(output_file, index=False)\n",
    "\n",
    "            print(f\"Data for {country_code_from} to {country_code_to} saved to '{output_file}'\")\n",
    "\n",
    "        except InvalidBusinessParameterError:\n",
    "            print(f\"Invalid business parameter for {country_code_from} to {country_code_to}. This pair may not be supported by the API.\")\n",
    "        \n",
    "        except NoMatchingDataError:\n",
    "            print(f\"No matching data for {country_code_from} to {country_code_to} for the period {start} to {end}. Skipping.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing data for {country_code_from} to {country_code_to}: {e}\")\n",
    "\n",
    "print(\"All data has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ff7d655-1f47-4729-ac12-9550c08e5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacements made and file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/country_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1100939/4086083268.py:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.replace('GR', 'EL') if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Replace 'GR' with 'EL' in the entire DataFrame\n",
    "df = df.applymap(lambda x: x.replace('GR', 'EL') if isinstance(x, str) else x)\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df.to_csv(cross_border_flows_country_list_file, index=False)\n",
    "\n",
    "print(f\"Replacements made and file saved: {cross_border_flows_country_list_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f06c431d-6d5f-464e-914b-5a4116440fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed file: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/GR.csv to /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/EL.csv\n",
      "Renamed file: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/GR.csv to /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/EL.csv\n",
      "Renamed file: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/GR.csv to /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/EL.csv\n",
      "Renamed directory: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/GR to /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL\n"
     ]
    }
   ],
   "source": [
    "# Walk through the directory tree\n",
    "for root, dirs, files in os.walk(cross_border_flows_raw_data_folder_path, topdown=False):\n",
    "    # Rename files\n",
    "    for name in files:\n",
    "        if 'GR' in name:\n",
    "            new_name = name.replace('GR', 'EL')\n",
    "            old_file_path = os.path.join(root, name)\n",
    "            new_file_path = os.path.join(root, new_name)\n",
    "            os.rename(old_file_path, new_file_path)\n",
    "            print(f\"Renamed file: {old_file_path} to {new_file_path}\")\n",
    "\n",
    "    # Rename directories\n",
    "    for name in dirs:\n",
    "        if 'GR' in name:\n",
    "            new_name = name.replace('GR', 'EL')\n",
    "            old_dir_path = os.path.join(root, name)\n",
    "            new_dir_path = os.path.join(root, new_name)\n",
    "            os.rename(old_dir_path, new_dir_path)\n",
    "            print(f\"Renamed directory: {old_dir_path} to {new_dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3884da9-281b-4848-ae8c-1b1755b1ec08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data list: ['\\n', 'G', 'R', '\\n', 'H', 'R', '\\n', 'M', 'K', ',', ' ', 'R', 'S', ',', ' ', 'T', 'R', '\\n', 'L', 'T', '\\n', 'R', 'U', '\\n', 'A', 'L', ',', ' ', 'M', 'K', ',', ' ', 'T', 'R', '\\n', 'R', 'U', '\\n', 'B', 'A', ',', ' ', 'R', 'S', '\\n', 'R', 'S', ',', ' ', 'U', 'A', '\\n', 'M', 'T', ',', ' ', 'M', 'E', '\\n', 'B', 'Y', ',', ' ', 'R', 'U', '\\n', 'R', 'U', '\\n', 'R', 'O', '\\n', 'I', 'T', '\\n', 'B', 'G', ',', ' ', 'G', 'R', '\\n', 'I', 'T', '\\n', 'U', 'A', '\\n', 'M', 'D', ',', ' ', 'R', 'S', ',', ' ', 'U', 'A', '\\n', 'E', 'E', ',', ' ', 'F', 'I', ',', ' ', 'L', 'V', ',', ' ', 'L', 'T', '\\n', 'B', 'G', ',', ' ', 'H', 'R', ',', ' ', 'H', 'U', ',', ' ', 'R', 'O', '\\n', 'U', 'A', '\\n', 'B', 'G', ',', ' ', 'G', 'R', '\\n', 'H', 'U', ',', ' ', 'P', 'L', ',', ' ', 'R', 'O', ',', ' ', 'S', 'K', '\\n']\n",
      "Updated cross_border_flows_per_unit_country_list: ['AL', 'BA', 'BG', 'BY', 'EE', 'EL', 'FI', 'HR', 'HU', 'IT', 'LT', 'LV', 'MD', 'ME', 'MK', 'MT', 'PL', 'RO', 'RU', 'RS', 'SK', 'TR', 'UA']\n"
     ]
    }
   ],
   "source": [
    "# Function to replace 'GR' with 'EL'\n",
    "def replace_gr_with_el(lst):\n",
    "    return ['EL' if x == 'GR' else x for x in lst]\n",
    "\n",
    "# Applying the function to both lists\n",
    "data = replace_gr_with_el(data)\n",
    "cross_border_flows_per_unit_country_list = replace_gr_with_el(cross_border_flows_per_unit_country_list)\n",
    "\n",
    "# Print the updated lists\n",
    "print(\"Updated data list:\", data)\n",
    "print(\"Updated cross_border_flows_per_unit_country_list:\", cross_border_flows_per_unit_country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8835c832-e4be-4216-a857-c826dfce40d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/AL.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/BA.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/BG.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/BY.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/EE.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL/EL.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/FI.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/HR.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/HU.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/IT.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/LT.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/LV.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/MK.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/MT.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/PL.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RO.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/RU.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RS.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/SK.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/TR.csv'\n",
      "Joined CSV file saved to '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/UA.csv'\n",
      "All data has been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the column 'Country_Folder' exists\n",
    "if 'Country_Folder' not in df.columns:\n",
    "    raise ValueError(\"Column 'Country_Folder' does not exist in the CSV file\")\n",
    "\n",
    "# Function to join CSV files in a directory\n",
    "def join_csv_files_in_directory(directory_path):\n",
    "    csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        return None\n",
    "    \n",
    "    # Read all CSV files into DataFrames\n",
    "    dataframes = {csv_file: pd.read_csv(os.path.join(directory_path, csv_file)) for csv_file in csv_files}\n",
    "    \n",
    "    # Find the CSV file with the largest number of rows\n",
    "    largest_file = max(dataframes, key=lambda x: len(dataframes[x]))\n",
    "    base_df = dataframes[largest_file].iloc[:, :2].copy()\n",
    "    base_df.columns = [base_df.columns[0], largest_file.replace('.csv', '')]\n",
    "    \n",
    "    # Merge the other CSV files based on the first column\n",
    "    for csv_file, df in dataframes.items():\n",
    "        if csv_file == largest_file:\n",
    "            continue\n",
    "        temp_df = df.iloc[:, [0, 1]]\n",
    "        temp_df.columns = [temp_df.columns[0], csv_file.replace('.csv', '')]\n",
    "        base_df = pd.merge(base_df, temp_df, on=base_df.columns[0], how='left')\n",
    "    \n",
    "    return base_df\n",
    "\n",
    "# Create a new column for the paths of the new CSV files\n",
    "df['Country_File_Path'] = ''\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    country_folder = row['Country_Folder']\n",
    "    \n",
    "    # Join CSV files in the directory\n",
    "    joined_df = join_csv_files_in_directory(country_folder)\n",
    "    \n",
    "    if joined_df is not None:\n",
    "        # Define the output file path\n",
    "        output_file = os.path.join(country_folder, f\"{os.path.basename(country_folder)}.csv\")\n",
    "        \n",
    "        # Save the joined DataFrame to a new CSV file\n",
    "        joined_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Update the DataFrame with the path of the new CSV file\n",
    "        df.at[index, 'Country_File_Path'] = output_file\n",
    "\n",
    "        print(f\"Joined CSV file saved to '{output_file}'\")\n",
    "\n",
    "# Save the updated DataFrame back to the main CSV file\n",
    "df.to_csv(cross_border_flows_country_list_file, index=False)\n",
    "\n",
    "print(\"All data has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d48b0461-7364-4b74-93f3-0a396a13b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/AL.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/BA.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/BG.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/BY.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/EE.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL/EL.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/FI.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/HR.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/HU.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/IT.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/LT.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/LV.csv'\n",
      "File path 'nan' does not exist or is empty. Skipping...\n",
      "File path 'nan' does not exist or is empty. Skipping...\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/MK.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/MT.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/PL.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RO.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/RU.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RS.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/SK.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/TR.csv'\n",
      "Updated headers in '/home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/UA.csv'\n",
      "All CSV files have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Read the main CSV file into a DataFrame\n",
    "df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the required columns exist\n",
    "if 'Country_From' not in df.columns or 'Country_File_Path' not in df.columns:\n",
    "    raise ValueError(\"The CSV file must contain 'Country_From' and 'Country_File_Path' columns.\")\n",
    "\n",
    "# Function to update the headers of a CSV file\n",
    "def update_csv_headers(file_path, new_header_prefix):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    csv_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get the current headers\n",
    "    current_headers = csv_df.columns.tolist()\n",
    "    \n",
    "    # Create new headers for columns from the second column onward\n",
    "    new_headers = [current_headers[0]] + [f\"{new_header_prefix} -> {col}\" for col in current_headers[1:]]\n",
    "    \n",
    "    # Update the DataFrame with the new headers\n",
    "    csv_df.columns = new_headers\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    csv_df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated headers in '{file_path}'\")\n",
    "\n",
    "# Iterate through each row in the main DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    country_from = row['Country_From']\n",
    "    country_file_path = row['Country_File_Path']\n",
    "    \n",
    "    # Check if the file path is not empty and exists\n",
    "    if pd.notna(country_file_path) and os.path.exists(country_file_path):\n",
    "        update_csv_headers(country_file_path, country_from)\n",
    "    else:\n",
    "        print(f\"File path '{country_file_path}' does not exist or is empty. Skipping...\")\n",
    "\n",
    "print(\"All CSV files have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2388e2-d408-4537-9a1b-b773582b3177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8387e-a77f-4a4a-a681-5cfd03211b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b97c9-954f-47fd-a4fb-89973ec1d082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10f3a7-c1ee-41ce-9242-25253c562783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089bc7f7-b14c-414f-a666-d9bdbee60d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831a3b25-9f7f-4e7a-a1aa-2505b3d5e854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c29dc0-4729-46a5-8032-8687e2a467e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0f883-4164-4b87-a743-255083036192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0d22b-29d8-4348-a825-5876f8c67b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102d99d-6318-4f6e-bef5-1f645b8f438d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77931856-79e9-4f89-924c-fa2b029546f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707eb6e-f7e3-4760-9ad3-b3e8a77fdfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80667de7-f250-40b5-a1ab-ed3f2d57628a",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    7. Raw Data Format\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Addapting the time step data to the UTC for all the countries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7963855-5bd9-4488-b5f5-40de4a3b4182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/AL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/BA.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/BG.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/BY.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/EE.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL/EL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/FI.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/HR.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/HU.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/IT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/LT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/LV.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/MK.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/MT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/PL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RO.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/RU.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RS.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/SK.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/TR.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/UA.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Define the function to convert time to UTC\n",
    "def convert_to_utc(time_str):\n",
    "    local_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S%z')\n",
    "    utc_time = local_time.astimezone(pytz.utc)\n",
    "    return utc_time.strftime('%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# Process each CSV file\n",
    "for file_path in country_list_df['Country_File_Path'].dropna():\n",
    "    # Ensure the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the 'index' column exists\n",
    "    if 'index' not in df.columns:\n",
    "        print(f\"'index' column not found in file: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    # Convert the 'index' column to UTC\n",
    "    df['index'] = df['index'].apply(convert_to_utc)\n",
    "    \n",
    "    # Save the updated CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d670d7e-6222-4fb1-baf0-eb038853da8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/AL/AL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BA/BA.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BG/BG.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/BY/BY.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EE/EE.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/EL/EL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/FI/FI.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HR/HR.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/HU/HU.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/IT/IT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LT/LT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/LV/LV.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MK/MK.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/MT/MT.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/PL/PL.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RO/RO.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RU/RU.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/RS/RS.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/SK/SK.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/TR/TR.csv\n",
      "Updated file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/Raw_Data_Sources/2017/UA/UA.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Function to update the year in the 'index' column\n",
    "def update_index_year(df, data_year):\n",
    "    # Ensure the 'index' column exists\n",
    "    if 'index' not in df.columns:\n",
    "        raise ValueError(\"'index' column not found in DataFrame\")\n",
    "    \n",
    "    # Update the year in the 'index' column\n",
    "    df['index'] = df['index'].apply(lambda x: f\"{data_year}{x[4:]}\" if str(x)[:4] != str(data_year) else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each CSV file specified in the 'Country_File_Path' column\n",
    "for file_path in country_list_df['Country_File_Path'].dropna():\n",
    "    # Ensure the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure there are enough rows to move the first four rows to the last\n",
    "    if len(df) < 4:\n",
    "        print(f\"Not enough rows to process in file: {file_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract the first four rows (excluding headers)\n",
    "    first_four_rows = df.iloc[:4].copy()\n",
    "    \n",
    "    # Drop the first four rows from the DataFrame\n",
    "    df = df.iloc[4:].reset_index(drop=True)\n",
    "    \n",
    "    # Append the first_four_rows to the end of the DataFrame\n",
    "    df = pd.concat([df, first_four_rows]).reset_index(drop=True)\n",
    "    \n",
    "    # Update the 'index' column year\n",
    "    df = update_index_year(df, data_year)\n",
    "    \n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Updated file saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de12470b-5500-4767-860b-76dff3017dc1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 3.0em; font-weight: bold; font-size: 18px; font-family: TimesNewRoman;\">\n",
    "    7. Cross Border Flows Clean File\n",
    "</div>\n",
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Joining all the cros border flows data to a single csv file with named as the analized year.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db2fd6f2-ef2c-4b13-9031-7a147b12f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the country list CSV file\n",
    "country_list_df = pd.read_csv(cross_border_flows_country_list_file)\n",
    "\n",
    "# Ensure the 'Country_File_Path' column exists\n",
    "if 'Country_File_Path' not in country_list_df.columns:\n",
    "    raise ValueError(\"Column 'Country_File_Path' does not exist in the CSV file\")\n",
    "\n",
    "# Process each CSV file specified in the 'Country_File_Path' column\n",
    "file_paths = country_list_df['Country_File_Path'].dropna().tolist()\n",
    "\n",
    "# Identify the CSV file with the largest number of rows\n",
    "max_rows = 0\n",
    "base_df = None\n",
    "for file_path in file_paths:\n",
    "    # Ensure the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > max_rows:\n",
    "            max_rows = len(df)\n",
    "            base_df = df.copy()\n",
    "\n",
    "# If no base_df was found, raise an error\n",
    "if base_df is None:\n",
    "    raise ValueError(\"No valid CSV files found.\")\n",
    "\n",
    "# Initialize the combined DataFrame with the first column from the base DataFrame\n",
    "combined_df = pd.DataFrame(base_df.iloc[:, 0])\n",
    "combined_df.columns = [base_df.columns[0]]  # Keep the original name of the first column\n",
    "\n",
    "# Add data from each CSV file to the combined DataFrame\n",
    "for file_path in file_paths:\n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Merge the data based on the first column\n",
    "        combined_df = pd.merge(combined_df, df, on=base_df.columns[0], how='left')\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file named after the data_year variable\n",
    "output_file_path = os.path.join(cross_border_flows_folder_path, f\"{data_year}.csv\")\n",
    "combined_df.to_csv(output_file_path, index=False)\n",
    "print(f\"Combined CSV file saved: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6caaf-602e-40a1-b46a-ae1620b6d9fc",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Dividing the clean data in time stepts of 15 minutes, 30 minutes, and 1 hour.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff2aa826-0a03-4dd0-a746-53771e73cf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/1h/2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1100939/2621242742.py:17: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  resampled_df = df.set_index('index').resample(interval).first().reset_index()\n",
      "/tmp/ipykernel_1100939/2621242742.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  resampled_df = df.set_index('index').resample(interval).first().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/30min/2017.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1100939/2621242742.py:17: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  resampled_df = df.set_index('index').resample(interval).first().reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved: /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/15min/2017.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = os.path.join(cross_border_flows_folder_path, f'{data_year}.csv')\n",
    "\n",
    "# Create the new directories\n",
    "intervals = ['1h', '30min', '15min']\n",
    "for interval in intervals:\n",
    "    os.makedirs(os.path.join(cross_border_flows_folder_path, interval), exist_ok=True)\n",
    "\n",
    "# Read the original CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Convert the 'index' column to datetime\n",
    "df['index'] = pd.to_datetime(df['index'], format='%Y-%m-%d %H:%M:%S%z')\n",
    "\n",
    "# Function to extract rows at a specific time step and save to a new CSV file\n",
    "def extract_and_save(df, interval, folder_name):\n",
    "    # Resample the DataFrame\n",
    "    resampled_df = df.set_index('index').resample(interval).first().reset_index()\n",
    "    \n",
    "    # Define the new file path\n",
    "    new_file_path = os.path.join(cross_border_flows_folder_path, folder_name, f'{data_year}.csv')\n",
    "    \n",
    "    # Save the resampled DataFrame to the new CSV file\n",
    "    resampled_df.to_csv(new_file_path, index=False)\n",
    "    print(f\"File saved: {new_file_path}\")\n",
    "\n",
    "# Extract and save rows at different time steps\n",
    "extract_and_save(df, '1H', '1h')\n",
    "extract_and_save(df, '30T', '30min')\n",
    "extract_and_save(df, '15T', '15min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635513e-ee86-4c66-8df3-65c1e37a7797",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left; margin-left: 0.0em; font-weight: unbold; font-size: 16px; font-family: TimesNewRoman;\">\n",
    "Copying the time already formated Cross Border Flows data to the main Dispa-SET data base dirtectory\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "118d9e2f-718d-4bbd-ae71-6acf9243de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_path_2 = \"/Database/CrossBorderFlows/\"\n",
    "\n",
    "# Construct the power_plants_raw_data_folder_path variable\n",
    "cross_border_flows_data_base_folder_path = dispaSET_unleash_folder_path + additional_path_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "425eda04-4885-4185-845c-c8ce803aa211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/1h/2017.csv to /home/ray/Dispa-SET_Unleash/Database/CrossBorderFlows/1h/2017.csv\n",
      "Copied /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/30min/2017.csv to /home/ray/Dispa-SET_Unleash/Database/CrossBorderFlows/30min/2017.csv\n",
      "Copied /home/ray/Dispa-SET_Unleash/RawData/CrossBorderFlows/15min/2017.csv to /home/ray/Dispa-SET_Unleash/Database/CrossBorderFlows/15min/2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the subfolder names\n",
    "subfolders = ['1h', '30min', '15min']\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(data_year, source_base_path, dest_base_path, subfolders):\n",
    "    for subfolder in subfolders:\n",
    "        source_path = os.path.join(source_base_path, subfolder, f\"{data_year}.csv\")\n",
    "        dest_folder_path = os.path.join(dest_base_path, subfolder)\n",
    "\n",
    "        # Create the destination subfolder if it does not exist\n",
    "        os.makedirs(dest_folder_path, exist_ok=True)\n",
    "\n",
    "        dest_path = os.path.join(dest_folder_path, f\"{data_year}.csv\")\n",
    "        \n",
    "        # Copy the file\n",
    "        if os.path.isfile(source_path):\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            print(f\"Copied {source_path} to {dest_path}\")\n",
    "        else:\n",
    "            print(f\"File {source_path} does not exist\")\n",
    "\n",
    "# Call the function\n",
    "copy_files(data_year, cross_border_flows_folder_path, cross_border_flows_data_base_folder_path, subfolders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
